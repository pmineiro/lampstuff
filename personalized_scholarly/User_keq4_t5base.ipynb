{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a258105a-3b77-499a-bc5c-6e1ceb3e92e5",
   "metadata": {},
   "source": [
    "# Step 1: fine-tune LLM using top result from (fixed) ranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f4483a-9d5d-4c65-923c-cc9e327c9498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******** augment = 4 max_iteration = 5 model_type = base *********\n",
      "n                 iter (since)         4 loss (since)       4 rouge1 (since)   4 rouge1 ema (since) 4 rouge1 (dev) (since)      dt\n",
      "1                0.000 (0.000)          2.260 (2.260)          0.426 (0.426)          0.426 (0.426)          0.000 (0.000)  7.66 s\n",
      "2                0.000 (0.000)          2.047 (1.835)          0.464 (0.502)          0.459 (0.492)          0.000 (0.000)  13.9 s\n",
      "4                0.000 (0.000)          1.949 (1.850)          0.436 (0.409)          0.462 (0.466)          0.000 (0.000)  25.2 s\n",
      "16               0.000 (0.000)          1.983 (1.987)          0.426 (0.431)          0.435 (0.424)          0.000 (0.000)  1.56 m\n",
      "32               0.000 (0.000)          1.967 (1.950)          0.438 (0.449)          0.440 (0.444)          0.000 (0.000)  3.01 m\n",
      "64               0.000 (0.000)          1.902 (1.838)          0.435 (0.433)          0.438 (0.437)          0.000 (0.000)  5.93 m\n",
      "128              0.000 (0.000)          1.846 (1.790)          0.436 (0.438)          0.439 (0.441)          0.000 (0.000)  11.8 m\n",
      "256              0.000 (0.000)          1.817 (1.756)          0.442 (0.452)          0.442 (0.447)          0.427 (0.427)  22.2 m\n",
      "503              0.000 (0.000)          1.817 (0.000)          0.442 (0.000)          0.442 (0.000)          0.422 (0.420)  39.7 m\n",
      "1006             0.500 (1.000)          1.793 (1.769)          0.452 (0.462)          0.449 (0.456)          0.424 (0.427)  1.33 h\n",
      "1509             1.000 (2.000)          1.769 (1.720)          0.456 (0.464)          0.455 (0.467)          0.425 (0.427)  2.02 h\n",
      "2012             1.500 (3.000)          1.760 (1.734)          0.453 (0.445)          0.454 (0.449)          0.426 (0.428)  2.69 h\n",
      "2515             2.000 (4.000)          1.751 (1.716)          0.454 (0.455)          0.453 (0.453)          0.427 (0.429)  3.36 h\n"
     ]
    }
   ],
   "source": [
    "def launch():\n",
    "    import os\n",
    "    import StepOne\n",
    "    import torch\n",
    "\n",
    "    os.environ['MODEL_TYPE'] = 'base'\n",
    "    os.environ['BATCH_SIZE'] = '8'\n",
    "    \n",
    "    world_size = torch.cuda.device_count()\n",
    "    torch.multiprocessing.spawn(StepOne.step_one,\n",
    "                                args=(world_size,),\n",
    "                                nprocs=world_size,\n",
    "                                join=True)\n",
    "    \n",
    "launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41c12d7-7558-410b-a1b5-768fc86f98a7",
   "metadata": {},
   "source": [
    "# Step 2: learn ranker using (fixed pre-finetuned) task LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcd3580-0bf2-450a-a316-849124fafd3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******** augment = 1 max_iteration = 20 model_type = base *********\n",
      "n                    iter (since)            4 loss (since)           4 rouge (since)       4 ema rouge (since) 4 ema rouge (dev) (since)            nsamps (since)      dt\n",
      "2                   0.000 (0.000)             0.655 (0.660)             0.407 (0.277)             0.372 (0.206)             0.000 (0.000)            60.500 (6.500)  1.04 m\n",
      "4                   0.000 (0.000)             0.676 (0.696)             0.457 (0.506)             0.424 (0.476)             0.000 (0.000)            44.625 (28.750)  1.96 m\n",
      "8                   0.000 (0.000)             0.648 (0.621)             0.480 (0.502)             0.481 (0.538)             0.000 (0.000)            33.000 (21.375)  3.14 m\n",
      "16                  0.000 (0.000)             0.662 (0.676)             0.466 (0.453)             0.453 (0.426)             0.000 (0.000)            27.688 (22.375)  5.78 m\n",
      "32                  0.000 (0.000)             0.678 (0.695)             0.410 (0.353)             0.413 (0.372)             0.000 (0.000)            20.703 (13.719)  10.5 m\n",
      "64                  0.000 (0.000)             0.673 (0.668)             0.439 (0.469)             0.436 (0.458)             0.000 (0.000)            17.453 (14.203)  19.4 m\n",
      "128                 0.000 (0.000)             0.693 (0.713)             0.463 (0.487)             0.467 (0.497)             0.000 (0.000)            14.285 (11.117)  36.3 m\n",
      "256                 0.000 (0.000)             0.700 (0.708)             0.479 (0.495)             0.485 (0.503)             0.000 (0.000)            13.109 (11.934)  1.16 h\n",
      "512                 0.000 (0.000)             0.716 (0.732)             0.476 (0.472)             0.478 (0.472)             0.000 (0.000)            13.026 (12.943)   2.3 h\n",
      "1024                0.000 (0.000)             0.711 (0.705)             0.465 (0.454)             0.467 (0.456)             0.000 (0.000)            11.261 (9.495)  4.39 h\n",
      "2048                0.000 (0.000)             0.714 (0.729)             0.465 (0.465)             0.468 (0.470)             0.423 (0.423)            10.767 (8.051)  7.03 h\n",
      "2460                0.000 (0.000)             0.714 (0.000)             0.465 (0.000)             0.468 (0.000)             0.425 (0.428)            10.767 (0.000)     8 h\n",
      "4920                0.500 (1.000)             0.701 (0.688)             0.462 (0.460)             0.463 (0.459)             0.426 (0.427)             9.253 (7.738)  15.5 h\n",
      "7380                1.000 (2.000)             0.693 (0.678)             0.463 (0.466)             0.463 (0.463)             0.426 (0.427)             8.720 (7.653)    23 h\n",
      "9840                1.500 (3.000)             0.689 (0.676)             0.462 (0.459)             0.462 (0.458)             0.427 (0.429)             8.409 (7.476)  1.27 d\n",
      "12300               2.000 (4.000)             0.686 (0.674)             0.462 (0.462)             0.462 (0.463)             0.428 (0.431)             8.226 (7.497)  1.58 d\n",
      "14760               2.500 (5.000)             0.684 (0.672)             0.463 (0.465)             0.463 (0.466)             0.428 (0.430)             8.095 (7.437)  1.89 d\n",
      "17220               3.000 (6.000)             0.682 (0.672)             0.464 (0.471)             0.464 (0.472)             0.428 (0.429)             7.938 (6.997)   2.2 d\n",
      "19680               3.500 (7.000)             0.680 (0.670)             0.464 (0.468)             0.465 (0.469)             0.429 (0.435)             7.825 (7.033)  2.51 d\n",
      "22140               4.000 (8.000)             0.679 (0.668)             0.464 (0.461)             0.464 (0.460)             0.429 (0.430)             7.705 (6.749)  2.82 d\n",
      "24600               4.500 (9.000)             0.678 (0.670)             0.464 (0.465)             0.464 (0.461)             0.430 (0.432)             7.615 (6.808)  3.13 d\n"
     ]
    }
   ],
   "source": [
    "def launch():\n",
    "    import os\n",
    "    import StepTwo\n",
    "    import torch\n",
    "\n",
    "    os.environ['MODEL_TYPE'] = 'base'\n",
    "    os.environ['BATCH_SIZE'] = '2'\n",
    "    os.environ['LEARN_BATCH_SIZE'] = '4'\n",
    "    os.environ['GRAD_FREE_BATCH_SIZE'] = '16'\n",
    "    \n",
    "    world_size = torch.cuda.device_count()\n",
    "    torch.multiprocessing.spawn(StepTwo.step_two,\n",
    "                                args=(world_size,),\n",
    "                                nprocs=world_size,\n",
    "                                join=True)\n",
    "    \n",
    "launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badf7e13-9188-4940-8b85-392df0f8946b",
   "metadata": {},
   "source": [
    "# Step 3: Prepare Submission Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669871e4-5422-4e0c-b1fa-484c9d9558d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** step1_iter: 4_augment4 step2_iter: 7_augment1 ***\n",
      "n       4 rouge (dev) (since)      dt\n",
      "1               0.422 (0.422)  7.84 s\n",
      "2               0.428 (0.434)  16.4 s\n",
      "4               0.449 (0.469)    36 s\n",
      "8               0.472 (0.495)  1.21 m\n",
      "16              0.451 (0.430)  2.44 m\n",
      "32              0.433 (0.415)  4.87 m\n",
      "64              0.422 (0.410)  9.68 m\n",
      "128             0.434 (0.446)  19.1 m\n",
      "256             0.434 (0.435)  38.1 m\n",
      "512             0.432 (0.422)  1.27 h\n"
     ]
    }
   ],
   "source": [
    "def prepare_submission(*, step2_iter, step1_iter, k):\n",
    "    import evaluate\n",
    "    import json\n",
    "    from RewardPredictor import RewardPredictor\n",
    "    from TaskLLM import TaskLLM\n",
    "    from PersonalizedScholarly import dev_loader, test_loader\n",
    "    from ProgressPrinter import ProgressPrinter\n",
    "    from transformers import T5ForConditionalGeneration\n",
    "    import torch\n",
    "    from Util import interleave\n",
    "    \n",
    "    device = 'cuda'\n",
    "    torch.set_default_device(device)\n",
    "    torch.manual_seed(8675309)\n",
    "\n",
    "    dev = dev_loader(batch_size=8)\n",
    "    test = test_loader(batch_size=8)\n",
    "\n",
    "    t5 = T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\n",
    "    taskllm_model_id = f'User_keq{k}_t5base_step1_iter{step1_iter}'\n",
    "    t5.load_adapter(taskllm_model_id, 'raw_taskllm')\n",
    "    t5.load_adapter(taskllm_model_id, 'ema_taskllm')\n",
    "    rhat_model_id = f'User_keq{k}_t5base_step2_iter{step2_iter}'\n",
    "    t5.load_adapter(rhat_model_id, 'raw_rhat')\n",
    "    t5.load_adapter(rhat_model_id, 'ema_rhat')\n",
    "    t5.enable_adapters()\n",
    "    \n",
    "    taskllm = TaskLLM(t5=t5, adapter_suffix=\"taskllm\")\n",
    "    rewardpredictor = RewardPredictor(t5=t5, adapter_suffix=\"rhat\", model_id=rhat_model_id)\n",
    "    rouge_metric = evaluate.load('rouge')\n",
    "    gradfree_batch_size = 128\n",
    "    n_randos = 128\n",
    "\n",
    "    gumbel = torch.distributions.gumbel.Gumbel(0,1)\n",
    "    def randomized_similarity(embeddings, nsamples):\n",
    "        scores = embeddings[0,:] @ embeddings[1:,:].T\n",
    "        temperature = scores[0].item() - scores[min(scores.shape[0]-1, 4)].item()\n",
    "        gumbel_shape = torch.Size([nsamples, scores.shape[0]])\n",
    "        gumbels = temperature * gumbel.sample(gumbel_shape).to(scores.device)\n",
    "        safek = min(k, scores.shape[0])\n",
    "        return torch.unique(torch.topk(scores.unsqueeze(0) + gumbels, dim=1, k=safek).indices, sorted=False, dim=0)\n",
    "\n",
    "    def inner_batch(func, inner_batch_size, inputs):\n",
    "        from more_itertools import chunked\n",
    "        return [ func(*ib) for ib in zip(*[ chunked(g, inner_batch_size) for g in inputs ]) ]\n",
    "    \n",
    "    print(f'*** step1_iter: {step1_iter} step2_iter: {step2_iter} ***')\n",
    "\n",
    "    devgolds, testgolds = [], []\n",
    "    with ProgressPrinter(f'{k} rouge (dev)') as printer:\n",
    "        cumsum = lambda z, acc=0: [0] + [ acc := acc + v for v in z ]\n",
    "\n",
    "        for isdev, (examples, labels) in interleave(dev, test, sequential=True):\n",
    "            with torch.no_grad():\n",
    "                texts_to_embed = [ [ text[:256]\n",
    "                                     for text in (' '.join(ex['abstract'].split()), )\n",
    "                                   ] +\n",
    "                                   [ text[:256]\n",
    "                                     for v in ex['profile']\n",
    "                                     for text in (' '.join(v['abstract'].split()), )\n",
    "                                   ]\n",
    "                                   for ex in examples\n",
    "                                 ]\n",
    "                embeddings = torch.cat(inner_batch(func = dev.embed,\n",
    "                                                   inner_batch_size = gradfree_batch_size,\n",
    "                                                   inputs = (sum(texts_to_embed, []),)\n",
    "                                                  ),\n",
    "                                       dim=0)\n",
    "                splits = cumsum(map(len, texts_to_embed))\n",
    "                randos = [ randomized_similarity(embeddings[a:b,:], n_randos) for a, b in zip(splits, splits[1:]) ]\n",
    "                prompts = [ [ dev.prepend_to_prompt(ex, [ ex['profile'][ind] for ind in indices ])\n",
    "                              for indices in rando.to('cpu').tolist()\n",
    "                            ]\n",
    "                            for ex, rando in zip(examples, randos)\n",
    "                          ]\n",
    "                rhats = torch.cat(inner_batch(func = rewardpredictor.predict,\n",
    "                                              inner_batch_size = gradfree_batch_size,\n",
    "                                              inputs = (sum(prompts, []),)\n",
    "                                             ),\n",
    "                                  dim=0)\n",
    "                splits = cumsum(map(len, prompts))\n",
    "                greedyaction = [ rhats[a:b].argmax().item() for a, b in zip(splits, splits[1:]) ]\n",
    "                greedyprompts = [ prompt[a] for prompt, a in zip(prompts, greedyaction) ]\n",
    "                guesses = sum(inner_batch(func = taskllm.generate,\n",
    "                                          inner_batch_size = gradfree_batch_size,\n",
    "                                          inputs = (greedyprompts,)\n",
    "                                         ),\n",
    "                              [])\n",
    "                if isdev:\n",
    "                    rewards = sum( ( rouge_metric.compute(predictions=[guess],\n",
    "                                                          references=[label],\n",
    "                                                          use_aggregator=False)['rouge1']\n",
    "                                     for guess, label in zip(guesses, labels)\n",
    "                                  ),\n",
    "                                  [])\n",
    "                    rewards = torch.Tensor(rewards, device='cpu').mean().item()\n",
    "                else:\n",
    "                    rewards = None\n",
    "                \n",
    "                for ex, guess in zip(examples, guesses):\n",
    "                    (devgolds if isdev else testgolds).append({ 'id': ex['id'], 'output': guess })\n",
    "\n",
    "            printer.addobs(rewards)\n",
    "\n",
    "    for wut, golds in ( ('dev', devgolds), ('test', testgolds) ):\n",
    "        with open(f'lamp5u_{wut}golds_t5base_keq{k}_step1_iter{step1_iter}_step2_iter{step2_iter}.json', 'w') as jsonfile:\n",
    "            json.dump({ 'task': 'LaMP_5', 'golds': golds }, jsonfile)\n",
    "            \n",
    "prepare_submission(k=4, step1_iter='4_augment4', step2_iter='7_augment1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
