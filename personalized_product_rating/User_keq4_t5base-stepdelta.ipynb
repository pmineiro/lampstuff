{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c724eaf4-ee13-40d9-bfaf-8b99acbe102c",
      "metadata": {},
      "source": [
        "# Step 1 Dev Set Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9eb41ecf-c9ed-4438-8af1-e86e0f3fdcf0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "*** step1_iter: 2_augment2 ***\n",
            "n       4 MAE (dev) (since) 4 MSE (dev) (since)      dt\n",
            "1             0.500 (0.500)       0.500 (0.500)  1.99 s\n",
            "2             0.312 (0.125)       0.312 (0.125)  3.85 s\n",
            "4             0.281 (0.250)       0.281 (0.250)  6.02 s\n",
            "8             0.266 (0.250)       0.266 (0.250)  13.2 s\n",
            "16            0.312 (0.359)       0.359 (0.453)  24.8 s\n",
            "32            0.281 (0.250)       0.336 (0.312)  50.1 s\n",
            "64            0.283 (0.285)       0.404 (0.473)  1.62 m\n",
            "128           0.256 (0.229)       0.332 (0.260)  3.26 m\n",
            "256           0.244 (0.231)       0.309 (0.286)  6.55 m\n",
            "313           0.247 (0.261)       0.312 (0.327)  8.03 m\n"
          ]
        }
      ],
      "source": [
        "def step1_dev_set_labels(*, step1_iter, k):\n",
        "    import json\n",
        "    from RewardPredictor import RewardPredictor\n",
        "    from TaskLLM import TaskLLM\n",
        "    from PersonalizedProductRating import dev_loader\n",
        "    from ProgressPrinter import ProgressPrinter\n",
        "    from transformers import T5ForConditionalGeneration\n",
        "    import torch\n",
        "    from Util import interleave\n",
        "    \n",
        "    device = 'cuda'\n",
        "    torch.set_default_device(device)\n",
        "    torch.manual_seed(8675309)\n",
        "\n",
        "    dev = dev_loader(batch_size=8)\n",
        "\n",
        "    t5 = T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\n",
        "    taskllm_model_id = f'User_keq{k}_t5base_step1_iter{step1_iter}'\n",
        "    t5.load_adapter(taskllm_model_id, 'taskllm')\n",
        "    \n",
        "    taskllm = TaskLLM(t5=t5, adapter_name=\"taskllm\")\n",
        "\n",
        "    def inner_batch(func, inner_batch_size, inputs):\n",
        "        from more_itertools import chunked\n",
        "        return [ func(*ib) for ib in zip(*[ chunked(g, inner_batch_size) for g in inputs ]) ]\n",
        "    \n",
        "    print(f'*** step1_iter: {step1_iter} ***')\n",
        "\n",
        "    devgolds = []\n",
        "    with ProgressPrinter(f'{k} MAE (dev)', f'{k} MSE (dev)') as printer:\n",
        "        cumsum = lambda z, acc=0: [0] + [ acc := acc + v for v in z ]\n",
        "\n",
        "        for examples, labels in dev:\n",
        "            with torch.no_grad():\n",
        "                texts_to_embed = [ [ text[:256]\n",
        "                                     for text in (' '.join(ex['review'].split()), )\n",
        "                                   ] +\n",
        "                                   [ text[:256]\n",
        "                                     for v in ex['profile']\n",
        "                                     for text in (' '.join(v['text'].split()), )\n",
        "                                   ]\n",
        "                                   for ex in examples\n",
        "                                 ]\n",
        "                embeddings = torch.cat(inner_batch(func = dev.embed,\n",
        "                                                   inner_batch_size = 128,\n",
        "                                                   inputs = (sum(texts_to_embed, []),)\n",
        "                                                  ),\n",
        "                                       dim=0)\n",
        "                splits = cumsum(map(len, texts_to_embed))\n",
        "                indices = [ torch.topk(embeddings[a,:] @ embeddings[a+1:b,:].T, dim=0, k=safek).indices\n",
        "                            for a, b in zip(splits, splits[1:])\n",
        "                            for safek in (max(0, min(k, b-a-1)),)\n",
        "                          ]\n",
        "                prompts = [ dev.prepend_to_prompt(ex, [ ex['profile'][ind] for ind in index.to('cpu').tolist() ])\n",
        "                            for ex, index in zip(examples, indices) ]\n",
        "                cumul = taskllm.predict(prompts).exp().cumsum(dim=1)\n",
        "                guesses = (cumul>=0.5).long().argmax(dim=1)\n",
        "                targets = [ int(label) - 1 for label in labels ]\n",
        "                targets = torch.Tensor(targets).long().to(guesses.device)\n",
        "                mae = torch.abs(guesses - targets).float().mean().item()\n",
        "                mse = torch.square(guesses - targets).float().mean().item()\n",
        "\n",
        "                for ex, guess in zip(examples, guesses):\n",
        "                    devgolds.append({ 'id': ex['id'], 'output': f'{1+guess}' })\n",
        "\n",
        "            printer.addobs(mae, mse)\n",
        "\n",
        "    with open(f'lamp3u_step1_dev_golds.json', 'w') as jsonfile:\n",
        "        json.dump({ 'task': 'LaMP_2', 'golds': devgolds }, jsonfile)\n",
        "            \n",
        "step1_dev_set_labels(k=4, step1_iter='2_augment2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8dfe04c6-5858-4130-a12c-308610861567",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "*** step1_iter: 2_augment2 step2_iter: 0_augment2 ***\n",
            "n       4 MAE (dev) (since) 4 MSE (dev) (since)      dt\n",
            "1             0.375 (0.375)       0.375 (0.375)  5.26 s\n",
            "2             0.250 (0.125)       0.250 (0.125)  10.2 s\n",
            "4             0.219 (0.188)       0.219 (0.188)  18.5 s\n",
            "8             0.234 (0.250)       0.234 (0.250)  36.9 s\n",
            "16            0.312 (0.391)       0.359 (0.484)  1.25 m\n",
            "32            0.297 (0.281)       0.344 (0.328)   2.5 m\n",
            "64            0.293 (0.289)       0.410 (0.477)  4.91 m\n",
            "128           0.270 (0.246)       0.348 (0.285)  9.87 m\n",
            "256           0.252 (0.234)       0.317 (0.287)  19.7 m\n",
            "313           0.254 (0.263)       0.322 (0.342)  24.2 m\n"
          ]
        }
      ],
      "source": [
        "def step2_dev_set_labels(*, step2_iter, step1_iter, k):\n",
        "    import json\n",
        "    from RewardPredictor import RewardPredictor\n",
        "    from TaskLLM import TaskLLM\n",
        "    from PersonalizedProductRating import dev_loader\n",
        "    from ProgressPrinter import ProgressPrinter\n",
        "    from transformers import T5ForConditionalGeneration\n",
        "    import torch\n",
        "    from Util import interleave\n",
        "    \n",
        "    device = 'cuda'\n",
        "    torch.set_default_device(device)\n",
        "    torch.manual_seed(8675309)\n",
        "\n",
        "    dev = dev_loader(batch_size=8)\n",
        "\n",
        "    t5 = T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\n",
        "    t5.load_adapter(f'User_keq{k}_t5base_step1_iter{step1_iter}', 'taskllm')\n",
        "    t5.load_adapter(f'User_keq{k}_t5base_step2_iter{step2_iter}', 'rhat')\n",
        "    t5.enable_adapters()\n",
        "    \n",
        "    taskllm = TaskLLM(t5=t5, adapter_name=\"taskllm\")\n",
        "    rewardpredictor = RewardPredictor(t5=t5, adapter_name=\"rhat\", model_id=f'User_keq{k}_t5base_step2_iter{step2_iter}')\n",
        "\n",
        "    gumbel = torch.distributions.gumbel.Gumbel(0,1)\n",
        "    def randomized_similarity(embeddings, nsamples):\n",
        "        scores = embeddings[0,:] @ embeddings[1:,:].T\n",
        "        temperature = scores[0].item() - scores[min(scores.shape[0], 4)].item()\n",
        "        gumbel_shape = torch.Size([nsamples, scores.shape[0]])\n",
        "        gumbels = temperature * gumbel.sample(gumbel_shape).to(scores.device)\n",
        "        return torch.unique(torch.topk(scores.unsqueeze(0) + gumbels, dim=1, k=k).indices, sorted=False, dim=0)\n",
        "\n",
        "    def inner_batch(func, inner_batch_size, inputs):\n",
        "        from more_itertools import chunked\n",
        "        return [ func(*ib) for ib in zip(*[ chunked(g, inner_batch_size) for g in inputs ]) ]\n",
        "\n",
        "    print(f'*** step1_iter: {step1_iter} step2_iter: {step2_iter} ***')\n",
        "    nvoters = 1\n",
        "    \n",
        "    with ProgressPrinter(f'{k} MAE (dev)', f'{k} MSE (dev)') as printer:\n",
        "        devgolds = []\n",
        "        cumsum = lambda z, acc=0: [0] + [ acc := acc + v for v in z ]\n",
        "\n",
        "        for examples, labels in dev:\n",
        "            with torch.no_grad():\n",
        "                texts_to_embed = [ [ text[:256]\n",
        "                                     for text in (' '.join(ex['review'].split()), )\n",
        "                                   ] +\n",
        "                                   [ text[:256]\n",
        "                                     for v in ex['profile']\n",
        "                                     for text in (' '.join(v['text'].split()), )\n",
        "                                   ]\n",
        "                                   for ex in examples\n",
        "                                 ]\n",
        "                embeddings = torch.cat(inner_batch(func = dev.embed,\n",
        "                                                   inner_batch_size = 128,\n",
        "                                                   inputs = (sum(texts_to_embed, []),)\n",
        "                                                  ),\n",
        "                                       dim=0)\n",
        "                splits = cumsum(map(len, texts_to_embed))\n",
        "                randos = [ randomized_similarity(embeddings[a:b,:], 64) for a, b in zip(splits, splits[1:]) ]\n",
        "                prompts = [ [ dev.prepend_to_prompt(ex, [ ex['profile'][ind] for ind in indices ])\n",
        "                              for indices in rando.to('cpu').tolist()\n",
        "                            ]\n",
        "                            for ex, rando in zip(examples, randos)\n",
        "                          ]\n",
        "                rhats = torch.cat(inner_batch(func = rewardpredictor.predict,\n",
        "                                              inner_batch_size = 128,\n",
        "                                              inputs = (sum(prompts, []),)\n",
        "                                             ),\n",
        "                                  dim=0)\n",
        "                splits = cumsum(map(len, prompts))\n",
        "                votingprompts = [ [ prompt[v] for v in torch.topk(rhats[a:b].view(-1), k=min(nvoters, b-a)).indices.to('cpu').tolist() ]\n",
        "                                    for a, b, prompt in zip(splits, splits[1:], prompts)\n",
        "                                ]\n",
        "                predicts = torch.cat(inner_batch(func = taskllm.predict,\n",
        "                                                 inner_batch_size = 128,\n",
        "                                                 inputs = (sum(votingprompts, []),)\n",
        "                                                ),\n",
        "                                     dim=0)\n",
        "                splits = cumsum(map(len, votingprompts))\n",
        "                guesses = torch.cat([ (predicts[a:b,:].logsumexp(dim=0, keepdim=True).exp().cumsum(dim=1) >= 0.5 * (b-a)).long().argmax(dim=1)\n",
        "                                      for a, b in zip(splits, splits[1:])\n",
        "                                    ],\n",
        "                                    dim=0)\n",
        "\n",
        "                targets = [ int(label) - 1 for label in labels ]\n",
        "                targets = torch.Tensor(targets).long().to(guesses.device)\n",
        "                mae = torch.abs(guesses - targets).float().mean().item()\n",
        "                mse = torch.square(guesses - targets).float().mean().item()\n",
        "\n",
        "                for ex, guess in zip(examples, guesses):\n",
        "                    devgolds.append({ 'id': ex['id'], 'output': f'{1+guess}' })\n",
        "\n",
        "            printer.addobs(mae, mse)\n",
        "\n",
        "        printer.print()\n",
        "        printer.autoprint = False\n",
        "\n",
        "    with open(f'lamp3u_step2_dev_golds.json', 'w') as jsonfile:\n",
        "        json.dump({ 'task': 'LaMP_2', 'golds': devgolds }, jsonfile)\n",
        "\n",
        "step2_dev_set_labels(k=4, step1_iter='2_augment2', step2_iter='0_augment2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c6e5680-3a8e-4c3a-856b-12c5dee7ee3e",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}