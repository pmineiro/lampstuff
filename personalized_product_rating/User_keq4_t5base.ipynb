{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a258105a-3b77-499a-bc5c-6e1ceb3e92e5",
   "metadata": {},
   "source": [
    "# Step 1: fine-tune LLM using top result from (fixed) ranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f4483a-9d5d-4c65-923c-cc9e327c9498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************ augment = 4 *************\n",
      "n              iter (since)      4 loss (since)       4 MAE (since) 4 MAE (dev) (since)      dt\n",
      "1             0.000 (0.000)       0.858 (0.858)       0.550 (0.550)       0.000 (0.000)  32.1 s\n",
      "2             0.000 (0.000)       0.711 (0.564)       0.400 (0.250)       0.000 (0.000)  44.7 s\n",
      "4             0.000 (0.000)       0.759 (0.807)       0.381 (0.362)       0.000 (0.000)  1.21 m\n",
      "8             0.000 (0.000)       0.770 (0.780)       0.378 (0.375)       0.000 (0.000)  2.06 m\n",
      "16            0.000 (0.000)       0.739 (0.709)       0.355 (0.331)       0.000 (0.000)  3.82 m\n",
      "32            0.000 (0.000)       0.684 (0.628)       0.325 (0.295)       0.000 (0.000)  7.27 m\n",
      "64            0.000 (0.000)       0.672 (0.661)       0.320 (0.315)       0.000 (0.000)  14.3 m\n",
      "128           0.000 (0.000)       0.652 (0.631)       0.309 (0.298)       0.000 (0.000)  28.1 m\n",
      "256           0.000 (0.000)       0.613 (0.574)       0.292 (0.275)       0.000 (0.000)  55.7 m\n",
      "512           0.000 (0.000)       0.594 (0.575)       0.283 (0.273)       0.000 (0.000)  1.86 h\n",
      "1024          0.000 (0.000)       0.583 (0.572)       0.276 (0.270)       0.000 (0.000)  3.71 h\n",
      "2048          0.000 (0.000)       0.565 (0.546)       0.266 (0.255)       0.000 (0.000)  7.42 h\n",
      "2657          0.000 (0.000)       0.560 (0.540)       0.263 (0.252)       0.254 (0.254)  9.29 h\n",
      "5314          0.500 (1.000)       0.542 (0.525)       0.254 (0.245)       0.255 (0.256)  18.6 h\n",
      "7971          1.000 (2.000)       0.533 (0.515)       0.249 (0.240)       0.251 (0.243)  1.16 d\n",
      "10628         1.500 (3.000)       0.528 (0.513)       0.246 (0.237)       0.250 (0.246)  1.55 d\n",
      "13285         2.000 (4.000)       0.524 (0.508)       0.244 (0.235)       0.250 (0.250)  1.94 d\n"
     ]
    }
   ],
   "source": [
    "def step_one(*, k, max_iteration):\n",
    "    import os\n",
    "    from PersonalizedProductRating import train_loader, dev_loader\n",
    "    from ProgressPrinter import ProgressPrinter\n",
    "    from peft import LoraConfig, TaskType\n",
    "    from TaskLLM import TaskLLM\n",
    "    from transformers import T5ForConditionalGeneration\n",
    "    import torch\n",
    "    from Util import interleave, set_directory\n",
    "\n",
    "    torch.manual_seed(2112)\n",
    "\n",
    "    augment = int(os.environ.get('AUGMENT', '4'))\n",
    "    train = train_loader(batch_size=8, augment=augment)\n",
    "    dev = dev_loader(batch_size=16)\n",
    "\n",
    "    t5 = T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\n",
    "    taskllm_config = LoraConfig(r=5, task_type=TaskType.SEQ_2_SEQ_LM)\n",
    "    t5.add_adapter(taskllm_config, \"taskllm\")\n",
    "    t5.enable_adapters()\n",
    "\n",
    "    taskllm = TaskLLM(t5=t5, adapter_name=\"taskllm\")\n",
    "\n",
    "    def inner_batch(func, inner_batch_size, inputs):\n",
    "        from more_itertools import chunked\n",
    "        return [ func(*ib) for ib in zip(*[ chunked(g, inner_batch_size) for g in inputs ]) ]\n",
    "\n",
    "    print(f'************ augment = {augment} *************')\n",
    "    with ProgressPrinter('iter', f'{k} loss', f'{k} MAE', f'{k} MAE (dev)') as printer:\n",
    "        cumsum = lambda z, acc=0: [0] + [ acc := acc + v for v in z ]\n",
    "        \n",
    "        for iteration in range(max_iteration):\n",
    "            for istrain, (examples, labels) in interleave(train, dev, sequential=True):\n",
    "                with torch.no_grad():\n",
    "                    texts_to_embed = [ [ text[:256]\n",
    "                                         for text in (' '.join(ex['review'].split()), ) \n",
    "                                       ] + \n",
    "                                       [ text[:256]\n",
    "                                         for v in ex['profile']\n",
    "                                         for text in (' '.join(v['text'].split()), )\n",
    "                                       ]\n",
    "                                       for ex in examples\n",
    "                                     ]\n",
    "                    embeddings = torch.cat(inner_batch(func = lambda t: dev.embed(t),\n",
    "                                                       inner_batch_size = 64 * torch.cuda.device_count(),\n",
    "                                                       inputs = (sum(texts_to_embed, []),)\n",
    "                                                      ),\n",
    "                                           dim=0)\n",
    "                    splits = cumsum(map(len, texts_to_embed))\n",
    "                    indices = [ torch.topk(embeddings[a,:] @ embeddings[a+1:b,:].T, dim=0, k=k).indices for a, b in zip(splits, splits[1:]) ]\n",
    "                    prompts = [ dev.prepend_to_prompt(ex, [ ex['profile'][ind] for ind in index.to('cpu').tolist() ])\n",
    "                                for ex, index in zip(examples, indices) ]\n",
    "                    targets = [ int(label) - 1 for label in labels ]\n",
    "                    cumul = taskllm.predict(prompts).exp().cumsum(dim=1)\n",
    "                    guesses = (cumul>=0.5).long().argmax(dim=1)\n",
    "                    targets = torch.Tensor(targets).long().to(guesses.device)\n",
    "                    mae = torch.abs(guesses - targets).float().mean().item()\n",
    "\n",
    "                loss = taskllm.learn(prompts, targets) if istrain else None\n",
    "                printer.addobs(iteration, loss, mae if istrain else None, mae if not istrain else None)\n",
    "\n",
    "            printer.print()\n",
    "            printer.autoprint = False\n",
    "            output_dir = os.environ.get('AMLT_OUTPUT_DIR', '.')\n",
    "            with set_directory(output_dir):\n",
    "                taskllm.save_pretrained(f'User_keq{k}_t5base_step1_iter{iteration}_augment{augment}')\n",
    "\n",
    "step_one(k=4, max_iteration=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf16df8-9320-4be9-98f9-da38da180a91",
   "metadata": {},
   "source": [
    "# Step 2: learn ranker using (fixed pre-finetuned) task LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58810c2b-35e5-4efe-bd43-1b5f25c16357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************ augment = 1 gamma = 1.0 step1_iter = 2_augment4 *************\n",
      "n              iter (since)      4 loss (since)       4 MAE (since) 4 MAE (dev) (since)       samps (since)      dt\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 40% | 10% |\n",
      "| ID | GPU  | MEM |\n",
      "-------------------\n",
      "|  0 | 100% | 85% |\n",
      "1             0.000 (0.000)       0.269 (0.269)       0.188 (0.188)       0.000 (0.000)      59.562 (59.562)  1.94 m\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 88% | 88% |\n",
      "2             0.000 (0.000)       0.306 (0.344)       0.219 (0.250)       0.000 (0.000)      30.906 (2.250)  2.69 m\n",
      "| ID | GPU  | MEM |\n",
      "-------------------\n",
      "|  0 | 100% | 88% |\n",
      "| ID | GPU  | MEM |\n",
      "-------------------\n",
      "|  0 | 100% | 88% |\n",
      "4             0.000 (0.000)       0.230 (0.153)       0.172 (0.125)       0.000 (0.000)      17.094 (3.281)   4.3 m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 121\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m set_directory(output_dir):\n\u001b[1;32m    119\u001b[0m                 taskllm\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUser_keq\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_t5base_step2_iter\u001b[39m\u001b[38;5;132;01m{\u001b[39;00miteration\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_augment\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maugment\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 121\u001b[0m \u001b[43mlearn_ranker\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 89\u001b[0m, in \u001b[0;36mlearn_ranker\u001b[0;34m(max_iteration, k)\u001b[0m\n\u001b[1;32m     87\u001b[0m nsamps \u001b[38;5;241m=\u001b[39m [ \u001b[38;5;28mlen\u001b[39m(aind) \u001b[38;5;28;01mfor\u001b[39;00m aind \u001b[38;5;129;01min\u001b[39;00m actionind ]\n\u001b[1;32m     88\u001b[0m guessprompts \u001b[38;5;241m=\u001b[39m [ [ prompt[a] \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m aind ] \u001b[38;5;28;01mfor\u001b[39;00m prompt, aind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(prompts, actionind) ]\n\u001b[0;32m---> 89\u001b[0m cumul \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(\u001b[43minner_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtaskllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcumsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m                              \u001b[49m\u001b[43minner_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_count\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m                              \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mguessprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m                             \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     93\u001b[0m                   dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     94\u001b[0m splits \u001b[38;5;241m=\u001b[39m cumsum(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mlen\u001b[39m, guessprompts))\n\u001b[1;32m     95\u001b[0m guesses \u001b[38;5;241m=\u001b[39m [ (cumul[a:b,:]\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m a, b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(splits, splits[\u001b[38;5;241m1\u001b[39m:]) ]\n",
      "Cell \u001b[0;32mIn[1], line 44\u001b[0m, in \u001b[0;36mlearn_ranker.<locals>.inner_batch\u001b[0;34m(func, inner_batch_size, inputs)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner_batch\u001b[39m(func, inner_batch_size, inputs):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmore_itertools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m chunked\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [ func(\u001b[38;5;241m*\u001b[39mib) \u001b[38;5;28;01mfor\u001b[39;00m ib \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m[ chunked(g, inner_batch_size) \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m inputs ]) ]\n",
      "Cell \u001b[0;32mIn[1], line 44\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner_batch\u001b[39m(func, inner_batch_size, inputs):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmore_itertools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m chunked\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [ \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mib\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m ib \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m[ chunked(g, inner_batch_size) \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m inputs ]) ]\n",
      "Cell \u001b[0;32mIn[1], line 89\u001b[0m, in \u001b[0;36mlearn_ranker.<locals>.<lambda>\u001b[0;34m(p)\u001b[0m\n\u001b[1;32m     87\u001b[0m nsamps \u001b[38;5;241m=\u001b[39m [ \u001b[38;5;28mlen\u001b[39m(aind) \u001b[38;5;28;01mfor\u001b[39;00m aind \u001b[38;5;129;01min\u001b[39;00m actionind ]\n\u001b[1;32m     88\u001b[0m guessprompts \u001b[38;5;241m=\u001b[39m [ [ prompt[a] \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m aind ] \u001b[38;5;28;01mfor\u001b[39;00m prompt, aind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(prompts, actionind) ]\n\u001b[0;32m---> 89\u001b[0m cumul \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(inner_batch(func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m p: \u001b[43mtaskllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mexp()\u001b[38;5;241m.\u001b[39mcumsum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     90\u001b[0m                               inner_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count(),\n\u001b[1;32m     91\u001b[0m                               inputs \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28msum\u001b[39m(guessprompts, []),)\n\u001b[1;32m     92\u001b[0m                              ),\n\u001b[1;32m     93\u001b[0m                   dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     94\u001b[0m splits \u001b[38;5;241m=\u001b[39m cumsum(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mlen\u001b[39m, guessprompts))\n\u001b[1;32m     95\u001b[0m guesses \u001b[38;5;241m=\u001b[39m [ (cumul[a:b,:]\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m a, b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(splits, splits[\u001b[38;5;241m1\u001b[39m:]) ]\n",
      "File \u001b[0;32m~/lampstuff/personalized_product_rating/TaskLLM.py:49\u001b[0m, in \u001b[0;36mTaskLLM.predict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transformer\u001b[38;5;241m.\u001b[39mset_adapter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_adapter_name)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m---> 49\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/lampstuff/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/lampstuff/personalized_product_rating/TaskLLM.py:29\u001b[0m, in \u001b[0;36mTaskLLM.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparallel\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mP\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[43mP\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transformer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_device_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m enc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer(data, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     31\u001b[0m scatterenc_input_ids \u001b[38;5;241m=\u001b[39m P\u001b[38;5;241m.\u001b[39mscatter(enc\u001b[38;5;241m.\u001b[39minput_ids, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_device_ids)\n",
      "File \u001b[0;32m~/miniconda3/envs/lampstuff/lib/python3.10/site-packages/torch/nn/parallel/replicate.py:115\u001b[0m, in \u001b[0;36mreplicate\u001b[0;34m(network, devices, detach)\u001b[0m\n\u001b[1;32m    113\u001b[0m module_indices[module] \u001b[38;5;241m=\u001b[39m i\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_replicas):\n\u001b[0;32m--> 115\u001b[0m     replica \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_replicate_for_data_parallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# This is a temporary fix for DDP. DDP needs to access the\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# replicated model parameters. It used to do so through\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;66;03m# `mode.parameters()`. The fix added in #33907 for DP stops the\u001b[39;00m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;66;03m# `parameters()` API from exposing the replicated parameters.\u001b[39;00m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# Hence, we add a `_former_parameters` dict here to support DDP.\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     replica\u001b[38;5;241m.\u001b[39m_former_parameters \u001b[38;5;241m=\u001b[39m OrderedDict()\n",
      "File \u001b[0;32m~/miniconda3/envs/lampstuff/lib/python3.10/site-packages/torch/nn/modules/module.py:2419\u001b[0m, in \u001b[0;36mModule._replicate_for_data_parallel\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2415\u001b[0m replica\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m   2417\u001b[0m \u001b[38;5;66;03m# replicas do not have parameters themselves, the replicas reference the original\u001b[39;00m\n\u001b[1;32m   2418\u001b[0m \u001b[38;5;66;03m# module.\u001b[39;00m\n\u001b[0;32m-> 2419\u001b[0m \u001b[43mreplica\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parameters\u001b[49m \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[1;32m   2420\u001b[0m replica\u001b[38;5;241m.\u001b[39m_buffers \u001b[38;5;241m=\u001b[39m replica\u001b[38;5;241m.\u001b[39m_buffers\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m   2421\u001b[0m replica\u001b[38;5;241m.\u001b[39m_modules \u001b[38;5;241m=\u001b[39m replica\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[0;32m~/miniconda3/envs/lampstuff/lib/python3.10/site-packages/torch/nn/modules/module.py:1627\u001b[0m, in \u001b[0;36mModule.__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   1624\u001b[0m                 d\u001b[38;5;241m.\u001b[39mdiscard(name)\n\u001b[1;32m   1626\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mParameter\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1628\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1629\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1630\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot assign parameters before Module.__init__() call\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/lampstuff/lib/python3.10/site-packages/torch/nn/parameter.py:9\u001b[0m, in \u001b[0;36m_ParameterMeta.__instancecheck__\u001b[0;34m(self, instance)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__instancecheck__\u001b[39m(\u001b[38;5;28mself\u001b[39m, instance):\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__instancecheck__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(instance, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(instance, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_is_param\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def learn_ranker(*, max_iteration, k):\n",
    "    import os\n",
    "    from RewardPredictor import RewardPredictor\n",
    "    from TaskLLM import TaskLLM\n",
    "    from PersonalizedProductRating import train_loader, dev_loader\n",
    "    from ProgressPrinter import ProgressPrinter\n",
    "    from SimpleRegret import SimpleRegretHypercubeSampler\n",
    "    from peft import LoraConfig, TaskType\n",
    "    from transformers import T5ForConditionalGeneration\n",
    "    import torch\n",
    "    from Util import interleave, set_directory, GPUMonitor\n",
    "\n",
    "    os.environ['TOKENIZERS_PARALLELISM'] = 'true'\n",
    "    step1_iter = os.environ.get('STEP1_ITER', '2_augment4')\n",
    "    augment = int(os.environ.get('AUGMENT', '1'))\n",
    "    gamma = float(os.environ.get('GAMMA', '1'))\n",
    "    output_dir = os.environ.get('AMLT_OUTPUT_DIR', '.')\n",
    "    \n",
    "    torch.manual_seed(8675309)\n",
    "\n",
    "    train = train_loader(batch_size=8, augment=augment)\n",
    "    dev = dev_loader(batch_size=16)\n",
    "\n",
    "    t5 = T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\n",
    "    t5.load_adapter(f'User_keq{k}_t5base_step1_iter{step1_iter}', 'taskllm')\n",
    "\n",
    "    rhat_config = LoraConfig(r=1, task_type=TaskType.SEQ_2_SEQ_LM)\n",
    "    t5.add_adapter(rhat_config, \"rhat\")\n",
    "    t5.enable_adapters()\n",
    "    \n",
    "    taskllm = TaskLLM(t5=t5, adapter_name=\"taskllm\")\n",
    "    rewardpredictor = RewardPredictor(t5=t5, adapter_name=\"rhat\")\n",
    "\n",
    "    gumbel = torch.distributions.gumbel.Gumbel(0,1)\n",
    "    def randomized_similarity(embeddings, nsamples):\n",
    "        scores = embeddings[0,:] @ embeddings[1:,:].T\n",
    "        temperature = scores[0].item() - scores[min(scores.shape[0], 4)].item()\n",
    "        gumbel_shape = torch.Size([nsamples, scores.shape[0]])\n",
    "        gumbels = temperature * gumbel.sample(gumbel_shape).to(scores.device)\n",
    "        return torch.unique(torch.topk(scores.unsqueeze(0) + gumbels, dim=1, k=k).indices, sorted=False, dim=0)\n",
    "\n",
    "    def inner_batch(func, inner_batch_size, inputs):\n",
    "        from more_itertools import chunked\n",
    "        return [ func(*ib) for ib in zip(*[ chunked(g, inner_batch_size) for g in inputs ]) ]\n",
    "\n",
    "    monitor = GPUMonitor(delay=60, maxcount=5)\n",
    "\n",
    "    print(f'************ augment = {augment} gamma = {gamma} step1_iter = {step1_iter} *************')\n",
    "    with ProgressPrinter('iter', f'{k} loss', f'{k} MAE', f'{k} MAE (dev)', 'samps') as printer:\n",
    "        cumsum = lambda z, acc=0: [0] + [ acc := acc + v for v in z ]\n",
    "        \n",
    "        for iteration in range(max_iteration):\n",
    "            for istrain, (examples, labels) in interleave(train, dev, sequential=True):\n",
    "                with torch.no_grad():\n",
    "                    texts_to_embed = [ [ text[:256]\n",
    "                                         for text in (' '.join(ex['review'].split()), ) \n",
    "                                       ] + \n",
    "                                       [ text[:256]\n",
    "                                         for v in ex['profile']\n",
    "                                         for text in (' '.join(v['text'].split()), )\n",
    "                                       ]\n",
    "                                       for ex in examples\n",
    "                                     ]\n",
    "                    embeddings = torch.cat(inner_batch(func = lambda t: dev.embed(t),\n",
    "                                                       inner_batch_size = 64 * torch.cuda.device_count(),\n",
    "                                                       inputs = (sum(texts_to_embed, []),)\n",
    "                                                      ),\n",
    "                                           dim=0)\n",
    "                    splits = cumsum(map(len, texts_to_embed))\n",
    "                    randos = [ randomized_similarity(embeddings[a:b,:], 64) for a, b in zip(splits, splits[1:]) ]\n",
    "                    prompts = [ [ dev.prepend_to_prompt(ex, [ ex['profile'][ind] for ind in indices ]) \n",
    "                                  for indices in rando.to('cpu').tolist() \n",
    "                                ]\n",
    "                                for ex, rando in zip(examples, randos) \n",
    "                              ]\n",
    "                    rhats = torch.cat(inner_batch(func = lambda p: rewardpredictor.predict(p),\n",
    "                                                  inner_batch_size = 64 * torch.cuda.device_count(),\n",
    "                                                  inputs = (sum(prompts, []),)\n",
    "                                                 ),\n",
    "                                      dim=0)\n",
    "                    splits = cumsum(map(len, prompts))\n",
    "                    samples = [ SimpleRegretHypercubeSampler(rhats[a:b].view(1, -1), gamma=gamma) for a, b in zip(splits, splits[1:]) ]\n",
    "                    actionind = [ [ exploit.item() ] + [ n for n, observed in enumerate(explore) if observed > 0 ]\n",
    "                                  for exploit, exploreraw in samples\n",
    "                                  for explore in (exploreraw[0].tolist() if istrain else [], )\n",
    "                                ]\n",
    "                    nsamps = [ len(aind) for aind in actionind ]\n",
    "                    guessprompts = [ [ prompt[a] for a in aind ] for prompt, aind in zip(prompts, actionind) ]\n",
    "                    cumul = torch.cat(inner_batch(func = lambda p: taskllm.predict(p).exp().cumsum(dim=1),\n",
    "                                                  inner_batch_size = 64 * torch.cuda.device_count(),\n",
    "                                                  inputs = (sum(guessprompts, []),)\n",
    "                                                 ),\n",
    "                                      dim=0)\n",
    "                    splits = cumsum(map(len, guessprompts))\n",
    "                    guesses = [ (cumul[a:b,:]>=0.5).long().argmax(dim=1) for a, b in zip(splits, splits[1:]) ]\n",
    "                    targets = [ int(label) - 1 for label in labels ]\n",
    "                    rewards = [ (1 - torch.abs((g - target)/4).float()).tolist() for g, target in zip(guesses, targets) ]\n",
    "                    greedymaes = [ torch.abs(g[0] - target).item() for g, target in zip(guesses, targets) ] \n",
    "\n",
    "                if istrain:\n",
    "                    rhatprompts = sum(guessprompts, [])\n",
    "                    rhattargets = sum(rewards, [])\n",
    "                    predlosses = inner_batch(func = lambda a, b: (len(a), rewardpredictor.learn(a, torch.Tensor([ [ r ] for r in b ]))),\n",
    "                                             inner_batch_size = 16 * torch.cuda.device_count(),\n",
    "                                             inputs = (rhatprompts, rhattargets)\n",
    "                                            )\n",
    "                    predloss = sum(n * v for n, v in predlosses) / sum(n for n, v in predlosses)\n",
    "                else:\n",
    "                    predloss = None\n",
    "\n",
    "                greedymae = torch.Tensor(greedymaes, device='cpu').float().mean().item()\n",
    "                nsamps = torch.Tensor(nsamps, device='cpu').float().mean().item() if istrain else None\n",
    "\n",
    "                printer.addobs(iteration, predloss, greedymae if istrain else None, greedymae if not istrain else None, nsamps)\n",
    "\n",
    "            printer.print()\n",
    "            printer.autoprint = False\n",
    "            with set_directory(output_dir):\n",
    "                taskllm.save_pretrained(f'User_keq{k}_t5base_step2_iter{iteration}_augment{augment}')\n",
    "                \n",
    "learn_ranker(k=4, max_iteration=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daa5c51-2bce-415a-8abf-e7ce3d0e1cbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
