{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a258105a-3b77-499a-bc5c-6e1ceb3e92e5",
   "metadata": {},
   "source": [
    "# Step 1: fine-tune LLM using top result from (fixed) ranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f4483a-9d5d-4c65-923c-cc9e327c9498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************ augment = 4 *************\n",
      "n              iter (since)      4 loss (since)       4 MAE (since) 4 MAE (dev) (since)      dt\n",
      "1             0.000 (0.000)       0.858 (0.858)       0.550 (0.550)       0.000 (0.000)  32.1 s\n",
      "2             0.000 (0.000)       0.711 (0.564)       0.400 (0.250)       0.000 (0.000)  44.7 s\n",
      "4             0.000 (0.000)       0.759 (0.807)       0.381 (0.362)       0.000 (0.000)  1.21 m\n",
      "8             0.000 (0.000)       0.770 (0.780)       0.378 (0.375)       0.000 (0.000)  2.06 m\n",
      "16            0.000 (0.000)       0.739 (0.709)       0.355 (0.331)       0.000 (0.000)  3.82 m\n",
      "32            0.000 (0.000)       0.684 (0.628)       0.325 (0.295)       0.000 (0.000)  7.27 m\n",
      "64            0.000 (0.000)       0.672 (0.661)       0.320 (0.315)       0.000 (0.000)  14.3 m\n",
      "128           0.000 (0.000)       0.652 (0.631)       0.309 (0.298)       0.000 (0.000)  28.1 m\n",
      "256           0.000 (0.000)       0.613 (0.574)       0.292 (0.275)       0.000 (0.000)  55.7 m\n",
      "512           0.000 (0.000)       0.594 (0.575)       0.283 (0.273)       0.000 (0.000)  1.86 h\n",
      "1024          0.000 (0.000)       0.583 (0.572)       0.276 (0.270)       0.000 (0.000)  3.71 h\n",
      "2048          0.000 (0.000)       0.565 (0.546)       0.266 (0.255)       0.000 (0.000)  7.42 h\n",
      "2657          0.000 (0.000)       0.560 (0.540)       0.263 (0.252)       0.254 (0.254)  9.29 h\n",
      "5314          0.500 (1.000)       0.542 (0.525)       0.254 (0.245)       0.255 (0.256)  18.6 h\n",
      "7971          1.000 (2.000)       0.533 (0.515)       0.249 (0.240)       0.251 (0.243)  1.16 d\n",
      "10628         1.500 (3.000)       0.528 (0.513)       0.246 (0.237)       0.250 (0.246)  1.55 d\n",
      "13285         2.000 (4.000)       0.524 (0.508)       0.244 (0.235)       0.250 (0.250)  1.94 d\n"
     ]
    }
   ],
   "source": [
    "def step_one(*, k, max_iteration):\n",
    "    import os\n",
    "    from PersonalizedProductRating import train_loader, dev_loader\n",
    "    from ProgressPrinter import ProgressPrinter\n",
    "    from peft import LoraConfig, TaskType\n",
    "    from TaskLLM import TaskLLM\n",
    "    from transformers import T5ForConditionalGeneration\n",
    "    import torch\n",
    "    from Util import interleave, set_directory\n",
    "\n",
    "    augment = int(os.environ.get('AUGMENT', '4'))\n",
    "\n",
    "    torch.manual_seed(2112)\n",
    "\n",
    "    train = train_loader(batch_size=8, augment=augment)\n",
    "    dev = dev_loader(batch_size=16)\n",
    "\n",
    "    t5 = T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\n",
    "    taskllm_config = LoraConfig(r=5, task_type=TaskType.SEQ_2_SEQ_LM)\n",
    "    t5.add_adapter(taskllm_config, \"taskllm\")\n",
    "    t5.enable_adapters()\n",
    "\n",
    "    taskllm = TaskLLM(t5=t5, adapter_name=\"taskllm\")\n",
    "\n",
    "    def inner_batch(func, inner_batch_size, inputs):\n",
    "        from more_itertools import chunked\n",
    "        return [ func(*ib) for ib in zip(*[ chunked(g, inner_batch_size) for g in inputs ]) ]\n",
    "\n",
    "    print(f'************ augment = {augment} *************')\n",
    "    with ProgressPrinter('iter', f'{k} loss', f'{k} MAE', f'{k} MAE (dev)') as printer:\n",
    "        cumsum = lambda z, acc=0: [0] + [ acc := acc + v for v in z ]\n",
    "        \n",
    "        for iteration in range(max_iteration):\n",
    "            for istrain, (examples, labels) in interleave(train, dev, sequential=True):\n",
    "                with torch.no_grad():\n",
    "                    texts_to_embed = [ [ text[:256]\n",
    "                                         for text in (' '.join(ex['review'].split()), ) \n",
    "                                       ] + \n",
    "                                       [ text[:256]\n",
    "                                         for v in ex['profile']\n",
    "                                         for text in (' '.join(v['text'].split()), )\n",
    "                                       ]\n",
    "                                       for ex in examples\n",
    "                                     ]\n",
    "                    embeddings = torch.cat(inner_batch(func = lambda t: dev.embed(t),\n",
    "                                                       inner_batch_size = 64 * torch.cuda.device_count(),\n",
    "                                                       inputs = (sum(texts_to_embed, []),)\n",
    "                                                      ),\n",
    "                                           dim=0)\n",
    "                    splits = cumsum(map(len, texts_to_embed))\n",
    "                    indices = [ torch.topk(embeddings[a,:] @ embeddings[a+1:b,:].T, dim=0, k=k).indices for a, b in zip(splits, splits[1:]) ]\n",
    "                    prompts = [ dev.prepend_to_prompt(ex, [ ex['profile'][ind] for ind in index.to('cpu').tolist() ])\n",
    "                                for ex, index in zip(examples, indices) ]\n",
    "                    targets = [ int(label) - 1 for label in labels ]\n",
    "                    cumul = taskllm.predict(prompts).exp().cumsum(dim=1)\n",
    "                    guesses = (cumul>=0.5).long().argmax(dim=1)\n",
    "                    targets = torch.Tensor(targets).long().to(guesses.device)\n",
    "                    mae = torch.abs(guesses - targets).float().mean().item()\n",
    "\n",
    "                loss = taskllm.learn(prompts, targets) if istrain else None\n",
    "                printer.addobs(iteration, loss, mae if istrain else None, mae if not istrain else None)\n",
    "\n",
    "            printer.print()\n",
    "            printer.autoprint = False\n",
    "            output_dir = os.environ.get('AMLT_OUTPUT_DIR', '.')\n",
    "            with set_directory(output_dir):\n",
    "                taskllm.save_pretrained(f'User_keq{k}_t5base_step1_iter{iteration}_augment{augment}')\n",
    "\n",
    "step_one(k=4, max_iteration=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf16df8-9320-4be9-98f9-da38da180a91",
   "metadata": {},
   "source": [
    "# Step 2: learn ranker using (fixed pre-finetuned) task LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58810c2b-35e5-4efe-bd43-1b5f25c16357",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_ranker(*, max_iteration, k):\n",
    "    import os\n",
    "    from RewardPredictor import RewardPredictor\n",
    "    from TaskLLM import TaskLLM\n",
    "    from PersonalizedProductRating import train_loader, dev_loader\n",
    "    from ProgressPrinter import ProgressPrinter\n",
    "    from SimpleRegret import SimpleRegretHypercubeSampler\n",
    "    from peft import LoraConfig, TaskType\n",
    "    from transformers import T5ForConditionalGeneration\n",
    "    import torch\n",
    "    from Util import interleave, set_directory, GPUMonitor\n",
    "\n",
    "    os.environ['TOKENIZERS_PARALLELISM'] = 'true'\n",
    "    step1_iter = os.environ.get('STEP1_ITER', '2_augment4')\n",
    "    augment = int(os.environ.get('AUGMENT', '1'))\n",
    "    gamma = float(os.environ.get('GAMMA', '1'))\n",
    "    output_dir = os.environ.get('AMLT_OUTPUT_DIR', '.')\n",
    "    \n",
    "    torch.manual_seed(8675309)\n",
    "\n",
    "    train = train_loader(batch_size=8, augment=augment)\n",
    "    dev = dev_loader(batch_size=16)\n",
    "\n",
    "    t5 = T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\n",
    "    t5.load_adapter(f'User_keq{k}_t5base_step1_iter{step1_iter}', 'taskllm')\n",
    "\n",
    "    rhat_config = LoraConfig(r=1, task_type=TaskType.SEQ_2_SEQ_LM)\n",
    "    t5.add_adapter(rhat_config, \"rhat\")\n",
    "    t5.enable_adapters()\n",
    "    \n",
    "    taskllm = TaskLLM(t5=t5, adapter_name=\"taskllm\")\n",
    "    rewardpredictor = RewardPredictor(t5=t5, adapter_name=\"rhat\")\n",
    "\n",
    "    gumbel = torch.distributions.gumbel.Gumbel(0,1)\n",
    "    def randomized_similarity(embeddings, nsamples):\n",
    "        scores = embeddings[0,:] @ embeddings[1:,:].T\n",
    "        temperature = scores[0].item() - scores[min(scores.shape[0], 4)].item()\n",
    "        gumbel_shape = torch.Size([nsamples, scores.shape[0]])\n",
    "        gumbels = temperature * gumbel.sample(gumbel_shape).to(scores.device)\n",
    "        return torch.unique(torch.topk(scores.unsqueeze(0) + gumbels, dim=1, k=k).indices, sorted=False, dim=0)\n",
    "\n",
    "    def inner_batch(func, inner_batch_size, inputs):\n",
    "        from more_itertools import chunked\n",
    "        return [ func(*ib) for ib in zip(*[ chunked(g, inner_batch_size) for g in inputs ]) ]\n",
    "\n",
    "    monitor = GPUMonitor(delay=60, maxcount=5)\n",
    "\n",
    "    print(f'************ augment = {augment} gamma = {gamma} step1_iter = {step1_iter} *************')\n",
    "    with ProgressPrinter('iter', f'{k} loss', f'{k} MAE', f'{k} MAE (dev)', 'samps') as printer:\n",
    "        cumsum = lambda z, acc=0: [0] + [ acc := acc + v for v in z ]\n",
    "        \n",
    "        for iteration in range(max_iteration):\n",
    "            for istrain, (examples, labels) in interleave(train, dev, sequential=True):\n",
    "                with torch.no_grad():\n",
    "                    texts_to_embed = [ [ text[:256]\n",
    "                                         for text in (' '.join(ex['review'].split()), ) \n",
    "                                       ] + \n",
    "                                       [ text[:256]\n",
    "                                         for v in ex['profile']\n",
    "                                         for text in (' '.join(v['text'].split()), )\n",
    "                                       ]\n",
    "                                       for ex in examples\n",
    "                                     ]\n",
    "                    embeddings = torch.cat(inner_batch(func = lambda t: dev.embed(t),\n",
    "                                                       inner_batch_size = 64 * torch.cuda.device_count(),\n",
    "                                                       inputs = (sum(texts_to_embed, []),)\n",
    "                                                      ),\n",
    "                                           dim=0)\n",
    "                    splits = cumsum(map(len, texts_to_embed))\n",
    "                    randos = [ randomized_similarity(embeddings[a:b,:], 64) for a, b in zip(splits, splits[1:]) ]\n",
    "                    prompts = [ [ dev.prepend_to_prompt(ex, [ ex['profile'][ind] for ind in indices ]) \n",
    "                                  for indices in rando.to('cpu').tolist() \n",
    "                                ]\n",
    "                                for ex, rando in zip(examples, randos) \n",
    "                              ]\n",
    "                    rhats = torch.cat(inner_batch(func = lambda p: rewardpredictor.predict(p),\n",
    "                                                  inner_batch_size = 64 * torch.cuda.device_count(),\n",
    "                                                  inputs = (sum(prompts, []),)\n",
    "                                                 ),\n",
    "                                      dim=0)\n",
    "                    splits = cumsum(map(len, prompts))\n",
    "                    samples = [ SimpleRegretHypercubeSampler(rhats[a:b].view(1, -1), gamma=gamma) for a, b in zip(splits, splits[1:]) ]\n",
    "                    actionind = [ [ exploit.item() ] + [ n for n, observed in enumerate(explore) if observed > 0 ]\n",
    "                                  for exploit, exploreraw in samples\n",
    "                                  for explore in (exploreraw[0].tolist() if istrain else [], )\n",
    "                                ]\n",
    "                    nsamps = [ len(aind) for aind in actionind ]\n",
    "                    guessprompts = [ [ prompt[a] for a in aind ] for prompt, aind in zip(prompts, actionind) ]\n",
    "                    cumul = torch.cat(inner_batch(func = lambda p: taskllm.predict(p).exp().cumsum(dim=1),\n",
    "                                                  inner_batch_size = 64 * torch.cuda.device_count(),\n",
    "                                                  inputs = (sum(guessprompts, []),)\n",
    "                                                 ),\n",
    "                                      dim=0)\n",
    "                    splits = cumsum(map(len, guessprompts))\n",
    "                    guesses = [ (cumul[a:b,:]>=0.5).long().argmax(dim=1) for a, b in zip(splits, splits[1:]) ]\n",
    "                    targets = [ int(label) - 1 for label in labels ]\n",
    "                    rewards = [ (1 - torch.abs((g - target)/4).float()).tolist() for g, target in zip(guesses, targets) ]\n",
    "                    greedymaes = [ torch.abs(g[0] - target).item() for g, target in zip(guesses, targets) ] \n",
    "\n",
    "                if istrain:\n",
    "                    rhatprompts = sum(guessprompts, [])\n",
    "                    rhattargets = sum(rewards, [])\n",
    "                    predlosses = inner_batch(func = lambda a, b: (len(a), rewardpredictor.learn(a, torch.Tensor([ [ r ] for r in b ]))),\n",
    "                                             inner_batch_size = 16 * torch.cuda.device_count(),\n",
    "                                             inputs = (rhatprompts, rhattargets)\n",
    "                                            )\n",
    "                    predloss = sum(n * v for n, v in predlosses) / sum(n for n, v in predlosses)\n",
    "                else:\n",
    "                    predloss = None\n",
    "\n",
    "                greedymae = torch.Tensor(greedymaes, device='cpu').float().mean().item()\n",
    "                nsamps = torch.Tensor(nsamps, device='cpu').float().mean().item() if istrain else None\n",
    "\n",
    "                printer.addobs(iteration, predloss, greedymae if istrain else None, greedymae if not istrain else None, nsamps)\n",
    "\n",
    "            printer.print()\n",
    "            printer.autoprint = False\n",
    "            with set_directory(output_dir):\n",
    "                taskllm.save_pretrained(f'User_keq{k}_t5base_step2_iter{iteration}_augment{augment}')\n",
    "                \n",
    "learn_ranker(k=4, max_iteration=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daa5c51-2bce-415a-8abf-e7ce3d0e1cbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
