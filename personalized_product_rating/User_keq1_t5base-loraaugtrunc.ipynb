{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a258105a-3b77-499a-bc5c-6e1ceb3e92e5",
   "metadata": {},
   "source": [
    "# Step 1: fine-tune LLM using top result from (fixed) ranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83f4483a-9d5d-4c65-923c-cc9e327c9498",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n              iter (since)      1 loss (since)       1 MAE (since) 1 MAE (dev) (since)      dt\n",
      "1             0.000 (0.000)       0.418 (0.418)       0.000 (0.000)       0.000 (0.000)  2.15 s\n",
      "2             0.000 (0.000)       1.283 (2.148)       0.500 (1.000)       0.000 (0.000)  2.68 s\n",
      "4             0.000 (0.000)       2.030 (2.778)       0.875 (1.250)       0.000 (0.000)  4.13 s\n",
      "8             0.000 (0.000)       1.573 (1.117)       0.812 (0.750)       0.000 (0.000)  7.97 s\n",
      "16            0.000 (0.000)       1.165 (0.756)       0.562 (0.312)       0.000 (0.000)  19.5 s\n",
      "32            0.000 (0.000)       0.827 (0.489)       0.375 (0.188)       0.000 (0.000)    34 s\n",
      "64            0.000 (0.000)       0.733 (0.638)       0.297 (0.219)       0.000 (0.000)  1.06 m\n",
      "128           0.000 (0.000)       0.699 (0.666)       0.316 (0.336)       0.000 (0.000)  2.03 m\n",
      "256           0.000 (0.000)       0.647 (0.594)       0.311 (0.305)       0.000 (0.000)  3.86 m\n",
      "512           0.000 (0.000)       0.672 (0.698)       0.339 (0.367)       0.000 (0.000)  7.77 m\n",
      "1024          0.000 (0.000)       0.657 (0.641)       0.324 (0.309)       0.000 (0.000)  15.8 m\n",
      "2048          0.000 (0.000)       0.649 (0.642)       0.312 (0.300)       0.000 (0.000)  31.4 m\n",
      "4096          0.000 (0.000)       0.633 (0.617)       0.305 (0.299)       0.000 (0.000)  1.05 h\n",
      "8192          0.000 (0.000)       0.611 (0.589)       0.291 (0.277)       0.000 (0.000)  2.11 h\n",
      "16384         0.000 (0.000)       0.594 (0.577)       0.281 (0.271)       0.000 (0.000)  4.25 h\n",
      "21250         0.000 (0.000)       0.586 (0.551)       0.277 (0.259)       0.269 (0.269)  5.45 h\n",
      "42500         0.500 (1.000)       0.566 (0.547)       0.266 (0.254)       0.270 (0.271)  10.9 h\n",
      "63750         1.000 (2.000)       0.558 (0.540)       0.261 (0.252)       0.270 (0.269)  14.1 h\n",
      "85000         1.500 (3.000)       0.550 (0.529)       0.257 (0.245)       0.268 (0.262)  17.1 h\n",
      "106250        2.000 (4.000)       0.545 (0.524)       0.254 (0.244)       0.268 (0.267)  22.5 h\n"
     ]
    }
   ],
   "source": [
    "def step_one(*, k, max_iteration):\n",
    "    from TaskLLM import TaskLLM\n",
    "    from PersonalizedProductRating import train_loader, dev_loader\n",
    "    from ProgressPrinter import ProgressPrinter\n",
    "    from peft import LoraConfig, TaskType\n",
    "    from transformers import T5ForConditionalGeneration\n",
    "    import torch\n",
    "    from Util import interleave\n",
    "    \n",
    "    device = 'cuda'\n",
    "    torch.set_default_device(device)\n",
    "    torch.manual_seed(2112)\n",
    "\n",
    "    train = train_loader(batch_size=1, augment=True)\n",
    "    dev = dev_loader(batch_size=2)\n",
    "\n",
    "    t5 = T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\n",
    "    taskllm_config = LoraConfig(r=5, task_type=TaskType.SEQ_2_SEQ_LM)\n",
    "    t5.add_adapter(taskllm_config, \"taskllm\")\n",
    "    t5.enable_adapters()\n",
    "\n",
    "    taskllm = TaskLLM(t5=t5, adapter_name=\"taskllm\")\n",
    "\n",
    "    with ProgressPrinter('iter', f'{k} loss', f'{k} MAE', f'{k} MAE (dev)') as printer:\n",
    "        with torch.no_grad():\n",
    "            probs_to_mean = torch.arange(dev.num_labels).unsqueeze(1).float().to(device)\n",
    "            \n",
    "        for iteration in range(max_iteration):\n",
    "            for istrain, (examples, labels) in interleave(train, dev, sequential=True):\n",
    "                with torch.no_grad():\n",
    "                    prompts, targets = [], []\n",
    "    \n",
    "                    for ex, label in zip(examples, labels):\n",
    "                        embeddings = dev.embed( [ text[:256]\n",
    "                                                  for text in (' '.join(ex['review'].split()), ) \n",
    "                                                ] + \n",
    "                                                [ text[:256]\n",
    "                                                  for v in ex['profile']\n",
    "                                                  for text in (' '.join(v['text'].split()), )\n",
    "                                                ])\n",
    "                        index = torch.topk(embeddings[0,:] @ embeddings[1:,:].T, dim=0, k=k).indices.to('cpu').tolist()\n",
    "                        profile_examples = [ ex['profile'][ind] for ind in index ]\n",
    "                        prompt = dev.prepend_to_prompt(ex, profile_examples)\n",
    "                        prompts.append(prompt)\n",
    "                        targets.append(int(label)-1)\n",
    "\n",
    "                    targets = torch.Tensor(targets).long().to(device)\n",
    "                    cumul = taskllm.predict(prompts).exp().cumsum(dim=1)\n",
    "                    guesses = (cumul>=0.5).long().argmax(dim=1)\n",
    "                    mae = torch.abs(guesses - targets).float().mean().item()\n",
    "    \n",
    "                loss = taskllm.learn(prompts, targets) if istrain else None\n",
    "                printer.addobs(iteration, loss, mae if istrain else None, mae if not istrain else None)\n",
    "\n",
    "            printer.print()\n",
    "            printer.autoprint = False\n",
    "            taskllm.save_pretrained(f'User_keq{k}_t5base_step1_iter{iteration}_loratruncaug')\n",
    "\n",
    "from Fork import SubProcess\n",
    "from Util import BadPipe\n",
    "with BadPipe(), SubProcess() as process: process.parent or step_one(k=1, max_iteration=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf16df8-9320-4be9-98f9-da38da180a91",
   "metadata": {},
   "source": [
    "# Step 2: learn ranker using (fixed pre-finetuned) task LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3c4dfb-cb62-4d2b-a0e7-d1c281da1c4d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def learn_ranker(*, step1_iter, rank, max_iteration):\n",
    "    from RewardPredictor import RewardPredictor\n",
    "    from TaskLLM import TaskLLM\n",
    "    from PersonalizedProductRating import train_loader, dev_loader\n",
    "    from ProgressPrinter import ProgressPrinter\n",
    "    from peft import IA3Config, TaskType\n",
    "    from transformers import T5ForConditionalGeneration\n",
    "    import torch\n",
    "    from Util import interleave\n",
    "    \n",
    "    device = 'cuda'\n",
    "    torch.set_default_device(device)\n",
    "    torch.manual_seed(8675309)\n",
    "\n",
    "    train = train_loader(batch_size=2)\n",
    "    dev = dev_loader(batch_size=2)\n",
    "\n",
    "    t5 = T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\n",
    "    t5.load_adapter(f'User_keq1_t5base_step1_iter{step1_iter}', 'taskllm')\n",
    "\n",
    "    rhat_config = IA3Config(task_type=TaskType.SEQ_2_SEQ_LM)\n",
    "    t5.add_adapter(rhat_config, \"rhat\")\n",
    "    t5.enable_adapters()\n",
    "    \n",
    "    taskllm = TaskLLM(t5=t5, adapter_name=\"taskllm\")\n",
    "    rewardpredictor = RewardPredictor(t5=t5, adapter_name=\"rhat\")\n",
    "\n",
    "    with ProgressPrinter('iter', f'{rank} loss', f'{rank} MAE', f'{rank} MAE (dev)') as printer:\n",
    "        for iteration in range(max_iteration):\n",
    "            for istrain, (examples, labels) in interleave(train, dev, sequential=True):\n",
    "                greedymaes, allloss = [], []\n",
    "                for ex, label in zip(examples, labels):\n",
    "                    with torch.no_grad():\n",
    "                        embeddings = dev.embed( [ ex['review'] ] + \n",
    "                                                [ v['text'] \n",
    "                                                  for v in ex['profile']\n",
    "                                                  if v['text'] != ex['review'] \n",
    "                                                ])\n",
    "                        effk = min(rank, embeddings.shape[0] - 1)\n",
    "                        index = torch.topk(embeddings[0,:] @ embeddings[1:,:].T, dim=0, k=effk).indices.to('cpu').tolist()\n",
    "                        prompts, rhatprompts = [], []\n",
    "                        for n, oneind in enumerate(index):\n",
    "                            profile_examples = [ ex['profile'][ind] for ind in (oneind,) ]\n",
    "                            prompt = dev.prepend_to_prompt(ex, profile_examples)\n",
    "                            prompts.append(prompt)\n",
    "                            maxlen = 256\n",
    "                            rhatprompt = '\\n'.join([ f\"Example: {text:.{maxlen}s}\\nScore: {v['score']}\" \n",
    "                                                     for ind in (oneind,)\n",
    "                                                     for v in (ex['profile'][ind],)\n",
    "                                                     for text in (' '.join(v['text'].split()),)\n",
    "                                                   ] + [ f\"Review: {ex['review']}\" ])\n",
    "                            rhatprompts.append(rhatprompt)\n",
    "                \n",
    "                        cumul = taskllm.predict(prompts).exp().cumsum(dim=1)\n",
    "                        guesses = (cumul>=0.5).long().argmax(dim=1)\n",
    "                        target = int(label) - 1\n",
    "                        rewards = 1 - torch.abs((guesses - target)/4).float().unsqueeze(1)\n",
    "                        rhats = rewardpredictor.predict(rhatprompts)\n",
    "                        greedy = torch.argmax(rhats, dim=0).item()\n",
    "                        greedymae = torch.abs(guesses[greedy] - target).item()\n",
    "                        greedymaes.append(greedymae)\n",
    "                        \n",
    "                    loss = rewardpredictor.learn(rhatprompts, rewards) if istrain else None\n",
    "                    allloss.append(loss)\n",
    "\n",
    "                greedymae = torch.Tensor(greedymaes).float().mean().item()\n",
    "                predloss = torch.Tensor(allloss).mean().item() if istrain else None\n",
    "\n",
    "                printer.addobs(iteration, predloss, greedymae if istrain else None, greedymae if not istrain else None)\n",
    "\n",
    "            printer.print()\n",
    "            printer.autoprint = False\n",
    "            rewardpredictor.save_pretrained(f'User_keq1_t5base_step2_iter{iteration}_rankeq{rank}')\n",
    "\n",
    "from Fork import SubProcess\n",
    "from Util import BadPipe\n",
    "with BadPipe(), SubProcess() as process: process.parent or learn_ranker(step1_iter=2, rank=8, max_iteration=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58810c2b-35e5-4efe-bd43-1b5f25c16357",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
