{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a258105a-3b77-499a-bc5c-6e1ceb3e92e5",
   "metadata": {},
   "source": [
    "# Step 1: fine-tune LLM using top result from (fixed) ranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f4483a-9d5d-4c65-923c-cc9e327c9498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n                 iter (since)         4 loss (since)       4 rouge1 (since) 4 rouge1 (dev) (since)      dt\n",
      "1                0.000 (0.000)          2.815 (2.815)          0.172 (0.172)          0.000 (0.000)  5.29 s\n",
      "2                0.000 (0.000)          3.177 (3.540)          0.126 (0.081)          0.000 (0.000)  9.11 s\n",
      "4                0.000 (0.000)          3.121 (3.064)          0.141 (0.156)          0.000 (0.000)  19.3 s\n",
      "8                0.000 (0.000)          3.138 (3.155)          0.148 (0.155)          0.000 (0.000)  32.1 s\n",
      "16               0.000 (0.000)          3.148 (3.158)          0.159 (0.170)          0.000 (0.000)   1.1 m\n",
      "32               0.000 (0.000)          3.141 (3.134)          0.155 (0.150)          0.000 (0.000)  2.23 m\n",
      "64               0.000 (0.000)          3.065 (2.990)          0.168 (0.182)          0.000 (0.000)  4.73 m\n",
      "128              0.000 (0.000)          3.041 (3.017)          0.170 (0.171)          0.000 (0.000)  9.46 m\n",
      "256              0.000 (0.000)          3.029 (3.018)          0.167 (0.165)          0.000 (0.000)    19 m\n",
      "512              0.000 (0.000)          2.991 (2.953)          0.174 (0.181)          0.000 (0.000)  38.3 m\n",
      "1024             0.000 (0.000)          2.960 (2.929)          0.179 (0.183)          0.000 (0.000)   1.3 h\n",
      "1687             0.000 (0.000)          2.945 (2.918)          0.180 (0.181)          0.182 (0.182)  2.11 h\n",
      "3374             0.500 (1.000)          2.914 (2.883)          0.182 (0.185)          0.185 (0.188)  4.24 h\n",
      "5061             1.000 (2.000)          2.897 (2.862)          0.183 (0.186)          0.186 (0.188)  6.39 h\n",
      "6748             1.500 (3.000)          2.886 (2.852)          0.183 (0.184)          0.185 (0.184)  8.53 h\n",
      "8435             2.000 (4.000)          2.875 (2.832)          0.184 (0.187)          0.186 (0.186)  10.7 h\n"
     ]
    }
   ],
   "source": [
    "def step_one(*, k, max_iteration):\n",
    "    import evaluate\n",
    "    from TaskLLM import TaskLLM\n",
    "    from PersonalizedNews import train_loader, dev_loader\n",
    "    from ProgressPrinter import ProgressPrinter\n",
    "    from peft import IA3Config, TaskType\n",
    "    from transformers import T5ForConditionalGeneration\n",
    "    import torch\n",
    "    from Util import interleave\n",
    "    \n",
    "    device = 'cuda'\n",
    "    torch.set_default_device(device)\n",
    "    torch.manual_seed(2112)\n",
    "\n",
    "    train = train_loader(batch_size=8, augment=True)\n",
    "    dev = dev_loader(batch_size=16)\n",
    "\n",
    "    t5 = T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\n",
    "    taskllm_config = IA3Config(task_type=TaskType.SEQ_2_SEQ_LM)\n",
    "    t5.add_adapter(taskllm_config, \"taskllm\")\n",
    "    t5.enable_adapters()\n",
    "\n",
    "    taskllm = TaskLLM(t5=t5, adapter_name=\"taskllm\")\n",
    "\n",
    "    rouge_metric = evaluate.load('rouge')\n",
    "    \n",
    "    with ProgressPrinter('iter', f'{k} loss', f'{k} rouge1', f'{k} rouge1 (dev)') as printer:\n",
    "        for iteration in range(max_iteration):\n",
    "            for istrain, (examples, labels) in interleave(train, dev, sequential=True):\n",
    "                with torch.no_grad():\n",
    "                    prompts = []\n",
    "    \n",
    "                    for ex in examples:\n",
    "                        embeddings = dev.embed( [ ex['article'] ] + \n",
    "                                                [ v['text']\n",
    "                                                  for v in ex['profile']\n",
    "                                                ])\n",
    "                        index = torch.topk(embeddings[0,:] @ embeddings[1:,:].T, dim=0, k=k).indices.to('cpu').tolist()\n",
    "                        profile_examples = [ ex['profile'][ind] for ind in index ]\n",
    "                        prompt = dev.prepend_to_prompt(ex, profile_examples)\n",
    "                        prompts.append(prompt)\n",
    "\n",
    "                    guesses = taskllm.generate(prompts)\n",
    "                    scores = rouge_metric.compute(predictions=guesses, references=labels)['rouge1']\n",
    "    \n",
    "                loss = taskllm.learn(prompts, labels) if istrain else None\n",
    "                printer.addobs(iteration, loss, scores if istrain else None, scores if not istrain else None)\n",
    "\n",
    "            printer.print()\n",
    "            printer.autoprint = False\n",
    "            taskllm.save_pretrained(f'User_keq{k}_t5base_step1_iter{iteration}')\n",
    "\n",
    "step_one(k=4, max_iteration=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41c12d7-7558-410b-a1b5-768fc86f98a7",
   "metadata": {},
   "source": [
    "# Step 2: learn ranker using (fixed pre-finetuned) task LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1f631e-df17-49c2-9cea-d39e7a60b5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_ranker(*, step1_iter, max_iteration, k):\n",
    "    import evaluate\n",
    "    from more_itertools import chunked\n",
    "    from RewardPredictor import RewardPredictor\n",
    "    from TaskLLM import TaskLLM\n",
    "    from PersonalizedNews import train_loader, dev_loader\n",
    "    from ProgressPrinter import ProgressPrinter\n",
    "    from SimpleRegret import SimpleRegretHypercubeSampler\n",
    "    from peft import IA3Config, TaskType\n",
    "    from transformers import T5ForConditionalGeneration\n",
    "    import torch\n",
    "    from Util import interleave\n",
    "    \n",
    "    device = 'cuda'\n",
    "    torch.set_default_device(device)\n",
    "    torch.manual_seed(2112)\n",
    "\n",
    "    train = train_loader(batch_size=8, augment=True)\n",
    "    dev = dev_loader(batch_size=16)\n",
    "\n",
    "    t5 = T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\n",
    "    t5.load_adapter(f'User_keq{k}_t5base_step1_iter{step1_iter}', 'taskllm')\n",
    "\n",
    "    rhat_config = IA3Config(task_type=TaskType.SEQ_2_SEQ_LM)\n",
    "    t5.add_adapter(rhat_config, \"rhat\")\n",
    "    t5.enable_adapters()\n",
    "    \n",
    "    taskllm = TaskLLM(t5=t5, adapter_name=\"taskllm\")\n",
    "    rewardpredictor = RewardPredictor(t5=t5, adapter_name=\"rhat\")\n",
    "\n",
    "    gumbel = torch.distributions.gumbel.Gumbel(0,1)\n",
    "    def randomized_similarity(ex, nsamples):\n",
    "        embeddings = dev.embed( [ ex['article'] ] + \n",
    "                                [ v['text']\n",
    "                                  for v in ex['profile']\n",
    "                                ])\n",
    "        scores = embeddings[0,:] @ embeddings[1:,:].T\n",
    "        temperature = scores[0].item() - scores[min(scores.shape[0], 4)].item()\n",
    "        gumbel_shape = torch.Size([nsamples, scores.shape[0]])\n",
    "        gumbels = temperature * gumbel.sample(gumbel_shape)\n",
    "        return torch.unique(torch.topk(scores.unsqueeze(0) + gumbels, dim=1, k=k).indices, sorted=False, dim=0).to('cpu')\n",
    "\n",
    "    rouge_metric = evaluate.load('rouge')\n",
    "\n",
    "    with ProgressPrinter('iter', f'{k} loss', f'{k} rouge1', f'{k} rouge1 (dev)', 'samps') as printer:\n",
    "        for iteration in range(max_iteration):\n",
    "            for istrain, (examples, labels) in interleave(train, dev, sequential=True):\n",
    "                greedyrewards, allloss, nsamps = [], [], []\n",
    "\n",
    "                for ex, label in zip(examples, labels):\n",
    "                    with torch.no_grad():\n",
    "                        randos = randomized_similarity(ex, 64)\n",
    "                        \n",
    "                        prompts, rhatprompts = [], []\n",
    "                        for rando in randos:\n",
    "                            profile_examples = [ ex['profile'][ind] for ind in rando ]\n",
    "                            prompt = dev.prepend_to_prompt(ex, profile_examples)\n",
    "                            prompts.append(prompt)\n",
    "                            rhatprompts.append(prompt)\n",
    "\n",
    "                        rhats = rewardpredictor.predict(rhatprompts)\n",
    "                        exploit, explore = SimpleRegretHypercubeSampler(rhats.view(1, -1), gamma=10)\n",
    "                        exploit = [ exploit.item() ]\n",
    "                        explore = explore[0].tolist() if istrain else []\n",
    "                        actionind = exploit + [ n for n, observed in enumerate(explore) if observed > 0 ]\n",
    "                        nsamps.append(len(actionind))\n",
    "                        guesses = taskllm.generate([ prompts[a] for a in actionind ])\n",
    "                        rewards = rouge_metric.compute(predictions=guesses,\n",
    "                                                       references=[label]*len(guesses),\n",
    "                                                       use_aggregator=False)['rouge1']\n",
    "                        greedyrewards.append(rewards[0])\n",
    "\n",
    "                    if istrain:\n",
    "                        inner_batch_size = 4\n",
    "                        loss = sum(\n",
    "                            len(inner_batch[0]) * \n",
    "                            rewardpredictor.learn([ rhatprompts[a] for a in inner_batch[0] ], \n",
    "                                                    torch.Tensor([ [ r ] for r in inner_batch[1] ]).to(device)\n",
    "                                                 )\n",
    "                            for inner_batch in zip(chunked(actionind, inner_batch_size), chunked(rewards, inner_batch_size))\n",
    "                        ) / len(actionind)\n",
    "                        allloss.append(loss)\n",
    "\n",
    "                greedyreward = torch.Tensor(greedyrewards).float().mean().item()\n",
    "                predloss = torch.Tensor(allloss).mean().item() if istrain else None\n",
    "                nsamps = torch.Tensor(nsamps).float().mean().item() if istrain else None\n",
    "\n",
    "                printer.addobs(iteration, predloss, greedyreward if istrain else None, greedyreward if not istrain else None, nsamps)\n",
    "\n",
    "            printer.print()\n",
    "            printer.autoprint = False\n",
    "            rewardpredictor.save_pretrained(f'User_keq{k}_t5base_step2_iter{iteration}')\n",
    "\n",
    "learn_ranker(k=4, max_iteration=12, step1_iter='1_1024')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a11da08-a961-4806-9f37-943584e8c50c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n                 iter (since)         4 loss (since)       4 rouge1 (since) 4 rouge1 (dev) (since)          samps (since)      dt\n",
      "1                0.000 (0.000)          0.421 (0.421)          0.170 (0.170)          0.000 (0.000)         46.188 (46.188)  1.11 m\n",
      "2                0.000 (0.000)          0.378 (0.335)          0.147 (0.124)          0.000 (0.000)         33.719 (21.250)  1.76 m\n",
      "4                0.000 (0.000)          0.426 (0.474)          0.187 (0.227)          0.000 (0.000)         25.375 (17.031)  2.95 m\n",
      "8                0.000 (0.000)          0.452 (0.479)          0.187 (0.188)          0.000 (0.000)         19.922 (14.469)  4.96 m\n",
      "16               0.000 (0.000)          0.473 (0.494)          0.194 (0.201)          0.000 (0.000)         15.938 (11.953)  8.88 m\n",
      "32               0.000 (0.000)          0.471 (0.468)          0.189 (0.184)          0.000 (0.000)         12.684 (9.430)    16 m\n",
      "64               0.000 (0.000)          0.476 (0.481)          0.193 (0.197)          0.000 (0.000)         10.066 (7.449)  29.1 m\n",
      "128              0.000 (0.000)          0.465 (0.454)          0.190 (0.186)          0.000 (0.000)          8.216 (6.365)  53.3 m\n",
      "256              0.000 (0.000)          0.457 (0.450)          0.185 (0.181)          0.000 (0.000)          7.188 (6.159)  1.69 h\n",
      "512              0.000 (0.000)          0.458 (0.458)          0.189 (0.192)          0.000 (0.000)          6.798 (6.409)  3.31 h\n",
      "1024             0.000 (0.000)          0.457 (0.455)          0.189 (0.190)          0.000 (0.000)          6.345 (5.891)   6.5 h\n",
      "1687             0.000 (0.000)          0.455 (0.452)          0.189 (0.188)          0.191 (0.191)          6.197 (5.917)  10.4 h\n",
      "3374             0.500 (1.000)          0.454 (0.452)          0.189 (0.190)          0.190 (0.189)          6.186 (6.174)  20.7 h\n",
      "5061             1.000 (2.000)          0.452 (0.449)          0.190 (0.190)          0.190 (0.189)          6.190 (6.199)  1.29 d\n",
      "6748             1.500 (3.000)          0.450 (0.446)          0.189 (0.187)          0.189 (0.186)          6.172 (6.119)  1.72 d\n",
      "8435             2.000 (4.000)          0.450 (0.449)          0.189 (0.191)          0.189 (0.192)          6.123 (5.929)  2.15 d\n",
      "10122            2.500 (5.000)          0.450 (0.448)          0.190 (0.190)          0.190 (0.191)          6.078 (5.850)  2.57 d\n",
      "11809            3.000 (6.000)          0.449 (0.448)          0.190 (0.191)          0.190 (0.191)          6.057 (5.935)     3 d\n",
      "12189            3.125 (7.000)          0.449 (0.449)          0.190 (0.191)          0.190 (0.000)          6.055 (5.978)   3.1 d\n"
     ]
    }
   ],
   "source": [
    "def learn_ranker(*, step1_iter, max_iteration, k):\n",
    "    import evaluate\n",
    "    from more_itertools import chunked\n",
    "    from RewardPredictor import RewardPredictor\n",
    "    from TaskLLM import TaskLLM\n",
    "    from PersonalizedNews import train_loader, dev_loader\n",
    "    from ProgressPrinter import ProgressPrinter\n",
    "    from SimpleRegret import SimpleRegretHypercubeSampler\n",
    "    from peft import IA3Config, TaskType\n",
    "    from transformers import T5ForConditionalGeneration\n",
    "    import torch\n",
    "    from Util import interleave\n",
    "    \n",
    "    device = 'cuda'\n",
    "    torch.set_default_device(device)\n",
    "    torch.manual_seed(2112)\n",
    "\n",
    "    train = train_loader(batch_size=8, augment=True)\n",
    "    dev = dev_loader(batch_size=16)\n",
    "\n",
    "    t5 = T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\n",
    "    t5.load_adapter(f'User_keq{k}_t5base_step1_iter{step1_iter}', 'taskllm')\n",
    "\n",
    "    rhat_config = IA3Config(task_type=TaskType.SEQ_2_SEQ_LM)\n",
    "    t5.add_adapter(rhat_config, \"rhat\")\n",
    "    t5.enable_adapters()\n",
    "    \n",
    "    taskllm = TaskLLM(t5=t5, adapter_name=\"taskllm\")\n",
    "    rewardpredictor = RewardPredictor(t5=t5, adapter_name=\"rhat\")\n",
    "\n",
    "    gumbel = torch.distributions.gumbel.Gumbel(0,1)\n",
    "    def randomized_similarity(ex, nsamples):\n",
    "        embeddings = dev.embed( [ ex['article'] ] + \n",
    "                                [ v['text']\n",
    "                                  for v in ex['profile']\n",
    "                                ])\n",
    "        scores = embeddings[0,:] @ embeddings[1:,:].T\n",
    "        temperature = scores[0].item() - scores[min(scores.shape[0], 4)].item()\n",
    "        gumbel_shape = torch.Size([nsamples, scores.shape[0]])\n",
    "        gumbels = temperature * gumbel.sample(gumbel_shape)\n",
    "        return torch.unique(torch.topk(scores.unsqueeze(0) + gumbels, dim=1, k=k).indices, sorted=False, dim=0).to('cpu')\n",
    "\n",
    "    rouge_metric = evaluate.load('rouge')\n",
    "\n",
    "    with ProgressPrinter('iter', f'{k} loss', f'{k} rouge1', f'{k} rouge1 (dev)', 'samps') as printer:\n",
    "        for iteration in range(max_iteration):\n",
    "            for istrain, (examples, labels) in interleave(train, dev, sequential=True):\n",
    "                greedyrewards, allloss, nsamps = [], [], []\n",
    "\n",
    "                for ex, label in zip(examples, labels):\n",
    "                    with torch.no_grad():\n",
    "                        randos = randomized_similarity(ex, 64)\n",
    "                        \n",
    "                        prompts, rhatprompts = [], []\n",
    "                        for rando in randos:\n",
    "                            profile_examples = [ ex['profile'][ind] for ind in rando ]\n",
    "                            prompt = dev.prepend_to_prompt(ex, profile_examples)\n",
    "                            prompts.append(prompt)\n",
    "                            rhatprompts.append(prompt)\n",
    "\n",
    "                        rhats = rewardpredictor.predict(rhatprompts)\n",
    "                        exploit, explore = SimpleRegretHypercubeSampler(rhats.view(1, -1), gamma=10)\n",
    "                        exploit = [ exploit.item() ]\n",
    "                        explore = explore[0].tolist() if istrain else []\n",
    "                        actionind = exploit + [ n for n, observed in enumerate(explore) if observed > 0 ]\n",
    "                        nsamps.append(len(actionind))\n",
    "                        guesses = taskllm.generate([ prompts[a] for a in actionind ])\n",
    "                        rewards = rouge_metric.compute(predictions=guesses,\n",
    "                                                       references=[label]*len(guesses),\n",
    "                                                       use_aggregator=False)['rouge1']\n",
    "                        greedyrewards.append(rewards[0])\n",
    "\n",
    "                    if istrain:\n",
    "                        inner_batch_size = 4\n",
    "                        loss = sum(\n",
    "                            len(inner_batch[0]) * \n",
    "                            rewardpredictor.learn([ rhatprompts[a] for a in inner_batch[0] ], \n",
    "                                                    torch.Tensor([ [ r ] for r in inner_batch[1] ]).to(device)\n",
    "                                                 )\n",
    "                            for inner_batch in zip(chunked(actionind, inner_batch_size), chunked(rewards, inner_batch_size))\n",
    "                        ) / len(actionind)\n",
    "                        allloss.append(loss)\n",
    "\n",
    "                greedyreward = torch.Tensor(greedyrewards).float().mean().item()\n",
    "                predloss = torch.Tensor(allloss).mean().item() if istrain else None\n",
    "                nsamps = torch.Tensor(nsamps).float().mean().item() if istrain else None\n",
    "\n",
    "                printer.addobs(iteration, predloss, greedyreward if istrain else None, greedyreward if not istrain else None, nsamps)\n",
    "\n",
    "            printer.print()\n",
    "            printer.autoprint = False\n",
    "            rewardpredictor.save_pretrained(f'User_keq{k}_t5base_step2_iter{iteration}')\n",
    "\n",
    "learn_ranker(k=4, max_iteration=12, step1_iter=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac86343e-22c9-46af-b586-ab10a5327bf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
