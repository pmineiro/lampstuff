{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e7966cd-9634-4bf7-b7bc-0370fdcfad59",
   "metadata": {},
   "source": [
    "# Step 1 Dev Set Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "178c2d34-2718-422e-b179-6e6c7ada2aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** step1_iter: 0_augment2 ***\n",
      "n       4 rouge (dev) (since)      dt\n",
      "1               0.119 (0.119)  11.7 s\n",
      "2               0.181 (0.243)  19.1 s\n",
      "4               0.205 (0.229)  37.9 s\n",
      "8               0.211 (0.218)  1.18 m\n",
      "16              0.225 (0.239)   2.2 m\n",
      "32              0.217 (0.208)  4.49 m\n",
      "64              0.215 (0.214)   9.1 m\n",
      "128             0.223 (0.230)  18.7 m\n",
      "241             0.225 (0.227)  34.5 m\n"
     ]
    }
   ],
   "source": [
    "def step1_dev_set_labels(*, step1_iter, k):\n",
    "    import evaluate\n",
    "    import json\n",
    "    from RewardPredictor import RewardPredictor\n",
    "    from TaskLLM import TaskLLM\n",
    "    from PersonalizedNews import dev_loader\n",
    "    from ProgressPrinter import ProgressPrinter\n",
    "    from transformers import T5ForConditionalGeneration\n",
    "    import torch\n",
    "    from Util import interleave\n",
    "    \n",
    "    device = 'cuda'\n",
    "    torch.set_default_device(device)\n",
    "    torch.manual_seed(8675309)\n",
    "\n",
    "    dev = dev_loader(batch_size=8)\n",
    "\n",
    "    t5 = T5ForConditionalGeneration.from_pretrained('google/flan-t5-xxl', load_in_8bit=True)\n",
    "    taskllm_model_id = f'User_keq{k}_t5xxl_step1_iter{step1_iter}'\n",
    "    t5.load_adapter(taskllm_model_id, 'raw_taskllm')\n",
    "    t5.load_adapter(taskllm_model_id, 'ema_taskllm')\n",
    "    \n",
    "    taskllm = TaskLLM(t5=t5, adapter_suffix=\"taskllm\")\n",
    "    rouge_metric = evaluate.load('rouge')\n",
    "    gradfree_batch_size = 128\n",
    "\n",
    "    def inner_batch(func, inner_batch_size, inputs):\n",
    "        from more_itertools import chunked\n",
    "        return [ func(*ib) for ib in zip(*[ chunked(g, inner_batch_size) for g in inputs ]) ]\n",
    "\n",
    "    print(f'*** step1_iter: {step1_iter} ***')\n",
    "\n",
    "    devgolds = []\n",
    "    with ProgressPrinter(f'{k} rouge (dev)') as printer:\n",
    "        cumsum = lambda z, acc=0: [0] + [ acc := acc + v for v in z ]\n",
    "\n",
    "        for examples, labels in dev:\n",
    "            with torch.no_grad():\n",
    "                texts_to_embed = [ [ text[:256]\n",
    "                                     for text in (' '.join(ex['article'].split()), )\n",
    "                                   ] +\n",
    "                                   [ text[:256]\n",
    "                                     for v in ex['profile']\n",
    "                                     for text in (' '.join(v['text'].split()), )\n",
    "                                   ]\n",
    "                                   for ex in examples\n",
    "                                 ]\n",
    "                embeddings = torch.cat(inner_batch(func = dev.embed,\n",
    "                                                   inner_batch_size = gradfree_batch_size,\n",
    "                                                   inputs = (sum(texts_to_embed, []),)\n",
    "                                                  ),\n",
    "                                       dim=0)\n",
    "                splits = cumsum(map(len, texts_to_embed))\n",
    "                indices = [ torch.topk(embeddings[a,:] @ embeddings[a+1:b,:].T, dim=0, k=k).indices for a, b in zip(splits, splits[1:]) ]\n",
    "                prompts = [ dev.prepend_to_prompt(ex, [ ex['profile'][ind] for ind in index.to('cpu').tolist() ])\n",
    "                            for ex, index in zip(examples, indices) ]\n",
    "                guesses = taskllm.generate(prompts)\n",
    "                scores = rouge_metric.compute(predictions=guesses, references=labels)['rouge1']\n",
    "                \n",
    "                for ex, guess in zip(examples, guesses):\n",
    "                    devgolds.append({ 'id': ex['id'], 'output': guess })\n",
    "\n",
    "            printer.addobs(scores)\n",
    "\n",
    "    with open(f'lamp4u_xxl_step1_dev_golds.json', 'w') as jsonfile:\n",
    "        json.dump({ 'task': 'LaMP_4', 'golds': devgolds }, jsonfile)\n",
    "            \n",
    "step1_dev_set_labels(k=4, step1_iter='0_augment2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df32db8-104b-4618-bafe-70a5cfb912aa",
   "metadata": {},
   "source": [
    "Note: The step 2 dev golds are generated when preparing the submission files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a75692-2dd1-43c7-b34e-ef99ed8cf1c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
