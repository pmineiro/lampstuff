{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49428069-474c-4ee9-825d-c73408767f9d",
   "metadata": {},
   "source": [
    "# Step 1 Dev Set Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18e96eb-73e6-41c0-be82-ac9e8f307193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** step1_iter: 0_augment2 ***\n",
      "n       4 acc (dev) (since)      dt\n",
      "1             1.000 (1.000)  2.14 s\n",
      "2             0.750 (0.500)  2.79 s\n",
      "4             0.750 (0.750)  4.38 s\n",
      "8             0.812 (0.875)  7.34 s\n",
      "16            0.844 (0.875)  12.9 s\n",
      "32            0.891 (0.938)    24 s\n",
      "64            0.898 (0.906)  45.6 s\n",
      "128           0.891 (0.883)  1.47 m\n",
      "256           0.863 (0.836)   2.9 m\n",
      "512           0.864 (0.865)  5.78 m\n",
      "526           0.866 (0.929)  5.95 m\n"
     ]
    }
   ],
   "source": [
    "def step1_dev_set_labels(*, step1_iter, k):\n",
    "    import json\n",
    "    from RewardPredictor import RewardPredictor\n",
    "    from TaskLLM import TaskLLM\n",
    "    from peft import prepare_model_for_kbit_training\n",
    "    from PersonalizedNewsCat import dev_loader\n",
    "    from ProgressPrinter import ProgressPrinter\n",
    "    from transformers import T5ForConditionalGeneration\n",
    "    import torch\n",
    "    import warnings\n",
    "    \n",
    "    device = 'cuda'\n",
    "    torch.set_default_device(device)\n",
    "    torch.manual_seed(8675309)\n",
    "\n",
    "    dev = dev_loader(batch_size=2)\n",
    "\n",
    "    t5 = prepare_model_for_kbit_training(T5ForConditionalGeneration.from_pretrained('google/flan-t5-xxl', load_in_8bit=True))\n",
    "    taskllm_model_id = f'User_keq{k}_t5xxl_step1_iter{step1_iter}'\n",
    "    t5.load_adapter(taskllm_model_id, 'raw_taskllm')\n",
    "    t5.load_adapter(taskllm_model_id, 'ema_taskllm')\n",
    "    \n",
    "    taskllm = TaskLLM(t5=t5, adapter_suffix=\"taskllm\", model_id=taskllm_model_id, choices=dev.choices)\n",
    "\n",
    "    def inner_batch(func, inner_batch_size, inputs):\n",
    "        from more_itertools import chunked\n",
    "        return [ func(*ib) for ib in zip(*[ chunked(g, inner_batch_size) for g in inputs ]) ]\n",
    "\n",
    "    def make_prior(profile):\n",
    "        from math import log\n",
    "\n",
    "        c = [1]*len(dev.choices)\n",
    "        for v in profile:\n",
    "            c[dev.choices.index(v['category'])] += 1\n",
    "        n = sum(c)\n",
    "\n",
    "        return [ log(cnt) - log(n) for cnt in c ]\n",
    "    \n",
    "    print(f'*** step1_iter: {step1_iter} ***')\n",
    "\n",
    "    devgolds = []\n",
    "    with ProgressPrinter(f'{k} acc (dev)') as printer, warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", \".*MatMul8bitLt:.*\")\n",
    "        cumsum = lambda z, acc=0: [0] + [ acc := acc + v for v in z ]\n",
    "\n",
    "        for examples, labels in dev:\n",
    "            with torch.no_grad():\n",
    "                prior = torch.Tensor([ make_prior(ex['profile']) for ex in examples ]).to(device)\n",
    "                texts_to_embed = [ [ text[:256]\n",
    "                                     for text in (' '.join(ex['article'].split()), )\n",
    "                                   ] +\n",
    "                                   [ text[:256]\n",
    "                                     for v in ex['profile']\n",
    "                                     for text in (' '.join(v['text'].split()), )\n",
    "                                   ]\n",
    "                                   for ex in examples\n",
    "                                 ]\n",
    "                embeddings = torch.cat(inner_batch(func = dev.embed,\n",
    "                                                   inner_batch_size = 128,\n",
    "                                                   inputs = (sum(texts_to_embed, []),)\n",
    "                                                  ),\n",
    "                                       dim=0)\n",
    "                splits = cumsum(map(len, texts_to_embed))\n",
    "                indices = [ torch.topk(embeddings[a,:] @ embeddings[a+1:b,:].T, dim=0, k=safek).indices\n",
    "                            for a, b in zip(splits, splits[1:])\n",
    "                            for safek in (max(0, min(k, b-a-1)),)\n",
    "                          ]\n",
    "                prompts = [ dev.prepend_to_prompt(ex, [ ex['profile'][ind] for ind in index.to('cpu').tolist() ])\n",
    "                            for ex, index in zip(examples, indices) ]\n",
    "                targets = [ dev.choices.index(label) for label in labels ]\n",
    "                guesses = taskllm.predict(prompts, prior=prior).argmax(dim=1)\n",
    "                targets = torch.Tensor(targets).long().to(guesses.device)\n",
    "                emaacc = (guesses == targets).float().mean().item()\n",
    "\n",
    "                for ex, guess in zip(examples, guesses):\n",
    "                    devgolds.append({ 'id': ex['id'], 'output': dev.choices[guess] })\n",
    "\n",
    "            printer.addobs(emaacc)\n",
    "\n",
    "    with open(f'lamp2u_xxl_step1_dev_golds.json', 'w') as jsonfile:\n",
    "        json.dump({ 'task': 'LaMP_2', 'golds': devgolds }, jsonfile)\n",
    "            \n",
    "step1_dev_set_labels(k=4, step1_iter='0_augment2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19b04cb-41dc-4ad3-b43a-662a2822ce4c",
   "metadata": {},
   "source": [
    "# Step 2 Dev Set Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfe04c6-5858-4130-a12c-308610861567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** step1_iter: 0_augment2 step2_iter: 0_augment2 ***\n",
      "n       4 acc (dev) (since)      dt\n",
      "1             1.000 (1.000)  10.3 s\n",
      "2             0.750 (0.500)  21.1 s\n",
      "4             0.750 (0.750)  44.7 s\n",
      "8             0.875 (1.000)  1.32 m\n",
      "16            0.844 (0.812)  2.28 m\n",
      "32            0.859 (0.875)  4.43 m\n",
      "64            0.875 (0.891)  8.55 m\n",
      "128           0.879 (0.883)  17.2 m\n",
      "256           0.854 (0.828)  35.1 m\n",
      "512           0.862 (0.871)  1.18 h\n",
      "526           0.864 (0.929)  1.21 h\n"
     ]
    }
   ],
   "source": [
    "def step2_dev_set_labels(*, step2_iter, step1_iter, k):\n",
    "    import json\n",
    "    from peft import prepare_model_for_kbit_training\n",
    "    from RewardPredictor import RewardPredictor\n",
    "    from TaskLLM import TaskLLM\n",
    "    from PersonalizedNewsCat import dev_loader\n",
    "    from ProgressPrinter import ProgressPrinter\n",
    "    from transformers import T5ForConditionalGeneration\n",
    "    import torch\n",
    "    import warnings\n",
    "    \n",
    "    device = 'cuda'\n",
    "    torch.set_default_device(device)\n",
    "    torch.manual_seed(8675309)\n",
    "\n",
    "    dev = dev_loader(batch_size=2)\n",
    "\n",
    "    t5 = prepare_model_for_kbit_training(T5ForConditionalGeneration.from_pretrained('google/flan-t5-xxl', load_in_8bit=True))\n",
    "    taskllm_model_id = f'User_keq{k}_t5xxl_step1_iter{step1_iter}'\n",
    "    t5.load_adapter(taskllm_model_id, 'raw_taskllm')\n",
    "    t5.load_adapter(taskllm_model_id, 'ema_taskllm')\n",
    "    rhat_model_id = f'User_keq{k}_t5xxl_step2_iter{step2_iter}'\n",
    "    t5.load_adapter(rhat_model_id, 'raw_rhat')\n",
    "    t5.load_adapter(rhat_model_id, 'ema_rhat')\n",
    "    t5.enable_adapters()\n",
    "    \n",
    "    taskllm = TaskLLM(t5=t5, adapter_suffix=\"taskllm\", model_id=taskllm_model_id, choices=dev.choices)\n",
    "    rewardpredictor = RewardPredictor(t5=t5, adapter_suffix=\"rhat\", model_id=rhat_model_id)\n",
    "\n",
    "    gumbel = torch.distributions.gumbel.Gumbel(0,1)\n",
    "    def randomized_similarity(embeddings, nsamples):\n",
    "        scores = embeddings[0,:] @ embeddings[1:,:].T\n",
    "        temperature = scores[0].item() - scores[min(scores.shape[0]-1, 4)].item()\n",
    "        gumbel_shape = torch.Size([nsamples, scores.shape[0]])\n",
    "        gumbels = temperature * gumbel.sample(gumbel_shape).to(scores.device)\n",
    "        safek = min(k, scores.shape[0])\n",
    "        return torch.unique(torch.topk(scores.unsqueeze(0) + gumbels, dim=1, k=safek).indices, sorted=False, dim=0)\n",
    "\n",
    "    def inner_batch(func, inner_batch_size, inputs):\n",
    "        from more_itertools import chunked\n",
    "        return [ func(*ib) for ib in zip(*[ chunked(g, inner_batch_size) for g in inputs ]) ]\n",
    "\n",
    "    def make_prior(profile):\n",
    "        from math import log\n",
    "\n",
    "        c = [1]*len(dev.choices)\n",
    "        for v in profile:\n",
    "            c[dev.choices.index(v['category'])] += 1\n",
    "        n = sum(c)\n",
    "\n",
    "        return [ log(cnt) - log(n) for cnt in c ]\n",
    "    \n",
    "    print(f'*** step1_iter: {step1_iter} step2_iter: {step2_iter} ***')\n",
    "    nvoters = 1\n",
    "\n",
    "    devgolds = []\n",
    "    with ProgressPrinter(f'{k} acc (dev)') as printer, warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", \".*MatMul8bitLt:.*\")\n",
    "        cumsum = lambda z, acc=0: [0] + [ acc := acc + v for v in z ]\n",
    "\n",
    "        for examples, labels in dev:\n",
    "            with torch.no_grad():\n",
    "                prior = [ make_prior(ex['profile']) for ex in examples ]\n",
    "                texts_to_embed = [ [ text[:256]\n",
    "                                     for text in (' '.join(ex['article'].split()), )\n",
    "                                   ] +\n",
    "                                   [ text[:256]\n",
    "                                     for v in ex['profile']\n",
    "                                     for text in (' '.join(v['text'].split()), )\n",
    "                                   ]\n",
    "                                   for ex in examples\n",
    "                                 ]\n",
    "                embeddings = torch.cat(inner_batch(func = dev.embed,\n",
    "                                                   inner_batch_size = 128,\n",
    "                                                   inputs = (sum(texts_to_embed, []),)\n",
    "                                                  ),\n",
    "                                       dim=0)\n",
    "                splits = cumsum(map(len, texts_to_embed))\n",
    "                randos = [ randomized_similarity(embeddings[a:b,:], 64) for a, b in zip(splits, splits[1:]) ]\n",
    "                prompts = [ [ dev.prepend_to_prompt(ex, [ ex['profile'][ind] for ind in indices ])\n",
    "                              for indices in rando.to('cpu').tolist()\n",
    "                            ]\n",
    "                            for ex, rando in zip(examples, randos)\n",
    "                          ]\n",
    "                rhats = torch.cat(inner_batch(func = rewardpredictor.predict,\n",
    "                                              inner_batch_size = 128,\n",
    "                                              inputs = (sum(prompts, []),)\n",
    "                                             ),\n",
    "                                  dim=0)\n",
    "                splits = cumsum(map(len, prompts))\n",
    "                votingprompts = [ [ prompt[v] for v in torch.topk(rhats[a:b].view(-1), k=min(nvoters, b-a)).indices.to('cpu').tolist() ]\n",
    "                                    for a, b, prompt in zip(splits, splits[1:], prompts)\n",
    "                                ]\n",
    "                votingpriors = [ [q]*min(nvoters, b-a) for a, b, q in zip(splits, splits[1:], prior) ]\n",
    "                predicts = torch.cat(inner_batch(func = lambda p, q: taskllm.predict(p, prior=torch.Tensor(q).to(device)),\n",
    "                                                 inner_batch_size = 128,\n",
    "                                                 inputs = (sum(votingprompts, []), sum(votingpriors, [])),\n",
    "                                                ),\n",
    "                                     dim=0)\n",
    "                splits = cumsum(map(len, votingprompts))\n",
    "                guesses = torch.cat([ predicts[a:b,:].logsumexp(dim=0, keepdim=True).argmax(dim=1)\n",
    "                                      for a, b in zip(splits, splits[1:])\n",
    "                                    ],\n",
    "                                    dim=0)\n",
    "\n",
    "                targets = [ dev.choices.index(label) for label in labels ]\n",
    "                targets = torch.Tensor(targets).long().to(guesses.device)\n",
    "                acc = (guesses == targets).float().mean().item()\n",
    "\n",
    "                for ex, guess in zip(examples, guesses):\n",
    "                    devgolds.append({ 'id': ex['id'], 'output': dev.choices[guess] })\n",
    "\n",
    "            printer.addobs(acc)\n",
    "\n",
    "    with open(f'lamp2u_xxl_step2_dev_golds.json', 'w') as jsonfile:\n",
    "        json.dump({ 'task': 'LaMP_2', 'golds': devgolds }, jsonfile)\n",
    "            \n",
    "step2_dev_set_labels(k=4, step1_iter='0_augment2', step2_iter='0_augment2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40943e7-ea94-4ac9-9179-914785ee3ec0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
