{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a258105a-3b77-499a-bc5c-6e1ceb3e92e5",
   "metadata": {},
   "source": [
    "# Step 1: fine-tune LLM using top result from (fixed) ranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83f4483a-9d5d-4c65-923c-cc9e327c9498",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n                  iter       since      1 loss       since       1 acc       since 1 acc (dev)       since      dt (s)\n",
      "1                     0           0       0.708       0.708           0           0           0           0       0.946\n",
      "2                     0           0        0.69       0.673         0.5           1           0           0        1.42\n",
      "4                     0           0        0.68       0.661       0.667           1           1           1        1.95\n",
      "8                     0           0       0.693       0.703       0.571         0.5           1           0        3.29\n",
      "16                    0           0       0.696         0.7       0.577       0.583         0.5        0.25         5.6\n",
      "32                    0           0       0.694       0.693       0.538         0.5        0.75           1        10.7\n",
      "64                    0           0        0.69       0.686       0.588        0.64       0.654       0.571        20.4\n",
      "128                   0           0       0.687       0.684       0.578       0.569       0.538       0.423        40.5\n",
      "256                   0           0       0.667       0.647       0.635       0.691       0.654       0.769        79.8\n",
      "512                   0           0       0.656       0.644       0.641       0.648       0.676       0.698         159\n",
      "1024                  0           0       0.648       0.641       0.644       0.646       0.643        0.61         317\n",
      "2048                  0           0       0.631       0.614       0.652       0.661       0.643       0.643         631\n",
      "4096                  0           0        0.61       0.589       0.665       0.678        0.66       0.676    1.27e+03\n",
      "6090                  0           0       0.608       0.602       0.663       0.659       0.662       0.666    1.88e+03\n",
      "7339               0.17           1       0.608           0       0.663           0       0.669       0.677     2.1e+03\n"
     ]
    }
   ],
   "source": [
    "def step_one():\n",
    "    from TaskLLM import PeftTaskLLM\n",
    "    from PersonalizedCitation import train_loader, dev_loader\n",
    "    from ProgressPrinter import ProgressPrinter\n",
    "    from peft import IA3Config, TaskType, prepare_model_for_kbit_training\n",
    "    from transformers import T5ForConditionalGeneration\n",
    "    import torch\n",
    "    \n",
    "    device = 'cuda'\n",
    "    torch.set_default_device(device)\n",
    "    torch.manual_seed(2112)\n",
    "\n",
    "    train = train_loader(batch_size=2)\n",
    "    dev = dev_loader(batch_size=2)\n",
    "\n",
    "    def interleave(a, b):\n",
    "        from math import inf\n",
    "        \n",
    "        atot, btot = a.num_examples, b.num_examples\n",
    "        aiter, biter = a.__iter__(), b.__iter__()\n",
    "        aelem, belem = next(aiter), next(biter)\n",
    "        anum, bnum = 1, 1\n",
    "\n",
    "        while anum != inf and bnum != inf:\n",
    "            if anum * btot <= bnum * atot:\n",
    "                yield (True, aelem)\n",
    "                try:\n",
    "                    aelem = next(aiter)\n",
    "                    anum += 1\n",
    "                except StopIteration:\n",
    "                    anum = inf\n",
    "            else:\n",
    "                yield (False, belem)\n",
    "                try:\n",
    "                    belem = next(biter)\n",
    "                    bnum += 1\n",
    "                except StopIteration:\n",
    "                    bnum = inf\n",
    "\n",
    "    peft_config = IA3Config(task_type=TaskType.SEQ_2_SEQ_LM)\n",
    "    t5 = T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\n",
    "    taskllm = PeftTaskLLM(peft_config, t5=t5)\n",
    "\n",
    "    k = 1\n",
    "    with ProgressPrinter('iter', f'{k} loss', f'{k} acc', f'{k} acc (dev)') as printer:\n",
    "        for iteration in range(2):\n",
    "            for istrain, (examples, labels) in interleave(train, dev):\n",
    "                if iteration == 0 or not istrain:\n",
    "                    with torch.no_grad():\n",
    "                        inputs = []\n",
    "                        target = []\n",
    "        \n",
    "                        for ex, label in zip(examples, labels):\n",
    "                            embeddings = train.embed( [ ex['ref1'], ex['ref2'] ] + \n",
    "                                                      [ v['title'] \n",
    "                                                       for v in ex['profile']\n",
    "                                                       if v['title'] != ex['title'] \n",
    "                                                     ])\n",
    "                            scores = torch.max(embeddings[[0,1],:] @ embeddings[2:,:].T, dim=0).values\n",
    "                            index = torch.topk(scores, dim=0, k=k).indices.to('cpu')\n",
    "                            for n, oneind in enumerate(index.tolist()):\n",
    "                                titles = [ f'{ex[\"profile\"][ind][\"title\"]}' for ind in (oneind,) ]\n",
    "                                concat_titles = ' and '.join([f'\"{v}\"' for v in titles])\n",
    "                                input = train.append_to_title(ex, concat_titles)\n",
    "                                inputs.append(input)\n",
    "                                target.append(int(label == train.choices[1]))\n",
    "\n",
    "                        target = torch.Tensor(target).long().to(device)\n",
    "                        acc = (taskllm.predict(inputs, augment=train.swap_refs).argmax(dim=1) == target).float().mean().item()\n",
    "    \n",
    "                    loss = taskllm.learn(inputs, target, augment=train.swap_refs) if istrain else None\n",
    "                    printer.addobs(iteration, loss, acc if istrain else None, acc if not istrain else None)\n",
    "\n",
    "            printer.print()\n",
    "            printer.autoprint = False\n",
    "            if iteration == 0:\n",
    "                taskllm.save_pretrained('User_keq1_t5base_step1')\n",
    "\n",
    "from Fork import SubProcess\n",
    "with SubProcess() as process: process.parent or step_one()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf16df8-9320-4be9-98f9-da38da180a91",
   "metadata": {},
   "source": [
    "# Step 2: learn ranker using (fixed pre-finetuned) task LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a3c4dfb-cb62-4d2b-a0e7-d1c281da1c4d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n                  iter       since      8 loss       since       8 acc       since 8 acc (dev)       since      dt (s)\n",
      "1                     0           0        0.71        0.71           1           1           0           0        1.23\n",
      "2                     0           0       0.694       0.678         0.5           0           0           0        1.91\n",
      "4                     0           0       0.683        0.66       0.667           1           1           1        2.98\n",
      "8                     0           0       0.677       0.673       0.857           1           1           0        5.56\n",
      "16                    0           0       0.671       0.664       0.769       0.667       0.667         0.5        10.2\n",
      "32                    0           0       0.684       0.697       0.769       0.769       0.667       0.667        20.1\n",
      "64                    0           0       0.665       0.645       0.706        0.64       0.692       0.714        39.6\n",
      "128                   0           0       0.658       0.652       0.618       0.529       0.577       0.462        78.6\n",
      "256                   0           0       0.654        0.65       0.627       0.637       0.577       0.577         156\n",
      "512                   0           0       0.652        0.65       0.663         0.7       0.552       0.528         310\n",
      "1024                  0           0       0.648       0.643       0.673       0.683       0.595       0.638         617\n",
      "2048                  0           0       0.649        0.65       0.696       0.719       0.636       0.676    1.24e+03\n",
      "4096                  0           0       0.643       0.637       0.711       0.725       0.654       0.671    2.45e+03\n",
      "6090                  0           0       0.641       0.636       0.714        0.72       0.677       0.726    3.61e+03\n",
      "7339               0.17           1       0.641           0       0.714           0       0.692       0.707    4.11e+03\n"
     ]
    }
   ],
   "source": [
    "def learn_ranker(rank):\n",
    "    from RewardPredictor import PeftRewardPredictor\n",
    "    from TaskLLM import PeftTaskLLM\n",
    "    from T5 import PeftT5Classifier\n",
    "    from PersonalizedCitation import train_loader, dev_loader\n",
    "    from ProgressPrinter import ProgressPrinter\n",
    "    from peft import PeftConfig, IA3Config, TaskType\n",
    "    from transformers import T5ForConditionalGeneration\n",
    "    import torch\n",
    "    \n",
    "    device = 'cuda'\n",
    "    torch.set_default_device(device)\n",
    "    torch.manual_seed(2112)\n",
    "\n",
    "    train = train_loader(batch_size=2)\n",
    "    dev = dev_loader(batch_size=2)\n",
    "\n",
    "    def interleave(a, b):\n",
    "        from math import inf\n",
    "        \n",
    "        atot, btot = a.num_examples, b.num_examples\n",
    "        aiter, biter = a.__iter__(), b.__iter__()\n",
    "        aelem, belem = next(aiter), next(biter)\n",
    "        anum, bnum = 1, 1\n",
    "\n",
    "        while anum != inf and bnum != inf:\n",
    "            if anum * btot <= bnum * atot:\n",
    "                yield (True, aelem)\n",
    "                try:\n",
    "                    aelem = next(aiter)\n",
    "                    anum += 1\n",
    "                except StopIteration:\n",
    "                    anum = inf\n",
    "            else:\n",
    "                yield (False, belem)\n",
    "                try:\n",
    "                    belem = next(biter)\n",
    "                    bnum += 1\n",
    "                except StopIteration:\n",
    "                    bnum = inf\n",
    "\n",
    "    peft_config = PeftConfig.from_pretrained('User_keq1_t5base_step1')\n",
    "    t5 = T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\n",
    "    taskllm = PeftTaskLLM(peft_config, t5=t5, model_id='User_keq1_t5base_step1')\n",
    "\n",
    "    rhat_peft_config = IA3Config(task_type=TaskType.SEQ_2_SEQ_LM)\n",
    "    rhat_t5 = T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\n",
    "    rewardpredictor = PeftRewardPredictor(rhat_peft_config, t5=rhat_t5)\n",
    "    def reward_augment(inputs):\n",
    "        import re\n",
    "        return [ re.sub(r'Ref1: (.*)\\nRef2: (.*)\\nExtra:',\n",
    "                        r'Ref1: \\2\\nRef2: \\1\\nExtra:',\n",
    "                        z)\n",
    "                 for z in inputs ]\n",
    "\n",
    "    with ProgressPrinter('iter', f'{rank} loss', f'{rank} acc', f'{rank} acc (dev)') as printer:\n",
    "        for iteration in range(2):\n",
    "            for istrain, (examples, labels) in interleave(train, dev):\n",
    "                if iteration == 0 or not istrain:\n",
    "                    for ex, label in zip(examples, labels):\n",
    "                        greedyrewards = []\n",
    "                        allloss = []\n",
    "                        with torch.no_grad():\n",
    "                            embeddings = train.embed( [ ex['ref1'], ex['ref2'] ] + \n",
    "                                                      [ v['title'] \n",
    "                                                       for v in ex['profile']\n",
    "                                                       if v['title'] != ex['title'] \n",
    "                                                     ])\n",
    "                            scores = torch.max(embeddings[[0,1],:] @ embeddings[2:,:].T, dim=0).values\n",
    "                            index = torch.topk(scores, dim=0, k=rank).indices.to('cpu')\n",
    "                            prompts = []\n",
    "                            rhatprompts = []\n",
    "                            for n, oneind in enumerate(index.tolist()):\n",
    "                                titles = [ f'{ex[\"profile\"][ind][\"title\"]}' for ind in (oneind,) ]\n",
    "                                concat_titles = ' and '.join([f'\"{v}\"' for v in titles])\n",
    "                                input = train.append_to_title(ex, concat_titles)\n",
    "                                prompts.append(input)\n",
    "                                rhatprompt = f\"Title: {ex['title']}\\nRef1: {ex['ref1']}\\nRef2: {ex['ref2']}\\nExtra: {titles[0]}\"\n",
    "                                rhatprompts.append(rhatprompt)\n",
    "                            \n",
    "                            guesses = taskllm.predict(prompts, augment=train.swap_refs).argmax(dim=1)\n",
    "                            target = int(label == train.choices[1])\n",
    "                            rewards = (guesses == target).float().unsqueeze(1)\n",
    "                            rhats = rewardpredictor.predict(rhatprompts, augment=reward_augment)\n",
    "                            greedy = torch.argmax(rhats, dim=0).item()\n",
    "                            greedyreward = rewards[greedy, 0].item()\n",
    "                            greedyrewards.append(greedyreward)\n",
    "                            \n",
    "                        loss = rewardpredictor.learn(rhatprompts, rewards, augment=reward_augment) if istrain else None\n",
    "                        allloss.append(loss)\n",
    "\n",
    "                    greedyacc = torch.Tensor(greedyrewards).float().mean().item()\n",
    "                    predloss = torch.Tensor(allloss).mean().item() if istrain else None\n",
    "\n",
    "                    printer.addobs(iteration, predloss, greedyacc if istrain else None, greedyacc if not istrain else None)\n",
    "\n",
    "            printer.print()\n",
    "            printer.autoprint = False\n",
    "            if iteration == 0:\n",
    "                rewardpredictor.save_pretrained(f'User_keq1_t5base_step2_rankeq{rank}')\n",
    "\n",
    "from Fork import SubProcess\n",
    "for rank in range(8, 9):\n",
    "    with SubProcess() as process: process.parent or learn_ranker(rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f69f9b0b-c5e7-405f-89b0-563698352cb8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n                  iter       since      8 loss       since       8 acc       since 8 acc (dev)       since      dt (s)\n",
      "1                     0           0        0.71        0.71           1           1           0           0        1.21\n",
      "2                     0           0       0.694       0.678         0.5           0           0           0         1.9\n",
      "4                     0           0       0.683        0.66       0.667           1           1           1        3.01\n",
      "8                     0           0       0.677       0.673       0.857           1           1           0         5.7\n",
      "16                    0           0       0.671       0.664       0.769       0.667       0.667         0.5        10.4\n",
      "32                    0           0       0.684       0.697       0.769       0.769       0.667       0.667        20.5\n",
      "64                    0           0       0.665       0.645       0.706        0.64       0.692       0.714          40\n",
      "128                   0           0       0.658       0.652       0.618       0.529       0.577       0.462        80.7\n",
      "256                   0           0       0.654        0.65       0.627       0.637       0.577       0.577         163\n",
      "512                   0           0       0.652        0.65       0.663         0.7       0.552       0.528         321\n",
      "1024                  0           0       0.648       0.643       0.673       0.683       0.595       0.638         635\n",
      "1273                  0           0       0.645       0.634       0.685       0.732       0.602       0.627         789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<class 'KeyboardInterrupt'>"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 105\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mFork\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SubProcess\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m rank \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m9\u001b[39m):\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m SubProcess() \u001b[38;5;28;01mas\u001b[39;00m process: process\u001b[38;5;241m.\u001b[39mparent \u001b[38;5;129;01mor\u001b[39;00m learn_ranker(rank)\n",
      "File \u001b[0;32m~/lampstuff/personalized_citation/Fork.py:16\u001b[0m, in \u001b[0;36mSubProcess.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfork()\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 16\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitpid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  File \"/tmp/ipykernel_783552/4133691015.py\", line 105, in <module>\n",
      "    with SubProcess() as process: process.parent or learn_ranker(rank)\n",
      "  File \"/tmp/ipykernel_783552/4133691015.py\", line 90, in learn_ranker\n",
      "    loss = rewardpredictor.learn(rhatprompts, rewards, augment=reward_augment) if istrain else None\n",
      "  File \"/home/pmineiro/lampstuff/personalized_citation/RewardPredictor.py\", line 62, in learn\n",
      "    self._optim.zero_grad()\n",
      "  File \"/home/pmineiro/miniconda3/envs/lampstuff/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/pmineiro/lampstuff/personalized_citation/RewardPredictor.py\", line 45, in forward\n",
      "    augment_outputs = self(augment(data))\n",
      "  File \"/home/pmineiro/miniconda3/envs/lampstuff/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/pmineiro/lampstuff/personalized_citation/RewardPredictor.py\", line 41, in forward\n",
      "    embeddings = self._transformer(**enc, decoder_input_ids=decoder_input_ids).encoder_last_hidden_state[:,-1,:]\n",
      "  File \"/home/pmineiro/miniconda3/envs/lampstuff/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/pmineiro/miniconda3/envs/lampstuff/lib/python3.10/site-packages/peft/peft_model.py\", line 1078, in forward\n",
      "    return self.base_model(\n",
      "  File \"/home/pmineiro/miniconda3/envs/lampstuff/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/pmineiro/miniconda3/envs/lampstuff/lib/python3.10/site-packages/peft/tuners/tuners_utils.py\", line 94, in forward\n",
      "    return self.model.forward(*args, **kwargs)\n",
      "  File \"/home/pmineiro/miniconda3/envs/lampstuff/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py\", line 1746, in forward\n",
      "    decoder_outputs = self.decoder(\n",
      "  File \"/home/pmineiro/miniconda3/envs/lampstuff/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/pmineiro/miniconda3/envs/lampstuff/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py\", line 1123, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/pmineiro/miniconda3/envs/lampstuff/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/pmineiro/miniconda3/envs/lampstuff/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py\", line 695, in forward\n",
      "    self_attention_outputs = self.layer[0](\n",
      "  File \"/home/pmineiro/miniconda3/envs/lampstuff/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/pmineiro/miniconda3/envs/lampstuff/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py\", line 602, in forward\n",
      "    attention_output = self.SelfAttention(\n",
      "  File \"/home/pmineiro/miniconda3/envs/lampstuff/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/pmineiro/miniconda3/envs/lampstuff/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py\", line 527, in forward\n",
      "    value_states = project(\n",
      "  File \"/home/pmineiro/miniconda3/envs/lampstuff/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py\", line 498, in project\n",
      "    hidden_states = shape(proj_layer(hidden_states))\n",
      "  File \"/home/pmineiro/miniconda3/envs/lampstuff/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/pmineiro/miniconda3/envs/lampstuff/lib/python3.10/site-packages/peft/tuners/ia3.py\", line 449, in forward\n",
      "    result = F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\n",
      "  File \"/home/pmineiro/miniconda3/envs/lampstuff/lib/python3.10/site-packages/torch/utils/_device.py\", line 62, in __torch_function__\n",
      "    return func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# logit sum expit ... almost identical\n",
    "def learn_ranker(rank):\n",
    "    from RewardPredictor import PeftRewardPredictor\n",
    "    from TaskLLM import PeftTaskLLM\n",
    "    from T5 import PeftT5Classifier\n",
    "    from PersonalizedCitation import train_loader, dev_loader\n",
    "    from ProgressPrinter import ProgressPrinter\n",
    "    from peft import PeftConfig, IA3Config, TaskType\n",
    "    from transformers import T5ForConditionalGeneration\n",
    "    import torch\n",
    "    \n",
    "    device = 'cuda'\n",
    "    torch.set_default_device(device)\n",
    "    torch.manual_seed(2112)\n",
    "\n",
    "    train = train_loader(batch_size=2)\n",
    "    dev = dev_loader(batch_size=2)\n",
    "\n",
    "    def interleave(a, b):\n",
    "        from math import inf\n",
    "        \n",
    "        atot, btot = a.num_examples, b.num_examples\n",
    "        aiter, biter = a.__iter__(), b.__iter__()\n",
    "        aelem, belem = next(aiter), next(biter)\n",
    "        anum, bnum = 1, 1\n",
    "\n",
    "        while anum != inf and bnum != inf:\n",
    "            if anum * btot <= bnum * atot:\n",
    "                yield (True, aelem)\n",
    "                try:\n",
    "                    aelem = next(aiter)\n",
    "                    anum += 1\n",
    "                except StopIteration:\n",
    "                    anum = inf\n",
    "            else:\n",
    "                yield (False, belem)\n",
    "                try:\n",
    "                    belem = next(biter)\n",
    "                    bnum += 1\n",
    "                except StopIteration:\n",
    "                    bnum = inf\n",
    "\n",
    "    peft_config = PeftConfig.from_pretrained('User_keq1_t5base_step1')\n",
    "    t5 = T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\n",
    "    taskllm = PeftTaskLLM(peft_config, t5=t5, model_id='User_keq1_t5base_step1')\n",
    "\n",
    "    rhat_peft_config = IA3Config(task_type=TaskType.SEQ_2_SEQ_LM)\n",
    "    rhat_t5 = T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\n",
    "    rewardpredictor = PeftRewardPredictor(rhat_peft_config, t5=rhat_t5)\n",
    "    def reward_augment(inputs):\n",
    "        import re\n",
    "        return [ re.sub(r'Ref1: (.*)\\nRef2: (.*)\\nExtra:',\n",
    "                        r'Ref1: \\2\\nRef2: \\1\\nExtra:',\n",
    "                        z)\n",
    "                 for z in inputs ]\n",
    "\n",
    "    with ProgressPrinter('iter', f'{rank} loss', f'{rank} acc', f'{rank} acc (dev)') as printer:\n",
    "        for iteration in range(2):\n",
    "            for istrain, (examples, labels) in interleave(train, dev):\n",
    "                if iteration == 0 or not istrain:\n",
    "                    for ex, label in zip(examples, labels):\n",
    "                        greedyrewards = []\n",
    "                        allloss = []\n",
    "                        with torch.no_grad():\n",
    "                            embeddings = train.embed( [ ex['ref1'], ex['ref2'] ] + \n",
    "                                                      [ v['title'] \n",
    "                                                       for v in ex['profile']\n",
    "                                                       if v['title'] != ex['title'] \n",
    "                                                     ])\n",
    "                            scores = torch.max(embeddings[[0,1],:] @ embeddings[2:,:].T, dim=0).values\n",
    "                            index = torch.topk(scores, dim=0, k=rank).indices.to('cpu')\n",
    "                            prompts = []\n",
    "                            rhatprompts = []\n",
    "                            for n, oneind in enumerate(index.tolist()):\n",
    "                                titles = [ f'{ex[\"profile\"][ind][\"title\"]}' for ind in (oneind,) ]\n",
    "                                concat_titles = ' and '.join([f'\"{v}\"' for v in titles])\n",
    "                                input = train.append_to_title(ex, concat_titles)\n",
    "                                prompts.append(input)\n",
    "                                rhatprompt = f\"Title: {ex['title']}\\nRef1: {ex['ref1']}\\nRef2: {ex['ref2']}\\nExtra: {titles[0]}\"\n",
    "                                rhatprompts.append(rhatprompt)\n",
    "                            \n",
    "                            guesses = taskllm.predict(prompts, augment=train.swap_refs).argmax(dim=1)\n",
    "                            target = int(label == train.choices[1])\n",
    "                            rewards = (guesses == target).float().unsqueeze(1)\n",
    "                            rhats = rewardpredictor.predict(rhatprompts, augment=reward_augment)\n",
    "                            greedy = torch.argmax(rhats, dim=0).item()\n",
    "                            greedyreward = rewards[greedy, 0].item()\n",
    "                            greedyrewards.append(greedyreward)\n",
    "                            \n",
    "                        loss = rewardpredictor.learn(rhatprompts, rewards, augment=reward_augment) if istrain else None\n",
    "                        allloss.append(loss)\n",
    "\n",
    "                    greedyacc = torch.Tensor(greedyrewards).float().mean().item()\n",
    "                    predloss = torch.Tensor(allloss).mean().item() if istrain else None\n",
    "\n",
    "                    printer.addobs(iteration, predloss, greedyacc if istrain else None, greedyacc if not istrain else None)\n",
    "\n",
    "            printer.print()\n",
    "            printer.autoprint = False\n",
    "            if iteration == 0:\n",
    "                rewardpredictor.save_pretrained(f'User_keq1_t5base_step2_rankeq{rank}')\n",
    "\n",
    "from Fork import SubProcess\n",
    "for rank in range(8, 9):\n",
    "    with SubProcess() as process: process.parent or learn_ranker(rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70d9d3dd-7804-4d61-b1d1-acc187264a85",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n                  iter       since      8 loss       since       8 acc       since 8 acc (dev)       since      dt (s)\n",
      "1                     0           0       0.702       0.702           0           0           0           0        1.21\n",
      "2                     0           0       0.701       0.701           0           0           0           0         1.9\n",
      "4                     0           0       0.693       0.684         0.5           1           0           0         3.3\n",
      "8                     0           0       0.672       0.646       0.571       0.667           1           1        5.73\n",
      "16                    0           0       0.704       0.732       0.533         0.5           1           0        11.2\n",
      "32                    0           0       0.653       0.598       0.621       0.714           1           1        21.3\n",
      "64                    0           0        0.64       0.626       0.667       0.714       0.714         0.5        41.7\n",
      "128                   0           0       0.637       0.635       0.675       0.684       0.643       0.571        82.5\n",
      "256                   0           0       0.636       0.634       0.696       0.717       0.621         0.6         165\n",
      "512                   0           0       0.638        0.64       0.678       0.661       0.569       0.517         329\n",
      "1024                  0           0       0.642       0.647       0.665       0.651        0.65       0.729         652\n",
      "2048                  0           0       0.642       0.641       0.678        0.69       0.637       0.624     1.3e+03\n",
      "4096                  0           0       0.638       0.635       0.698       0.718       0.665       0.692    2.59e+03\n",
      "8192                  0           0       0.633       0.627       0.718       0.739       0.696       0.726    5.13e+03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bad pipe message: %s [b'\\xeaC\\t\\x05MX\\xcb\\xc8\\x17\\x14mLc\\x9e\\xb0\\xed\\xe1\\xf3 2\\x08\\x9e\\t\"\\x02\\x84\\xfa*\\xd9\\xaf\\x83\\x07\\x8b\\nXl\\x0fxaa\\x83\\x9d\\x1f\\x95\\xfe\\xff\\x83j\\xf8\\xbf_\\x00\\x08\\x13\\x02\\x13\\x03\\x13\\x01\\x00\\xff\\x01\\x00\\x00\\x8f\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x00\\x1e\\x00\\x1c\\x04\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08\\x08\\x08\\t\\x08\\n\\x08\\x0b\\x08\\x04\\x08\\x05\\x08\\x06\\x04\\x01\\x05\\x01\\x06\\x01\\x00+\\x00\\x03\\x02\\x03\\x04\\x00-\\x00\\x02\\x01\\x01\\x003\\x00&\\x00$\\x00\\x1d\\x00 h\\x06\\xee\\xa8\\x12\\x03*\\x8c']\n",
      "Bad pipe message: %s [b\"\\xe4o\\xf2\\x9b\\n./Ef\\x10\\xb9H\\xe4\\x8b\\x1a\\xa7!\\x1a\\x00\\x00\\xa6\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0s\\xc0w\\x00\\xc4\\x00\\xc3\\xc0#\\xc0'\\x00g\\x00@\\xc0r\\xc0v\\x00\\xbe\\x00\\xbd\\xc0\\n\\xc0\\x14\\x009\\x008\\x00\\x88\\x00\\x87\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9a\\x00\\x99\\x00E\\x00D\\xc0\\x07\\xc0\\x11\\xc0\\x08\\xc0\\x12\\x00\\x16\\x00\\x13\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00\\xc0\\x00<\\x00\\xba\\x005\\x00\\x84\\x00/\\x00\\x96\\x00A\\x00\\x05\\x00\\n\\x00\\xff\\x01\\x00\\x00j\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x00\", b'.\\x04\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08\\x08\\x08\\t\\x08\\n\\x08\\x0b\\x08\\x04\\x08\\x05\\x08\\x06\\x04\\x01\\x05\\x01\\x06\\x01\\x03\\x03\\x02\\x03\\x03\\x01\\x02\\x01\\x03\\x02\\x02\\x02\\x04\\x02\\x05\\x02\\x06\\x02']\n",
      "Bad pipe message: %s [b'\\xf7^\\xf5\\x07c\\xc7\\xa8\\x8fc\\xd0`\\x1c\\\\\\xd4\\xad\\xea#\\x9a\\x00\\x00>\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\xc0\\x0f\\xc0\\x05\\x005\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00\\x19\\x00']\n",
      "Bad pipe message: %s [b'\\x1b\\x00\\x18\\x00\\x1a\\x00\\x16\\x00\\x0e\\x00\\r\\x00\\x0b\\x00\\x0c\\x00\\t\\x00\\n\\x00#\\x00\\x00\\x00\\x0f\\x00\\x01']\n",
      "Bad pipe message: %s [b'', b'\\x00\\x02']\n",
      "Bad pipe message: %s [b'1\\x92\\x8bi\\xb5d\\x00?\\xb9\\xb5\\xc9\\xaa\\xd8\\xed']\n",
      "Bad pipe message: %s [b'P\\xcf\\xb4\\x02\\xcc\\x87[d:\\x96^]\\xf9\\x11\\xcf\\xcbOJ\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00\\x06\\x00\\x17\\x00\\x03\\xc0\\x10\\xc0\\x06\\xc0\\x15\\xc0\\x0b\\xc0\\x01\\x00\\x02\\x00\\x01\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00', b'\\x0c\\x00\\x00\\t127.0.0.1']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10931                 0           0       0.632       0.629       0.721       0.728       0.705       0.732    6.82e+03\n",
      "12180             0.103           1       0.632           0       0.721           0       0.711       0.717    7.33e+03\n"
     ]
    }
   ],
   "source": [
    "# with data doubling ...\n",
    "def learn_ranker(rank):\n",
    "    from RewardPredictor import PeftRewardPredictor\n",
    "    from TaskLLM import PeftTaskLLM\n",
    "    from T5 import PeftT5Classifier\n",
    "    from PersonalizedCitation import train_loader, dev_loader\n",
    "    from ProgressPrinter import ProgressPrinter\n",
    "    from peft import PeftConfig, IA3Config, TaskType\n",
    "    from transformers import T5ForConditionalGeneration\n",
    "    import torch\n",
    "    \n",
    "    device = 'cuda'\n",
    "    torch.set_default_device(device)\n",
    "    torch.manual_seed(2112)\n",
    "\n",
    "    train = train_loader(batch_size=2, double_data=True)\n",
    "    dev = dev_loader(batch_size=2)\n",
    "\n",
    "    def interleave(a, b):\n",
    "        from math import inf\n",
    "        \n",
    "        atot, btot = a.num_examples, b.num_examples\n",
    "        aiter, biter = a.__iter__(), b.__iter__()\n",
    "        aelem, belem = next(aiter), next(biter)\n",
    "        anum, bnum = 1, 1\n",
    "\n",
    "        while anum != inf and bnum != inf:\n",
    "            if anum * btot <= bnum * atot:\n",
    "                yield (True, aelem)\n",
    "                try:\n",
    "                    aelem = next(aiter)\n",
    "                    anum += 1\n",
    "                except StopIteration:\n",
    "                    anum = inf\n",
    "            else:\n",
    "                yield (False, belem)\n",
    "                try:\n",
    "                    belem = next(biter)\n",
    "                    bnum += 1\n",
    "                except StopIteration:\n",
    "                    bnum = inf\n",
    "\n",
    "    peft_config = PeftConfig.from_pretrained('User_keq1_t5base_step1')\n",
    "    t5 = T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\n",
    "    taskllm = PeftTaskLLM(peft_config, t5=t5, model_id='User_keq1_t5base_step1')\n",
    "\n",
    "    rhat_peft_config = IA3Config(task_type=TaskType.SEQ_2_SEQ_LM)\n",
    "    rhat_t5 = T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\n",
    "    rewardpredictor = PeftRewardPredictor(rhat_peft_config, t5=rhat_t5)\n",
    "    def reward_augment(inputs):\n",
    "        import re\n",
    "        return [ re.sub(r'Ref1: (.*)\\nRef2: (.*)\\nExtra:',\n",
    "                        r'Ref1: \\2\\nRef2: \\1\\nExtra:',\n",
    "                        z)\n",
    "                 for z in inputs ]\n",
    "\n",
    "    with ProgressPrinter('iter', f'{rank} loss', f'{rank} acc', f'{rank} acc (dev)') as printer:\n",
    "        for iteration in range(2):\n",
    "            for istrain, (examples, labels) in interleave(train, dev):\n",
    "                if iteration == 0 or not istrain:\n",
    "                    for ex, label in zip(examples, labels):\n",
    "                        greedyrewards = []\n",
    "                        allloss = []\n",
    "                        with torch.no_grad():\n",
    "                            embeddings = train.embed( [ ex['ref1'], ex['ref2'] ] + \n",
    "                                                      [ v['title'] \n",
    "                                                       for v in ex['profile']\n",
    "                                                       if v['title'] != ex['title'] \n",
    "                                                     ])\n",
    "                            scores = torch.max(embeddings[[0,1],:] @ embeddings[2:,:].T, dim=0).values\n",
    "                            index = torch.topk(scores, dim=0, k=rank).indices.to('cpu')\n",
    "                            prompts = []\n",
    "                            rhatprompts = []\n",
    "                            for n, oneind in enumerate(index.tolist()):\n",
    "                                titles = [ f'{ex[\"profile\"][ind][\"title\"]}' for ind in (oneind,) ]\n",
    "                                concat_titles = ' and '.join([f'\"{v}\"' for v in titles])\n",
    "                                input = train.append_to_title(ex, concat_titles)\n",
    "                                prompts.append(input)\n",
    "                                rhatprompt = f\"Title: {ex['title']}\\nRef1: {ex['ref1']}\\nRef2: {ex['ref2']}\\nExtra: {titles[0]}\"\n",
    "                                rhatprompts.append(rhatprompt)\n",
    "                            \n",
    "                            guesses = taskllm.predict(prompts, augment=train.swap_refs).argmax(dim=1)\n",
    "                            target = int(label == train.choices[1])\n",
    "                            rewards = (guesses == target).float().unsqueeze(1)\n",
    "                            rhats = rewardpredictor.predict(rhatprompts, augment=reward_augment)\n",
    "                            greedy = torch.argmax(rhats, dim=0).item()\n",
    "                            greedyreward = rewards[greedy, 0].item()\n",
    "                            greedyrewards.append(greedyreward)\n",
    "                            \n",
    "                        loss = rewardpredictor.learn(rhatprompts, rewards, augment=reward_augment) if istrain else None\n",
    "                        allloss.append(loss)\n",
    "\n",
    "                    greedyacc = torch.Tensor(greedyrewards).float().mean().item()\n",
    "                    predloss = torch.Tensor(allloss).mean().item() if istrain else None\n",
    "\n",
    "                    printer.addobs(iteration, predloss, greedyacc if istrain else None, greedyacc if not istrain else None)\n",
    "\n",
    "            printer.print()\n",
    "            printer.autoprint = False\n",
    "            if iteration == 0:\n",
    "                rewardpredictor.save_pretrained(f'User_keq1_t5base_step2_rankeq{rank}')\n",
    "\n",
    "from Fork import SubProcess\n",
    "for rank in range(8, 9):\n",
    "    with SubProcess() as process: process.parent or learn_ranker(rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6d86c3-4b3c-436c-85cb-d74bcc6043ef",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n                  iter       since      8 loss       since       8 acc       since 8 acc (dev)       since      dt (s)\n",
      "1                     0           0        0.71        0.71           1           1           0           0        1.28\n",
      "2                     0           0       0.694       0.678         0.5           0           0           0        2.08\n",
      "4                     0           0       0.683        0.66       0.667           1           1           1         3.2\n",
      "8                     0           0       0.677       0.673       0.857           1           1           0        5.95\n",
      "16                    0           0       0.671       0.664       0.769       0.667       0.667         0.5        10.7\n",
      "32                    0           0       0.684       0.697       0.769       0.769       0.667       0.667          21\n",
      "64                    0           0       0.665       0.645       0.706        0.64       0.692       0.714        40.8\n",
      "128                   0           0       0.658       0.652       0.618       0.529       0.577       0.462        81.5\n",
      "256                   0           0       0.654        0.65       0.627       0.637       0.577       0.577         162\n",
      "512                   0           0       0.652        0.65       0.663         0.7       0.552       0.528         326\n",
      "1024                  0           0       0.648       0.643       0.673       0.683       0.595       0.638         654\n",
      "2048                  0           0       0.649        0.65       0.696       0.719       0.636       0.676     1.3e+03\n"
     ]
    }
   ],
   "source": [
    "# multiple training passes ...\n",
    "def learn_ranker(rank):\n",
    "    from RewardPredictor import PeftRewardPredictor\n",
    "    from TaskLLM import PeftTaskLLM\n",
    "    from T5 import PeftT5Classifier\n",
    "    from PersonalizedCitation import train_loader, dev_loader\n",
    "    from ProgressPrinter import ProgressPrinter\n",
    "    from peft import PeftConfig, IA3Config, TaskType\n",
    "    from transformers import T5ForConditionalGeneration\n",
    "    import torch\n",
    "    \n",
    "    device = 'cuda'\n",
    "    torch.set_default_device(device)\n",
    "    torch.manual_seed(2112)\n",
    "\n",
    "    train = train_loader(batch_size=2)\n",
    "    dev = dev_loader(batch_size=2)\n",
    "\n",
    "    def interleave(a, b):\n",
    "        from math import inf\n",
    "        \n",
    "        atot, btot = a.num_examples, b.num_examples\n",
    "        aiter, biter = a.__iter__(), b.__iter__()\n",
    "        aelem, belem = next(aiter), next(biter)\n",
    "        anum, bnum = 1, 1\n",
    "\n",
    "        while anum != inf and bnum != inf:\n",
    "            if anum * btot <= bnum * atot:\n",
    "                yield (True, aelem)\n",
    "                try:\n",
    "                    aelem = next(aiter)\n",
    "                    anum += 1\n",
    "                except StopIteration:\n",
    "                    anum = inf\n",
    "            else:\n",
    "                yield (False, belem)\n",
    "                try:\n",
    "                    belem = next(biter)\n",
    "                    bnum += 1\n",
    "                except StopIteration:\n",
    "                    bnum = inf\n",
    "\n",
    "    peft_config = PeftConfig.from_pretrained('User_keq1_t5base_step1')\n",
    "    t5 = T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\n",
    "    taskllm = PeftTaskLLM(peft_config, t5=t5, model_id='User_keq1_t5base_step1')\n",
    "\n",
    "    rhat_peft_config = IA3Config(task_type=TaskType.SEQ_2_SEQ_LM)\n",
    "    rhat_t5 = T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\n",
    "    rewardpredictor = PeftRewardPredictor(rhat_peft_config, t5=rhat_t5)\n",
    "    def reward_augment(inputs):\n",
    "        import re\n",
    "        return [ re.sub(r'Ref1: (.*)\\nRef2: (.*)\\nExtra:',\n",
    "                        r'Ref1: \\2\\nRef2: \\1\\nExtra:',\n",
    "                        z)\n",
    "                 for z in inputs ]\n",
    "\n",
    "    with ProgressPrinter('iter', f'{rank} loss', f'{rank} acc', f'{rank} acc (dev)') as printer:\n",
    "        max_iteration = 4\n",
    "        for iteration in range(max_iteration):\n",
    "            for istrain, (examples, labels) in interleave(train, dev):\n",
    "                if iteration + 1 < max_iteration or not istrain:\n",
    "                    for ex, label in zip(examples, labels):\n",
    "                        greedyrewards = []\n",
    "                        allloss = []\n",
    "                        with torch.no_grad():\n",
    "                            embeddings = train.embed( [ ex['ref1'], ex['ref2'] ] + \n",
    "                                                      [ v['title'] \n",
    "                                                       for v in ex['profile']\n",
    "                                                       if v['title'] != ex['title'] \n",
    "                                                     ])\n",
    "                            scores = torch.max(embeddings[[0,1],:] @ embeddings[2:,:].T, dim=0).values\n",
    "                            index = torch.topk(scores, dim=0, k=rank).indices.to('cpu')\n",
    "                            prompts = []\n",
    "                            rhatprompts = []\n",
    "                            for n, oneind in enumerate(index.tolist()):\n",
    "                                titles = [ f'{ex[\"profile\"][ind][\"title\"]}' for ind in (oneind,) ]\n",
    "                                concat_titles = ' and '.join([f'\"{v}\"' for v in titles])\n",
    "                                input = train.append_to_title(ex, concat_titles)\n",
    "                                prompts.append(input)\n",
    "                                rhatprompt = f\"Title: {ex['title']}\\nRef1: {ex['ref1']}\\nRef2: {ex['ref2']}\\nExtra: {titles[0]}\"\n",
    "                                rhatprompts.append(rhatprompt)\n",
    "                            \n",
    "                            guesses = taskllm.predict(prompts, augment=train.swap_refs).argmax(dim=1)\n",
    "                            target = int(label == train.choices[1])\n",
    "                            rewards = (guesses == target).float().unsqueeze(1)\n",
    "                            rhats = rewardpredictor.predict(rhatprompts, augment=reward_augment)\n",
    "                            greedy = torch.argmax(rhats, dim=0).item()\n",
    "                            greedyreward = rewards[greedy, 0].item()\n",
    "                            greedyrewards.append(greedyreward)\n",
    "                            \n",
    "                        loss = rewardpredictor.learn(rhatprompts, rewards, augment=reward_augment) if istrain else None\n",
    "                        allloss.append(loss)\n",
    "\n",
    "                    greedyacc = torch.Tensor(greedyrewards).float().mean().item()\n",
    "                    predloss = torch.Tensor(allloss).mean().item() if istrain else None\n",
    "\n",
    "                    printer.addobs(iteration, predloss, greedyacc if istrain else None, greedyacc if not istrain else None)\n",
    "\n",
    "            printer.print()\n",
    "            printer.autoprint = False\n",
    "            if iteration + 1 == max_iteration:\n",
    "                rewardpredictor.save_pretrained(f'User_keq1_t5base_step2_rankeq{rank}')\n",
    "\n",
    "from Fork import SubProcess\n",
    "for rank in range(8, 9):\n",
    "    with SubProcess() as process: process.parent or learn_ranker(rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d85faaed-cafb-48c6-9093-9c8af1d22e73",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n                  iter       since      8 loss       since       8 acc       since 8 acc (dev)       since      dt (s)\n",
      "1                     0           0       0.702       0.702           0           0           0           0        1.19\n",
      "2                     0           0       0.701       0.701           0           0           0           0        1.68\n",
      "4                     0           0       0.693       0.684         0.5           1           0           0        2.69\n",
      "8                     0           0       0.672       0.646       0.571       0.667           1           1        4.43\n",
      "16                    0           0       0.704       0.732       0.533         0.5           1           0        8.29\n",
      "32                    0           0       0.653       0.598       0.621       0.714           1           1        15.7\n",
      "64                    0           0        0.64       0.626       0.667       0.714       0.714         0.5        30.6\n",
      "128                   0           0       0.637       0.635       0.649       0.632       0.643       0.571        60.4\n",
      "256                   0           0       0.636       0.634       0.683       0.717       0.621         0.6         121\n",
      "512                   0           0       0.638        0.64        0.67       0.656       0.569       0.517         242\n",
      "1024                  0           0       0.642       0.647       0.669       0.669        0.65       0.729         483\n",
      "2048                  0           0       0.642       0.642       0.672       0.675       0.632       0.615         976\n",
      "4096                  0           0        0.64       0.637       0.693       0.713       0.667       0.701    1.94e+03\n",
      "8192                  0           0       0.635        0.63        0.71       0.727       0.696       0.724    3.82e+03\n",
      "10931                 0           0       0.634       0.633       0.712        0.72       0.699       0.709    5.04e+03\n",
      "12180             0.103           1       0.634           0       0.712           0       0.704       0.709    5.46e+03\n"
     ]
    }
   ],
   "source": [
    "# with data doubling and no reward augment ...\n",
    "def learn_ranker(rank):\n",
    "    from RewardPredictor import PeftRewardPredictor\n",
    "    from TaskLLM import PeftTaskLLM\n",
    "    from T5 import PeftT5Classifier\n",
    "    from PersonalizedCitation import train_loader, dev_loader\n",
    "    from ProgressPrinter import ProgressPrinter\n",
    "    from peft import PeftConfig, IA3Config, TaskType\n",
    "    from transformers import T5ForConditionalGeneration\n",
    "    import torch\n",
    "    \n",
    "    device = 'cuda'\n",
    "    torch.set_default_device(device)\n",
    "    torch.manual_seed(2112)\n",
    "\n",
    "    train = train_loader(batch_size=2, double_data=True)\n",
    "    dev = dev_loader(batch_size=2)\n",
    "\n",
    "    def interleave(a, b):\n",
    "        from math import inf\n",
    "        \n",
    "        atot, btot = a.num_examples, b.num_examples\n",
    "        aiter, biter = a.__iter__(), b.__iter__()\n",
    "        aelem, belem = next(aiter), next(biter)\n",
    "        anum, bnum = 1, 1\n",
    "\n",
    "        while anum != inf and bnum != inf:\n",
    "            if anum * btot <= bnum * atot:\n",
    "                yield (True, aelem)\n",
    "                try:\n",
    "                    aelem = next(aiter)\n",
    "                    anum += 1\n",
    "                except StopIteration:\n",
    "                    anum = inf\n",
    "            else:\n",
    "                yield (False, belem)\n",
    "                try:\n",
    "                    belem = next(biter)\n",
    "                    bnum += 1\n",
    "                except StopIteration:\n",
    "                    bnum = inf\n",
    "\n",
    "    peft_config = PeftConfig.from_pretrained('User_keq1_t5base_step1')\n",
    "    t5 = T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\n",
    "    taskllm = PeftTaskLLM(peft_config, t5=t5, model_id='User_keq1_t5base_step1')\n",
    "\n",
    "    rhat_peft_config = IA3Config(task_type=TaskType.SEQ_2_SEQ_LM)\n",
    "    rhat_t5 = T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\n",
    "    rewardpredictor = PeftRewardPredictor(rhat_peft_config, t5=rhat_t5)\n",
    "\n",
    "    with ProgressPrinter('iter', f'{rank} loss', f'{rank} acc', f'{rank} acc (dev)') as printer:\n",
    "        for iteration in range(2):\n",
    "            for istrain, (examples, labels) in interleave(train, dev):\n",
    "                if iteration == 0 or not istrain:\n",
    "                    for ex, label in zip(examples, labels):\n",
    "                        greedyrewards = []\n",
    "                        allloss = []\n",
    "                        with torch.no_grad():\n",
    "                            embeddings = train.embed( [ ex['ref1'], ex['ref2'] ] + \n",
    "                                                      [ v['title'] \n",
    "                                                       for v in ex['profile']\n",
    "                                                       if v['title'] != ex['title'] \n",
    "                                                     ])\n",
    "                            scores = torch.max(embeddings[[0,1],:] @ embeddings[2:,:].T, dim=0).values\n",
    "                            index = torch.topk(scores, dim=0, k=rank).indices.to('cpu')\n",
    "                            prompts = []\n",
    "                            rhatprompts = []\n",
    "                            for n, oneind in enumerate(index.tolist()):\n",
    "                                titles = [ f'{ex[\"profile\"][ind][\"title\"]}' for ind in (oneind,) ]\n",
    "                                concat_titles = ' and '.join([f'\"{v}\"' for v in titles])\n",
    "                                input = train.append_to_title(ex, concat_titles)\n",
    "                                prompts.append(input)\n",
    "                                rhatprompt = f\"Title: {ex['title']}\\nRef1: {ex['ref1']}\\nRef2: {ex['ref2']}\\nExtra: {titles[0]}\"\n",
    "                                rhatprompts.append(rhatprompt)\n",
    "                            \n",
    "                            guesses = taskllm.predict(prompts, augment=train.swap_refs).argmax(dim=1)\n",
    "                            target = int(label == train.choices[1])\n",
    "                            rewards = (guesses == target).float().unsqueeze(1)\n",
    "                            rhats = rewardpredictor.predict(rhatprompts)\n",
    "                            greedy = torch.argmax(rhats, dim=0).item()\n",
    "                            greedyreward = rewards[greedy, 0].item()\n",
    "                            greedyrewards.append(greedyreward)\n",
    "                            \n",
    "                        loss = rewardpredictor.learn(rhatprompts, rewards) if istrain else None\n",
    "                        allloss.append(loss)\n",
    "\n",
    "                    greedyacc = torch.Tensor(greedyrewards).float().mean().item()\n",
    "                    predloss = torch.Tensor(allloss).mean().item() if istrain else None\n",
    "\n",
    "                    printer.addobs(iteration, predloss, greedyacc if istrain else None, greedyacc if not istrain else None)\n",
    "\n",
    "            printer.print()\n",
    "            printer.autoprint = False\n",
    "            if iteration == 0:\n",
    "                rewardpredictor.save_pretrained(f'User_keq1_t5base_step2_rankeq{rank}')\n",
    "\n",
    "from Fork import SubProcess\n",
    "for rank in range(8, 9):\n",
    "    with SubProcess() as process: process.parent or learn_ranker(rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0498ff16-f362-4447-a4fe-cf1e87721616",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
