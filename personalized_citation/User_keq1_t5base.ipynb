{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a258105a-3b77-499a-bc5c-6e1ceb3e92e5",
   "metadata": {},
   "source": [
    "# Step 1: fine-tune LLM using top result from (fixed) ranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83f4483a-9d5d-4c65-923c-cc9e327c9498",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n                  iter       since      1 loss       since       1 acc       since 1 acc (dev)       since      dt (s)\n",
      "1                     0           0       0.708       0.708           0           0           0           0        1.21\n",
      "2                     0           0        0.69       0.673         0.5           1           0           0        1.74\n",
      "4                     0           0        0.68       0.661       0.667           1           1           1        2.53\n",
      "8                     0           0       0.693       0.703       0.571         0.5           1           0        4.53\n",
      "16                    0           0       0.696         0.7       0.577       0.583         0.5        0.25        8.03\n",
      "32                    0           0       0.694       0.692       0.538         0.5        0.75           1        16.1\n",
      "64                    0           0        0.69       0.686       0.588        0.64       0.654       0.571          31\n",
      "128                   0           0       0.687       0.684       0.578       0.569       0.519       0.385        62.1\n",
      "256                   0           0       0.676       0.664        0.61       0.642       0.625       0.731         123\n",
      "512                   0           0       0.658       0.639       0.633       0.655       0.652       0.679         246\n",
      "1024                  0           0       0.646       0.635       0.638       0.644       0.631        0.61         491\n",
      "2048                  0           0        0.63       0.613       0.649        0.66       0.638       0.645         990\n",
      "4096                  0           0       0.609       0.587       0.666       0.682       0.663       0.688    1.97e+03\n",
      "6090                  0           0       0.606         0.6       0.667       0.669       0.669       0.681    2.91e+03\n",
      "7339               0.17           1       0.606           0       0.667           0       0.671       0.674    3.33e+03\n"
     ]
    }
   ],
   "source": [
    "def step_one(k):\n",
    "    from TaskLLM import TaskLLM\n",
    "    from PersonalizedCitation import train_loader, dev_loader\n",
    "    from ProgressPrinter import ProgressPrinter\n",
    "    from peft import IA3Config, TaskType\n",
    "    from transformers import T5ForConditionalGeneration\n",
    "    import torch\n",
    "    from Util import interleave\n",
    "    \n",
    "    device = 'cuda'\n",
    "    torch.set_default_device(device)\n",
    "    torch.manual_seed(2112)\n",
    "\n",
    "    train = train_loader(batch_size=2)\n",
    "    dev = dev_loader(batch_size=2)\n",
    "\n",
    "    t5 = T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\n",
    "    taskllm_config = IA3Config(task_type=TaskType.SEQ_2_SEQ_LM)\n",
    "    t5.add_adapter(taskllm_config, \"taskllm\")\n",
    "    t5.enable_adapters()\n",
    "\n",
    "    taskllm = TaskLLM(t5=t5, adapter_name=\"taskllm\")\n",
    "\n",
    "    with ProgressPrinter('iter', f'{k} loss', f'{k} acc', f'{k} acc (dev)') as printer:\n",
    "        for iteration in range(2):\n",
    "            for istrain, (examples, labels) in interleave(train, dev):\n",
    "                if iteration == 0 or not istrain:\n",
    "                    with torch.no_grad():\n",
    "                        inputs = []\n",
    "                        target = []\n",
    "        \n",
    "                        for ex, label in zip(examples, labels):\n",
    "                            embeddings = train.embed( [ ex['ref1'], ex['ref2'] ] + \n",
    "                                                      [ v['title'] \n",
    "                                                       for v in ex['profile']\n",
    "                                                       if v['title'] != ex['title'] \n",
    "                                                     ])\n",
    "                            scores = torch.max(embeddings[[0,1],:] @ embeddings[2:,:].T, dim=0).values\n",
    "                            index = torch.topk(scores, dim=0, k=k).indices.to('cpu')\n",
    "                            for n, oneind in enumerate(index.tolist()):\n",
    "                                titles = [ f'{ex[\"profile\"][ind][\"title\"]}' for ind in (oneind,) ]\n",
    "                                concat_titles = ' and '.join([f'\"{v}\"' for v in titles])\n",
    "                                input = train.append_to_title(ex, concat_titles)\n",
    "                                inputs.append(input)\n",
    "                                target.append(int(label == train.choices[1]))\n",
    "\n",
    "                        target = torch.Tensor(target).long().to(device)\n",
    "                        acc = (taskllm.predict(inputs, augment=train.swap_refs).argmax(dim=1) == target).float().mean().item()\n",
    "    \n",
    "                    loss = taskllm.learn(inputs, target, augment=train.swap_refs) if istrain else None\n",
    "                    printer.addobs(iteration, loss, acc if istrain else None, acc if not istrain else None)\n",
    "\n",
    "            printer.print()\n",
    "            printer.autoprint = False\n",
    "            if iteration == 0:\n",
    "                taskllm.save_pretrained(f'User_keq{k}_t5base_step1')\n",
    "\n",
    "step_one(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf16df8-9320-4be9-98f9-da38da180a91",
   "metadata": {},
   "source": [
    "# Step 2: learn ranker using (fixed pre-finetuned) task LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7df7c543-4619-46f6-860e-a1ec270d7367",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n                  iter       since      8 loss       since       8 acc       since 8 acc (dev)       since      dt (s)\n",
      "1                     0           0       0.718       0.718           0           0           0           0        1.85\n",
      "2                     0           0       0.708       0.699           0           0           0           0        2.94\n",
      "4                     0           0       0.709        0.71         0.5           1           0           0        5.16\n",
      "8                     0           0       0.686       0.656       0.571       0.667           1           1         9.1\n",
      "16                    0           0       0.706       0.724       0.467       0.375           1           0        19.1\n",
      "32                    0           0       0.638       0.565       0.621       0.786           1           1        36.9\n",
      "64                    0           0       0.637       0.636       0.667       0.714       0.714         0.5        74.1\n",
      "128                   0           0       0.639       0.641       0.658       0.649       0.714       0.714         151\n",
      "256                   0           0       0.635        0.63       0.665       0.673        0.69       0.667         302\n",
      "512                   0           0       0.636       0.636       0.685       0.705       0.672       0.655         607\n",
      "1024                  0           0       0.642       0.649       0.668       0.651       0.658       0.644    1.21e+03\n",
      "2048                  0           0       0.641        0.64       0.691       0.713       0.667       0.675    2.42e+03\n",
      "4096                  0           0       0.637       0.633        0.71       0.729       0.684       0.701    4.85e+03\n",
      "8192                  0           0       0.632       0.627       0.723       0.735       0.723       0.763    9.68e+03\n",
      "10931                 0           0        0.63       0.625       0.727       0.742       0.721       0.712    1.29e+04\n",
      "12180             0.103           1        0.63           0       0.727           0       0.719       0.717    1.39e+04\n"
     ]
    }
   ],
   "source": [
    "def learn_ranker(rank):\n",
    "    from RewardPredictor import RewardPredictor\n",
    "    from TaskLLM import TaskLLM\n",
    "    from PersonalizedCitation import train_loader, dev_loader\n",
    "    from ProgressPrinter import ProgressPrinter\n",
    "    from peft import IA3Config, TaskType\n",
    "    from transformers import T5ForConditionalGeneration\n",
    "    import torch\n",
    "    from Util import interleave\n",
    "    \n",
    "    device = 'cuda'\n",
    "    torch.set_default_device(device)\n",
    "    torch.manual_seed(2112)\n",
    "\n",
    "    train = train_loader(batch_size=2, double_data=True)\n",
    "    dev = dev_loader(batch_size=2)\n",
    "\n",
    "    t5 = T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\n",
    "    t5.load_adapter('User_keq1_t5base_step1', 'taskllm')\n",
    "\n",
    "    rhat_config = IA3Config(task_type=TaskType.SEQ_2_SEQ_LM)\n",
    "    t5.add_adapter(rhat_config, \"rhat\")\n",
    "    t5.enable_adapters()\n",
    "    \n",
    "    taskllm = TaskLLM(t5=t5, adapter_name=\"taskllm\")\n",
    "    rewardpredictor = RewardPredictor(t5=t5, adapter_name=\"rhat\")\n",
    "    \n",
    "    def reward_augment(inputs):\n",
    "        import re\n",
    "        return [ re.sub(r'Ref1: (.*)\\nRef2: (.*)\\nExtra:',\n",
    "                        r'Ref1: \\2\\nRef2: \\1\\nExtra:',\n",
    "                        z)\n",
    "                 for z in inputs ]\n",
    "\n",
    "    with ProgressPrinter('iter', f'{rank} loss', f'{rank} acc', f'{rank} acc (dev)') as printer:\n",
    "        for iteration in range(2):\n",
    "            for istrain, (examples, labels) in interleave(train, dev):\n",
    "                if iteration == 0 or not istrain:\n",
    "                    for ex, label in zip(examples, labels):\n",
    "                        greedyrewards = []\n",
    "                        allloss = []\n",
    "                        with torch.no_grad():\n",
    "                            embeddings = train.embed( [ ex['ref1'], ex['ref2'] ] + \n",
    "                                                      [ v['title'] \n",
    "                                                       for v in ex['profile']\n",
    "                                                       if v['title'] != ex['title'] \n",
    "                                                     ])\n",
    "                            scores = torch.max(embeddings[[0,1],:] @ embeddings[2:,:].T, dim=0).values\n",
    "                            index = torch.topk(scores, dim=0, k=rank).indices.to('cpu')\n",
    "                            prompts = []\n",
    "                            rhatprompts = []\n",
    "                            for n, oneind in enumerate(index.tolist()):\n",
    "                                titles = [ f'{ex[\"profile\"][ind][\"title\"]}' for ind in (oneind,) ]\n",
    "                                concat_titles = ' and '.join([f'\"{v}\"' for v in titles])\n",
    "                                input = train.append_to_title(ex, concat_titles)\n",
    "                                prompts.append(input)\n",
    "                                rhatprompt = f\"Title: {ex['title']}\\nRef1: {ex['ref1']}\\nRef2: {ex['ref2']}\\nExtra: {titles[0]}\"\n",
    "                                rhatprompts.append(rhatprompt)\n",
    "                            \n",
    "                            guesses = taskllm.predict(prompts, augment=train.swap_refs).argmax(dim=1)\n",
    "                            target = int(label == train.choices[1])\n",
    "                            rewards = (guesses == target).float().unsqueeze(1)\n",
    "                            rhats = rewardpredictor.predict(rhatprompts, augment=reward_augment)\n",
    "                            greedy = torch.argmax(rhats, dim=0).item()\n",
    "                            greedyreward = rewards[greedy, 0].item()\n",
    "                            greedyrewards.append(greedyreward)\n",
    "                            \n",
    "                        loss = rewardpredictor.learn(rhatprompts, rewards, augment=reward_augment) if istrain else None\n",
    "                        allloss.append(loss)\n",
    "\n",
    "                    greedyacc = torch.Tensor(greedyrewards).float().mean().item()\n",
    "                    predloss = torch.Tensor(allloss).mean().item() if istrain else None\n",
    "\n",
    "                    printer.addobs(iteration, predloss, greedyacc if istrain else None, greedyacc if not istrain else None)\n",
    "\n",
    "            printer.print()\n",
    "            printer.autoprint = False\n",
    "            if iteration == 0:\n",
    "                rewardpredictor.save_pretrained(f'User_keq1_t5base_step2_rankeq{rank}')\n",
    "\n",
    "learn_ranker(rank=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ceafbc-7af5-4035-8b41-712b221c6d39",
   "metadata": {},
   "source": [
    "# Step 3: Prepare leaderboard submission files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "00691200-4034-407f-ae0b-a3a8b39d5799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n           8 acc (dev)       since      dt (s)\n",
      "1                   0.5         0.5        1.08\n",
      "4                  0.75           1         2.8\n",
      "8                 0.625         0.5        5.17\n",
      "16                0.688        0.75        9.38\n",
      "32                0.688       0.688        18.6\n",
      "64                0.703       0.719        38.5\n",
      "128               0.688       0.672        77.8\n",
      "256               0.703       0.719         154\n",
      "512               0.686       0.668         318\n",
      "1024              0.704       0.723         642\n",
      "2048              0.712       0.721    1.28e+03\n",
      "2500              0.718       0.741    1.57e+03\n"
     ]
    }
   ],
   "source": [
    "def prepare_submission(*, rank):\n",
    "    import json\n",
    "    from RewardPredictor import RewardPredictor\n",
    "    from TaskLLM import TaskLLM\n",
    "    from PersonalizedCitation import train_loader, dev_loader, test_loader\n",
    "    from ProgressPrinter import ProgressPrinter\n",
    "    from peft import PeftConfig, IA3Config, TaskType\n",
    "    from transformers import T5ForConditionalGeneration\n",
    "    import torch\n",
    "    from Util import interleave\n",
    "    \n",
    "    device = 'cuda'\n",
    "    torch.set_default_device(device)\n",
    "    torch.manual_seed(2112)\n",
    "\n",
    "    dev = dev_loader(batch_size=2)\n",
    "    test = test_loader(batch_size=2)\n",
    "\n",
    "    t5 = T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\n",
    "    t5.load_adapter('User_keq1_t5base_step1', 'taskllm')\n",
    "    t5.load_adapter(f'User_keq1_t5base_step2_rankeq{rank}', 'rhat')\n",
    "    t5.enable_adapters()\n",
    "    \n",
    "    taskllm = TaskLLM(t5=t5, adapter_name=\"taskllm\")\n",
    "    rewardpredictor = RewardPredictor(t5=t5, adapter_name=\"rhat\", model_id=f'User_keq1_t5base_step2_rankeq{rank}')\n",
    "    \n",
    "    def reward_augment(inputs):\n",
    "        import re\n",
    "        return [ re.sub(r'Ref1: (.*)\\nRef2: (.*)\\nExtra:',\n",
    "                        r'Ref1: \\2\\nRef2: \\1\\nExtra:',\n",
    "                        z)\n",
    "                 for z in inputs ]\n",
    "\n",
    "    with ProgressPrinter(f'{rank} acc (dev)') as printer:\n",
    "        devgolds, testgolds = [], []\n",
    "        \n",
    "        for isdev, (examples, labels) in interleave(dev, test):\n",
    "            greedyrewards = []\n",
    "            for ex, label in zip(examples, labels):\n",
    "                with torch.no_grad():\n",
    "                    embeddings = dev.embed( [ ex['ref1'], ex['ref2'] ] + \n",
    "                                              [ v['title'] \n",
    "                                               for v in ex['profile']\n",
    "                                               if v['title'] != ex['title'] \n",
    "                                             ])\n",
    "                    scores = torch.max(embeddings[[0,1],:] @ embeddings[2:,:].T, dim=0).values\n",
    "                    index = torch.topk(scores, dim=0, k=rank).indices.to('cpu')\n",
    "                    prompts = []\n",
    "                    rhatprompts = []\n",
    "                    for n, oneind in enumerate(index.tolist()):\n",
    "                        titles = [ f'{ex[\"profile\"][ind][\"title\"]}' for ind in (oneind,) ]\n",
    "                        concat_titles = ' and '.join([f'\"{v}\"' for v in titles])\n",
    "                        input = dev.append_to_title(ex, concat_titles)\n",
    "                        prompts.append(input)\n",
    "                        rhatprompt = f\"Title: {ex['title']}\\nRef1: {ex['ref1']}\\nRef2: {ex['ref2']}\\nExtra: {titles[0]}\"\n",
    "                        rhatprompts.append(rhatprompt)\n",
    "\n",
    "                    rhats = rewardpredictor.predict(rhatprompts, augment=reward_augment)\n",
    "                    greedy = torch.argmax(rhats, dim=0).item()\n",
    "                    guess = taskllm.predict([ prompts[greedy] ], augment=dev.swap_refs).argmax(dim=1)\n",
    "                    if isdev:\n",
    "                        target = int(label == dev.choices[1])\n",
    "                        reward = int(guess.item() == target)\n",
    "                        greedyrewards.append(reward)\n",
    "\n",
    "                    (devgolds if isdev else testgolds).append({ 'id': ex['id'], 'output': \"[2]\" if guess else \"[1]\" })\n",
    "\n",
    "            greedyacc = torch.Tensor(greedyrewards).float().mean().item() if isdev else None\n",
    "\n",
    "            printer.addobs(greedyacc)\n",
    "\n",
    "        printer.print()\n",
    "        printer.autoprint = False\n",
    "\n",
    "        for wut, golds in ( ('dev', devgolds), ('test', testgolds) ):\n",
    "            with open(f'lamp1u_{wut}golds_rankeq{rank}.json', 'w') as jsonfile:\n",
    "                json.dump({ 'task': 'LaMP_1', 'golds': golds }, jsonfile)\n",
    "            \n",
    "prepare_submission(rank=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58810c2b-35e5-4efe-bd43-1b5f25c16357",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
