{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a258105a-3b77-499a-bc5c-6e1ceb3e92e5",
   "metadata": {},
   "source": [
    "# Step 1: fine-tune LLM using top result from (fixed) ranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83f4483a-9d5d-4c65-923c-cc9e327c9498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n                  iter       since      1 loss       since       1 acc       since 1 acc (dev)       since      dt (s)\n",
      "1                     0           0       0.708       0.708           0           0           0           0        1.21\n",
      "2                     0           0        0.69       0.673         0.5           1           0           0        1.74\n",
      "4                     0           0        0.68       0.661       0.667           1           1           1        2.53\n",
      "8                     0           0       0.693       0.703       0.571         0.5           1           0        4.53\n",
      "16                    0           0       0.696         0.7       0.577       0.583         0.5        0.25        8.03\n",
      "32                    0           0       0.694       0.692       0.538         0.5        0.75           1        16.1\n",
      "64                    0           0        0.69       0.686       0.588        0.64       0.654       0.571          31\n",
      "128                   0           0       0.687       0.684       0.578       0.569       0.519       0.385        62.1\n",
      "256                   0           0       0.676       0.664        0.61       0.642       0.625       0.731         123\n",
      "512                   0           0       0.658       0.639       0.633       0.655       0.652       0.679         246\n",
      "1024                  0           0       0.646       0.635       0.638       0.644       0.631        0.61         491\n",
      "2048                  0           0        0.63       0.613       0.649        0.66       0.638       0.645         990\n",
      "4096                  0           0       0.609       0.587       0.666       0.682       0.663       0.688    1.97e+03\n",
      "6090                  0           0       0.606         0.6       0.667       0.669       0.669       0.681    2.91e+03\n",
      "7339               0.17           1       0.606           0       0.667           0       0.671       0.674    3.33e+03\n"
     ]
    }
   ],
   "source": [
    "def step_one(k):\n",
    "    from TaskLLM import TaskLLM\n",
    "    from PersonalizedCitation import train_loader, dev_loader\n",
    "    from ProgressPrinter import ProgressPrinter\n",
    "    from peft import IA3Config, TaskType\n",
    "    from transformers import T5ForConditionalGeneration\n",
    "    import torch\n",
    "    from Util import interleave\n",
    "    \n",
    "    device = 'cuda'\n",
    "    torch.set_default_device(device)\n",
    "    torch.manual_seed(2112)\n",
    "\n",
    "    train = train_loader(batch_size=2)\n",
    "    dev = dev_loader(batch_size=2)\n",
    "\n",
    "    t5 = T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\n",
    "    taskllm_config = IA3Config(task_type=TaskType.SEQ_2_SEQ_LM)\n",
    "    t5.add_adapter(taskllm_config, \"taskllm\")\n",
    "    t5.enable_adapters()\n",
    "\n",
    "    taskllm = TaskLLM(t5=t5, adapter_name=\"taskllm\")\n",
    "\n",
    "    with ProgressPrinter('iter', f'{k} loss', f'{k} acc', f'{k} acc (dev)') as printer:\n",
    "        for iteration in range(2):\n",
    "            for istrain, (examples, labels) in interleave(train, dev):\n",
    "                if iteration == 0 or not istrain:\n",
    "                    with torch.no_grad():\n",
    "                        inputs = []\n",
    "                        target = []\n",
    "        \n",
    "                        for ex, label in zip(examples, labels):\n",
    "                            embeddings = train.embed( [ ex['ref1'], ex['ref2'] ] + \n",
    "                                                      [ v['title'] \n",
    "                                                       for v in ex['profile']\n",
    "                                                       if v['title'] != ex['title'] \n",
    "                                                     ])\n",
    "                            scores = torch.max(embeddings[[0,1],:] @ embeddings[2:,:].T, dim=0).values\n",
    "                            index = torch.topk(scores, dim=0, k=k).indices.to('cpu')\n",
    "                            for n, oneind in enumerate(index.tolist()):\n",
    "                                titles = [ f'{ex[\"profile\"][ind][\"title\"]}' for ind in (oneind,) ]\n",
    "                                concat_titles = ' and '.join([f'\"{v}\"' for v in titles])\n",
    "                                input = train.append_to_title(ex, concat_titles)\n",
    "                                inputs.append(input)\n",
    "                                target.append(int(label == train.choices[1]))\n",
    "\n",
    "                        target = torch.Tensor(target).long().to(device)\n",
    "                        acc = (taskllm.predict(inputs, augment=train.swap_refs).argmax(dim=1) == target).float().mean().item()\n",
    "    \n",
    "                    loss = taskllm.learn(inputs, target, augment=train.swap_refs) if istrain else None\n",
    "                    printer.addobs(iteration, loss, acc if istrain else None, acc if not istrain else None)\n",
    "\n",
    "            printer.print()\n",
    "            printer.autoprint = False\n",
    "            if iteration == 0:\n",
    "                taskllm.save_pretrained(f'User_keq{k}_t5base_step1')\n",
    "\n",
    "from Fork import SubProcess\n",
    "with SubProcess() as process: process.parent or step_one(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf16df8-9320-4be9-98f9-da38da180a91",
   "metadata": {},
   "source": [
    "# Step 2: learn ranker using (fixed pre-finetuned) task LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a3c4dfb-cb62-4d2b-a0e7-d1c281da1c4d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n                  iter       since      8 loss       since       8 acc       since 8 acc (dev)       since      dt (s)\n",
      "1                     0           0       0.718       0.718           0           0           0           0        1.85\n",
      "2                     0           0       0.708       0.699           0           0           0           0        2.94\n",
      "4                     0           0       0.709        0.71         0.5           1           0           0        5.16\n",
      "8                     0           0       0.686       0.656       0.571       0.667           1           1         9.1\n",
      "16                    0           0       0.706       0.724       0.467       0.375           1           0        19.1\n",
      "32                    0           0       0.638       0.565       0.621       0.786           1           1        36.9\n",
      "64                    0           0       0.637       0.636       0.667       0.714       0.714         0.5        74.1\n",
      "128                   0           0       0.639       0.641       0.658       0.649       0.714       0.714         151\n",
      "256                   0           0       0.635        0.63       0.665       0.673        0.69       0.667         302\n",
      "512                   0           0       0.636       0.636       0.685       0.705       0.672       0.655         607\n",
      "1024                  0           0       0.642       0.649       0.668       0.651       0.658       0.644    1.21e+03\n",
      "2048                  0           0       0.641        0.64       0.691       0.713       0.667       0.675    2.42e+03\n",
      "4096                  0           0       0.637       0.633        0.71       0.729       0.684       0.701    4.85e+03\n",
      "8192                  0           0       0.632       0.627       0.723       0.735       0.723       0.763    9.68e+03\n",
      "10931                 0           0        0.63       0.625       0.727       0.742       0.721       0.712    1.29e+04\n",
      "12180             0.103           1        0.63           0       0.727           0       0.719       0.717    1.39e+04\n"
     ]
    }
   ],
   "source": [
    "def learn_ranker(rank):\n",
    "    from RewardPredictor import RewardPredictor\n",
    "    from TaskLLM import TaskLLM\n",
    "    from PersonalizedCitation import train_loader, dev_loader\n",
    "    from ProgressPrinter import ProgressPrinter\n",
    "    from peft import IA3Config, TaskType\n",
    "    from transformers import T5ForConditionalGeneration\n",
    "    import torch\n",
    "    from Util import interleave\n",
    "    \n",
    "    device = 'cuda'\n",
    "    torch.set_default_device(device)\n",
    "    torch.manual_seed(2112)\n",
    "\n",
    "    train = train_loader(batch_size=2, double_data=True)\n",
    "    dev = dev_loader(batch_size=2)\n",
    "\n",
    "    t5 = T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\n",
    "    t5.load_adapter('User_keq1_t5base_step1', 'taskllm')\n",
    "\n",
    "    rhat_config = IA3Config(task_type=TaskType.SEQ_2_SEQ_LM)\n",
    "    t5.add_adapter(rhat_config, \"rhat\")\n",
    "    t5.enable_adapters()\n",
    "    \n",
    "    taskllm = TaskLLM(t5=t5, adapter_name=\"taskllm\")\n",
    "    rewardpredictor = RewardPredictor(t5=t5, adapter_name=\"rhat\")\n",
    "    \n",
    "    def reward_augment(inputs):\n",
    "        import re\n",
    "        return [ re.sub(r'Ref1: (.*)\\nRef2: (.*)\\nExtra:',\n",
    "                        r'Ref1: \\2\\nRef2: \\1\\nExtra:',\n",
    "                        z)\n",
    "                 for z in inputs ]\n",
    "\n",
    "    with ProgressPrinter('iter', f'{rank} loss', f'{rank} acc', f'{rank} acc (dev)') as printer:\n",
    "        for iteration in range(2):\n",
    "            for istrain, (examples, labels) in interleave(train, dev):\n",
    "                if iteration == 0 or not istrain:\n",
    "                    for ex, label in zip(examples, labels):\n",
    "                        greedyrewards = []\n",
    "                        allloss = []\n",
    "                        with torch.no_grad():\n",
    "                            embeddings = train.embed( [ ex['ref1'], ex['ref2'] ] + \n",
    "                                                      [ v['title'] \n",
    "                                                       for v in ex['profile']\n",
    "                                                       if v['title'] != ex['title'] \n",
    "                                                     ])\n",
    "                            scores = torch.max(embeddings[[0,1],:] @ embeddings[2:,:].T, dim=0).values\n",
    "                            index = torch.topk(scores, dim=0, k=rank).indices.to('cpu')\n",
    "                            prompts = []\n",
    "                            rhatprompts = []\n",
    "                            for n, oneind in enumerate(index.tolist()):\n",
    "                                titles = [ f'{ex[\"profile\"][ind][\"title\"]}' for ind in (oneind,) ]\n",
    "                                concat_titles = ' and '.join([f'\"{v}\"' for v in titles])\n",
    "                                input = train.append_to_title(ex, concat_titles)\n",
    "                                prompts.append(input)\n",
    "                                rhatprompt = f\"Title: {ex['title']}\\nRef1: {ex['ref1']}\\nRef2: {ex['ref2']}\\nExtra: {titles[0]}\"\n",
    "                                rhatprompts.append(rhatprompt)\n",
    "                            \n",
    "                            guesses = taskllm.predict(prompts, augment=train.swap_refs).argmax(dim=1)\n",
    "                            target = int(label == train.choices[1])\n",
    "                            rewards = (guesses == target).float().unsqueeze(1)\n",
    "                            rhats = rewardpredictor.predict(rhatprompts, augment=reward_augment)\n",
    "                            greedy = torch.argmax(rhats, dim=0).item()\n",
    "                            greedyreward = rewards[greedy, 0].item()\n",
    "                            greedyrewards.append(greedyreward)\n",
    "                            \n",
    "                        loss = rewardpredictor.learn(rhatprompts, rewards, augment=reward_augment) if istrain else None\n",
    "                        allloss.append(loss)\n",
    "\n",
    "                    greedyacc = torch.Tensor(greedyrewards).float().mean().item()\n",
    "                    predloss = torch.Tensor(allloss).mean().item() if istrain else None\n",
    "\n",
    "                    printer.addobs(iteration, predloss, greedyacc if istrain else None, greedyacc if not istrain else None)\n",
    "\n",
    "            printer.print()\n",
    "            printer.autoprint = False\n",
    "            if iteration == 0:\n",
    "                rewardpredictor.save_pretrained(f'User_keq1_t5base_step2_rankeq{rank}')\n",
    "\n",
    "from Fork import SubProcess\n",
    "for rank in range(8, 9):\n",
    "    with SubProcess() as process: process.parent or learn_ranker(rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc8e983-57d0-4e7f-8077-dfa64999a795",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## No benefit from multiple passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0498ff16-f362-4447-a4fe-cf1e87721616",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n                  iter       since      8 loss       since       8 acc       since 8 acc (dev)       since      dt (s)\n",
      "1                     0           0       0.718       0.718           0           0           0           0        1.94\n",
      "2                     0           0       0.708       0.699           0           0           0           0        3.01\n",
      "4                     0           0       0.709        0.71         0.5           1           0           0        5.19\n",
      "8                     0           0       0.686       0.656       0.571       0.667           1           1        9.08\n",
      "16                    0           0       0.706       0.724       0.467       0.375           1           0        18.9\n",
      "32                    0           0       0.638       0.565       0.621       0.786           1           1        36.4\n",
      "64                    0           0       0.637       0.636       0.667       0.714       0.714         0.5        72.8\n",
      "128                   0           0       0.639       0.641       0.658       0.649       0.714       0.714         148\n",
      "256                   0           0       0.635        0.63       0.665       0.673        0.69       0.667         298\n",
      "512                   0           0       0.636       0.636       0.685       0.705       0.672       0.655         603\n",
      "1024                  0           0       0.642       0.649       0.668       0.651       0.658       0.644    1.21e+03\n",
      "2048                  0           0       0.641        0.64       0.691       0.713       0.667       0.675    2.42e+03\n",
      "4096                  0           0       0.637       0.633        0.71       0.729       0.684       0.701    4.85e+03\n",
      "8192                  0           0       0.632       0.627       0.723       0.735       0.723       0.763     9.7e+03\n",
      "10931                 0           0        0.63       0.625       0.727       0.742       0.721       0.712    1.29e+04\n",
      "21862               0.5           1       0.624       0.619       0.739       0.751       0.717       0.714    2.58e+04\n",
      "23111             0.581           2       0.624           0       0.739           0       0.718       0.719    2.68e+04\n"
     ]
    }
   ],
   "source": [
    "# two training passes\n",
    "def learn_ranker(rank):\n",
    "    from RewardPredictor import RewardPredictor\n",
    "    from TaskLLM import TaskLLM\n",
    "    from PersonalizedCitation import train_loader, dev_loader\n",
    "    from ProgressPrinter import ProgressPrinter\n",
    "    from peft import PeftConfig, IA3Config, TaskType\n",
    "    from transformers import T5ForConditionalGeneration\n",
    "    import torch\n",
    "    from Util import interleave\n",
    "    \n",
    "    device = 'cuda'\n",
    "    torch.set_default_device(device)\n",
    "    torch.manual_seed(2112)\n",
    "\n",
    "    train = train_loader(batch_size=2, double_data=True)\n",
    "    dev = dev_loader(batch_size=2)\n",
    "\n",
    "    t5 = T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\n",
    "    t5.load_adapter('User_keq1_t5base_step1', 'taskllm')\n",
    "\n",
    "    rhat_config = IA3Config(task_type=TaskType.SEQ_2_SEQ_LM)\n",
    "    t5.add_adapter(rhat_config, \"rhat\")\n",
    "    t5.enable_adapters()\n",
    "    \n",
    "    taskllm = TaskLLM(t5=t5, adapter_name=\"taskllm\")\n",
    "    rewardpredictor = RewardPredictor(t5=t5, adapter_name=\"rhat\")\n",
    "    \n",
    "    def reward_augment(inputs):\n",
    "        import re\n",
    "        return [ re.sub(r'Ref1: (.*)\\nRef2: (.*)\\nExtra:',\n",
    "                        r'Ref1: \\2\\nRef2: \\1\\nExtra:',\n",
    "                        z)\n",
    "                 for z in inputs ]\n",
    "\n",
    "    with ProgressPrinter('iter', f'{rank} loss', f'{rank} acc', f'{rank} acc (dev)') as printer:\n",
    "        max_iteration = 3\n",
    "        for iteration in range(max_iteration):\n",
    "            for istrain, (examples, labels) in interleave(train, dev):\n",
    "                if iteration + 1 < max_iteration or not istrain:\n",
    "                    for ex, label in zip(examples, labels):\n",
    "                        greedyrewards = []\n",
    "                        allloss = []\n",
    "                        with torch.no_grad():\n",
    "                            embeddings = train.embed( [ ex['ref1'], ex['ref2'] ] + \n",
    "                                                      [ v['title'] \n",
    "                                                       for v in ex['profile']\n",
    "                                                       if v['title'] != ex['title'] \n",
    "                                                     ])\n",
    "                            scores = torch.max(embeddings[[0,1],:] @ embeddings[2:,:].T, dim=0).values\n",
    "                            index = torch.topk(scores, dim=0, k=rank).indices.to('cpu')\n",
    "                            prompts = []\n",
    "                            rhatprompts = []\n",
    "                            for n, oneind in enumerate(index.tolist()):\n",
    "                                titles = [ f'{ex[\"profile\"][ind][\"title\"]}' for ind in (oneind,) ]\n",
    "                                concat_titles = ' and '.join([f'\"{v}\"' for v in titles])\n",
    "                                input = train.append_to_title(ex, concat_titles)\n",
    "                                prompts.append(input)\n",
    "                                rhatprompt = f\"Title: {ex['title']}\\nRef1: {ex['ref1']}\\nRef2: {ex['ref2']}\\nExtra: {titles[0]}\"\n",
    "                                rhatprompts.append(rhatprompt)\n",
    "                            \n",
    "                            guesses = taskllm.predict(prompts, augment=train.swap_refs).argmax(dim=1)\n",
    "                            target = int(label == train.choices[1])\n",
    "                            rewards = (guesses == target).float().unsqueeze(1)\n",
    "                            rhats = rewardpredictor.predict(rhatprompts, augment=reward_augment)\n",
    "                            greedy = torch.argmax(rhats, dim=0).item()\n",
    "                            greedyreward = rewards[greedy, 0].item()\n",
    "                            greedyrewards.append(greedyreward)\n",
    "                            \n",
    "                        loss = rewardpredictor.learn(rhatprompts, rewards, augment=reward_augment) if istrain else None\n",
    "                        allloss.append(loss)\n",
    "\n",
    "                    greedyacc = torch.Tensor(greedyrewards).float().mean().item()\n",
    "                    predloss = torch.Tensor(allloss).mean().item() if istrain else None\n",
    "\n",
    "                    printer.addobs(iteration, predloss, greedyacc if istrain else None, greedyacc if not istrain else None)\n",
    "\n",
    "            printer.print()\n",
    "            printer.autoprint = False\n",
    "            \n",
    "        rewardpredictor.save_pretrained(f'User_keq1_t5base_step2_rankeq{rank}')\n",
    "\n",
    "from Fork import SubProcess\n",
    "for rank in range(8, 9):\n",
    "    with SubProcess() as process: process.parent or learn_ranker(rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4209542-8348-4a45-96a4-3d1a9c6d9289",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n                  iter       since      8 loss       since       8 acc       since 8 acc (dev)       since      dt (s)\n",
      "1                     0           0       0.718       0.718           0           0           0           0        1.84\n",
      "2                     0           0       0.708       0.699           0           0           0           0         2.9\n",
      "4                     0           0       0.709        0.71         0.5           1           0           0        5.06\n",
      "8                     0           0       0.686       0.656       0.571       0.667           1           1        8.88\n",
      "16                    0           0       0.706       0.724       0.467       0.375           1           0        18.5\n",
      "32                    0           0       0.638       0.565       0.621       0.786           1           1        35.7\n",
      "64                    0           0       0.637       0.636       0.667       0.714       0.714         0.5        71.6\n",
      "128                   0           0       0.639       0.641       0.658       0.649       0.714       0.714         146\n",
      "256                   0           0       0.635        0.63       0.665       0.673        0.69       0.667         295\n",
      "512                   0           0       0.636       0.636       0.685       0.705       0.672       0.655         599\n",
      "1024                  0           0       0.642       0.649       0.668       0.651       0.658       0.644     1.2e+03\n",
      "2048                  0           0       0.641        0.64       0.691       0.713       0.667       0.675    2.41e+03\n",
      "4096                  0           0       0.637       0.633        0.71       0.729       0.684       0.701    4.82e+03\n",
      "8192                  0           0       0.632       0.627       0.723       0.735       0.723       0.763    9.65e+03\n",
      "10931                 0           0        0.63       0.625       0.727       0.742       0.721       0.712    1.29e+04\n",
      "21862               0.5           1       0.624       0.619       0.739       0.751       0.717       0.714    2.58e+04\n",
      "32793                 1           2        0.62       0.611       0.744       0.752       0.715        0.71    3.87e+04\n",
      "34042              1.07           3        0.62           0       0.744           0       0.714       0.712    3.97e+04\n"
     ]
    }
   ],
   "source": [
    "# three training passes\n",
    "def learn_ranker(rank):\n",
    "    from RewardPredictor import RewardPredictor\n",
    "    from TaskLLM import TaskLLM\n",
    "    from PersonalizedCitation import train_loader, dev_loader\n",
    "    from ProgressPrinter import ProgressPrinter\n",
    "    from peft import PeftConfig, IA3Config, TaskType\n",
    "    from transformers import T5ForConditionalGeneration\n",
    "    import torch\n",
    "    from Util import interleave\n",
    "    \n",
    "    device = 'cuda'\n",
    "    torch.set_default_device(device)\n",
    "    torch.manual_seed(2112)\n",
    "\n",
    "    train = train_loader(batch_size=2, double_data=True)\n",
    "    dev = dev_loader(batch_size=2)\n",
    "\n",
    "    t5 = T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\n",
    "    t5.load_adapter('User_keq1_t5base_step1', 'taskllm')\n",
    "\n",
    "    rhat_config = IA3Config(task_type=TaskType.SEQ_2_SEQ_LM)\n",
    "    t5.add_adapter(rhat_config, \"rhat\")\n",
    "    t5.enable_adapters()\n",
    "    \n",
    "    taskllm = TaskLLM(t5=t5, adapter_name=\"taskllm\")\n",
    "    rewardpredictor = RewardPredictor(t5=t5, adapter_name=\"rhat\")\n",
    "    \n",
    "    def reward_augment(inputs):\n",
    "        import re\n",
    "        return [ re.sub(r'Ref1: (.*)\\nRef2: (.*)\\nExtra:',\n",
    "                        r'Ref1: \\2\\nRef2: \\1\\nExtra:',\n",
    "                        z)\n",
    "                 for z in inputs ]\n",
    "\n",
    "    with ProgressPrinter('iter', f'{rank} loss', f'{rank} acc', f'{rank} acc (dev)') as printer:\n",
    "        max_iteration = 4\n",
    "        for iteration in range(max_iteration):\n",
    "            for istrain, (examples, labels) in interleave(train, dev):\n",
    "                if iteration + 1 < max_iteration or not istrain:\n",
    "                    for ex, label in zip(examples, labels):\n",
    "                        greedyrewards = []\n",
    "                        allloss = []\n",
    "                        with torch.no_grad():\n",
    "                            embeddings = train.embed( [ ex['ref1'], ex['ref2'] ] + \n",
    "                                                      [ v['title'] \n",
    "                                                       for v in ex['profile']\n",
    "                                                       if v['title'] != ex['title'] \n",
    "                                                     ])\n",
    "                            scores = torch.max(embeddings[[0,1],:] @ embeddings[2:,:].T, dim=0).values\n",
    "                            index = torch.topk(scores, dim=0, k=rank).indices.to('cpu')\n",
    "                            prompts = []\n",
    "                            rhatprompts = []\n",
    "                            for n, oneind in enumerate(index.tolist()):\n",
    "                                titles = [ f'{ex[\"profile\"][ind][\"title\"]}' for ind in (oneind,) ]\n",
    "                                concat_titles = ' and '.join([f'\"{v}\"' for v in titles])\n",
    "                                input = train.append_to_title(ex, concat_titles)\n",
    "                                prompts.append(input)\n",
    "                                rhatprompt = f\"Title: {ex['title']}\\nRef1: {ex['ref1']}\\nRef2: {ex['ref2']}\\nExtra: {titles[0]}\"\n",
    "                                rhatprompts.append(rhatprompt)\n",
    "                            \n",
    "                            guesses = taskllm.predict(prompts, augment=train.swap_refs).argmax(dim=1)\n",
    "                            target = int(label == train.choices[1])\n",
    "                            rewards = (guesses == target).float().unsqueeze(1)\n",
    "                            rhats = rewardpredictor.predict(rhatprompts, augment=reward_augment)\n",
    "                            greedy = torch.argmax(rhats, dim=0).item()\n",
    "                            greedyreward = rewards[greedy, 0].item()\n",
    "                            greedyrewards.append(greedyreward)\n",
    "                            \n",
    "                        loss = rewardpredictor.learn(rhatprompts, rewards, augment=reward_augment) if istrain else None\n",
    "                        allloss.append(loss)\n",
    "\n",
    "                    greedyacc = torch.Tensor(greedyrewards).float().mean().item()\n",
    "                    predloss = torch.Tensor(allloss).mean().item() if istrain else None\n",
    "\n",
    "                    printer.addobs(iteration, predloss, greedyacc if istrain else None, greedyacc if not istrain else None)\n",
    "\n",
    "            printer.print()\n",
    "            printer.autoprint = False\n",
    "            \n",
    "        rewardpredictor.save_pretrained(f'User_keq1_t5base_step2_rankeq{rank}')\n",
    "\n",
    "from Fork import SubProcess\n",
    "for rank in range(8, 9):\n",
    "    with SubProcess() as process: process.parent or learn_ranker(rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7495cf9f-8659-4ef3-b346-82ed566d1840",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## No benefit from larger rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "405a3450-e82e-4350-b3ee-dab2c89cd942",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n                    iter        since      12 loss        since       12 acc        since 12 acc (dev)        since       dt (s)\n",
      "1                       0            0         0.72         0.72            0            0            0            0         2.15\n",
      "2                       0            0        0.708        0.695            0            0            0            0         3.65\n",
      "4                       0            0        0.704        0.701          0.5            1            0            0          6.6\n",
      "8                       0            0        0.678        0.644        0.571        0.667            1            1           12\n",
      "16                      0            0        0.709        0.735        0.533          0.5            1            0         25.9\n",
      "32                      0            0        0.645        0.577        0.655        0.786            1            1         50.7\n",
      "64                      0            0        0.634        0.623        0.702         0.75        0.714          0.5          102\n",
      "128                     0            0        0.643        0.651        0.667        0.632        0.786        0.857          207\n",
      "256                     0            0        0.643        0.643        0.661        0.655        0.759        0.733          412\n",
      "512                     0            0        0.643        0.644        0.659        0.656        0.672        0.586          828\n",
      "1024                    0            0        0.649        0.656        0.656        0.653        0.667        0.661     1.65e+03\n",
      "2048                    0            0        0.648        0.646        0.686        0.717         0.65        0.632      3.3e+03\n",
      "4096                    0            0        0.644        0.641        0.712        0.738        0.679        0.709      6.6e+03\n",
      "8192                    0            0         0.64        0.636        0.721        0.729        0.704        0.729     1.32e+04\n",
      "10931                   0            0        0.639        0.636        0.725        0.738        0.708        0.719     1.76e+04\n",
      "21862                 0.5            1        0.634        0.628        0.738        0.751        0.712        0.716     3.52e+04\n",
      "23111               0.581            2        0.634            0        0.738            0        0.712        0.712     3.64e+04\n",
      "n                    iter        since      16 loss        since       16 acc        since 16 acc (dev)        since       dt (s)\n",
      "1                       0            0        0.721        0.721            0            0            0            0         2.54\n",
      "2                       0            0        0.705         0.69            0            0            0            0         4.43\n",
      "4                       0            0        0.705        0.704          0.5            1            0            0         8.37\n",
      "8                       0            0        0.686        0.661        0.571        0.667            1            1         15.4\n",
      "16                      0            0        0.707        0.726        0.467        0.375            1            0         33.2\n",
      "32                      0            0        0.654        0.598        0.621        0.786            1            1         65.2\n",
      "64                      0            0        0.639        0.623        0.667        0.714        0.857         0.75          131\n",
      "128                     0            0        0.649        0.658        0.658        0.649        0.786        0.714          264\n",
      "256                     0            0        0.648        0.648        0.683        0.708         0.69          0.6          523\n",
      "512                     0            0         0.65        0.651        0.656         0.63        0.621        0.552     1.05e+03\n",
      "1024                    0            0        0.654        0.659        0.643        0.629         0.65        0.678     2.09e+03\n",
      "2048                    0            0        0.652        0.649        0.672        0.701        0.611        0.573     4.19e+03\n",
      "4096                    0            0        0.648        0.645        0.698        0.725        0.643        0.675     8.38e+03\n",
      "8192                    0            0        0.645        0.641        0.717        0.735        0.678        0.714     1.67e+04\n",
      "10931                   0            0        0.644         0.64        0.721        0.732        0.689        0.722     2.23e+04\n",
      "21862                 0.5            1        0.639        0.634        0.737        0.754        0.709        0.728     4.46e+04\n",
      "23111               0.581            2        0.639            0        0.737            0         0.71        0.714     4.61e+04\n",
      "n                    iter        since      20 loss        since       20 acc        since 20 acc (dev)        since       dt (s)\n",
      "1                       0            0        0.719        0.719            1            1            0            0         2.98\n",
      "2                       0            0        0.705        0.692          0.5            0            0            0         5.27\n",
      "4                       0            0        0.704        0.702         0.75            1            0            0         9.99\n",
      "8                       0            0        0.685        0.659        0.714        0.667            1            1         18.5\n",
      "16                      0            0        0.707        0.727        0.533        0.375            1            0         40.2\n",
      "32                      0            0        0.659        0.607        0.655        0.786            1            1         78.8\n",
      "64                      0            0        0.639        0.619        0.684        0.714        0.857         0.75          159\n",
      "128                     0            0        0.649        0.658        0.675        0.667        0.714        0.571          320\n",
      "256                     0            0        0.651        0.653        0.674        0.673        0.552          0.4          633\n",
      "512                     0            0        0.652        0.654        0.667        0.661        0.534        0.517     1.27e+03\n",
      "1024                    0            0        0.656         0.66        0.656        0.645        0.564        0.593     2.52e+03\n",
      "2048                    0            0        0.654        0.652        0.676        0.697        0.585        0.607     5.04e+03\n",
      "4096                    0            0        0.652         0.65        0.698        0.719        0.643        0.701     1.01e+04\n",
      "8192                    0            0        0.649        0.646        0.714         0.73        0.681        0.718     2.01e+04\n",
      "10931                   0            0        0.648        0.645        0.719        0.734        0.688        0.709     2.69e+04\n",
      "21862                 0.5            1        0.643        0.638        0.735        0.751        0.702        0.717     5.38e+04\n",
      "23111               0.581            2        0.643            0        0.735            0          0.7        0.696     5.57e+04\n",
      "n                    iter        since      24 loss        since       24 acc        since 24 acc (dev)        since       dt (s)\n",
      "1                       0            0        0.717        0.717            1            1            0            0         3.39\n",
      "2                       0            0        0.703        0.689          0.5            0            0            0          6.1\n",
      "4                       0            0        0.703        0.702         0.75            1            0            0         11.6\n",
      "8                       0            0        0.684        0.659        0.714        0.667            1            1         21.8\n",
      "16                      0            0        0.705        0.723        0.533        0.375            1            0         47.5\n",
      "32                      0            0        0.662        0.616        0.655        0.786            1            1         92.9\n",
      "64                      0            0        0.645        0.627        0.667        0.679        0.857         0.75          187\n",
      "128                     0            0        0.655        0.665        0.658        0.649        0.786        0.714          378\n",
      "256                     0            0        0.656        0.657        0.652        0.646        0.655        0.533          748\n",
      "512                     0            0        0.656        0.656        0.654        0.656        0.569        0.483      1.5e+03\n",
      "1024                    0            0         0.66        0.663        0.645        0.636         0.59         0.61     2.98e+03\n",
      "2048                    0            0        0.657        0.655        0.665        0.685        0.594        0.598     5.96e+03\n",
      "4096                    0            0        0.655        0.652        0.685        0.705        0.622         0.65     1.19e+04\n",
      "6493                    0            0        0.652        0.648        0.697        0.718        0.662         0.73     1.89e+04\n"
     ]
    }
   ],
   "source": [
    "def learn_ranker(rank, max_iteration):\n",
    "    from RewardPredictor import RewardPredictor\n",
    "    from TaskLLM import TaskLLM\n",
    "    from PersonalizedCitation import train_loader, dev_loader\n",
    "    from ProgressPrinter import ProgressPrinter\n",
    "    from peft import PeftConfig, IA3Config, TaskType\n",
    "    from transformers import T5ForConditionalGeneration\n",
    "    import torch\n",
    "    from Util import interleave\n",
    "    \n",
    "    device = 'cuda'\n",
    "    torch.set_default_device(device)\n",
    "    torch.manual_seed(2112)\n",
    "\n",
    "    train = train_loader(batch_size=2, double_data=True)\n",
    "    dev = dev_loader(batch_size=2)\n",
    "\n",
    "    t5 = T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\n",
    "    t5.load_adapter('User_keq1_t5base_step1', 'taskllm')\n",
    "\n",
    "    rhat_config = IA3Config(task_type=TaskType.SEQ_2_SEQ_LM)\n",
    "    t5.add_adapter(rhat_config, \"rhat\")\n",
    "    t5.enable_adapters()\n",
    "    \n",
    "    taskllm = TaskLLM(t5=t5, adapter_name=\"taskllm\")\n",
    "    rewardpredictor = RewardPredictor(t5=t5, adapter_name=\"rhat\")\n",
    "    \n",
    "    def reward_augment(inputs):\n",
    "        import re\n",
    "        return [ re.sub(r'Ref1: (.*)\\nRef2: (.*)\\nExtra:',\n",
    "                        r'Ref1: \\2\\nRef2: \\1\\nExtra:',\n",
    "                        z)\n",
    "                 for z in inputs ]\n",
    "\n",
    "    with ProgressPrinter('iter', f'{rank} loss', f'{rank} acc', f'{rank} acc (dev)') as printer:\n",
    "        for iteration in range(max_iteration):\n",
    "            for istrain, (examples, labels) in interleave(train, dev):\n",
    "                if iteration + 1 < max_iteration or not istrain:\n",
    "                    for ex, label in zip(examples, labels):\n",
    "                        greedyrewards = []\n",
    "                        allloss = []\n",
    "                        with torch.no_grad():\n",
    "                            embeddings = train.embed( [ ex['ref1'], ex['ref2'] ] + \n",
    "                                                      [ v['title'] \n",
    "                                                       for v in ex['profile']\n",
    "                                                       if v['title'] != ex['title'] \n",
    "                                                     ])\n",
    "                            scores = torch.max(embeddings[[0,1],:] @ embeddings[2:,:].T, dim=0).values\n",
    "                            index = torch.topk(scores, dim=0, k=rank).indices.to('cpu')\n",
    "                            prompts = []\n",
    "                            rhatprompts = []\n",
    "                            for n, oneind in enumerate(index.tolist()):\n",
    "                                titles = [ f'{ex[\"profile\"][ind][\"title\"]}' for ind in (oneind,) ]\n",
    "                                concat_titles = ' and '.join([f'\"{v}\"' for v in titles])\n",
    "                                input = train.append_to_title(ex, concat_titles)\n",
    "                                prompts.append(input)\n",
    "                                rhatprompt = f\"Title: {ex['title']}\\nRef1: {ex['ref1']}\\nRef2: {ex['ref2']}\\nExtra: {titles[0]}\"\n",
    "                                rhatprompts.append(rhatprompt)\n",
    "                            \n",
    "                            guesses = taskllm.predict(prompts, augment=train.swap_refs).argmax(dim=1)\n",
    "                            target = int(label == train.choices[1])\n",
    "                            rewards = (guesses == target).float().unsqueeze(1)\n",
    "                            rhats = rewardpredictor.predict(rhatprompts, augment=reward_augment)\n",
    "                            greedy = torch.argmax(rhats, dim=0).item()\n",
    "                            greedyreward = rewards[greedy, 0].item()\n",
    "                            greedyrewards.append(greedyreward)\n",
    "                            \n",
    "                        loss = rewardpredictor.learn(rhatprompts, rewards, augment=reward_augment) if istrain else None\n",
    "                        allloss.append(loss)\n",
    "\n",
    "                    greedyacc = torch.Tensor(greedyrewards).float().mean().item()\n",
    "                    predloss = torch.Tensor(allloss).mean().item() if istrain else None\n",
    "\n",
    "                    printer.addobs(iteration, predloss, greedyacc if istrain else None, greedyacc if not istrain else None)\n",
    "\n",
    "            printer.print()\n",
    "            printer.autoprint = False\n",
    "            \n",
    "        rewardpredictor.save_pretrained(f'User_keq1_t5base_step2_rankeq{rank}')\n",
    "\n",
    "from Fork import SubProcess\n",
    "for rank in range(12, 28, 4):\n",
    "    with SubProcess() as process: process.parent or learn_ranker(rank, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ceafbc-7af5-4035-8b41-712b221c6d39",
   "metadata": {},
   "source": [
    "# Step 3: Prepare leaderboard submission files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "00691200-4034-407f-ae0b-a3a8b39d5799",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n           8 acc (dev)       since      dt (s)\n",
      "1                   0.5         0.5        1.08\n",
      "4                  0.75           1         2.8\n",
      "8                 0.625         0.5        5.17\n",
      "16                0.688        0.75        9.38\n",
      "32                0.688       0.688        18.6\n",
      "64                0.703       0.719        38.5\n",
      "128               0.688       0.672        77.8\n",
      "256               0.703       0.719         154\n",
      "512               0.686       0.668         318\n",
      "1024              0.704       0.723         642\n",
      "2048              0.712       0.721    1.28e+03\n",
      "2500              0.718       0.741    1.57e+03\n"
     ]
    }
   ],
   "source": [
    "def prepare_submission(*, rank):\n",
    "    import json\n",
    "    from RewardPredictor import RewardPredictor\n",
    "    from TaskLLM import TaskLLM\n",
    "    from PersonalizedCitation import train_loader, dev_loader, test_loader\n",
    "    from ProgressPrinter import ProgressPrinter\n",
    "    from peft import PeftConfig, IA3Config, TaskType\n",
    "    from transformers import T5ForConditionalGeneration\n",
    "    import torch\n",
    "    from Util import interleave\n",
    "    \n",
    "    device = 'cuda'\n",
    "    torch.set_default_device(device)\n",
    "    torch.manual_seed(2112)\n",
    "\n",
    "    dev = dev_loader(batch_size=2)\n",
    "    test = test_loader(batch_size=2)\n",
    "\n",
    "    t5 = T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\n",
    "    t5.load_adapter('User_keq1_t5base_step1', 'taskllm')\n",
    "    t5.load_adapter(f'User_keq1_t5base_step2_rankeq{rank}', 'rhat')\n",
    "    t5.enable_adapters()\n",
    "    \n",
    "    taskllm = TaskLLM(t5=t5, adapter_name=\"taskllm\")\n",
    "    rewardpredictor = RewardPredictor(t5=t5, adapter_name=\"rhat\", model_id=f'User_keq1_t5base_step2_rankeq{rank}')\n",
    "    \n",
    "    def reward_augment(inputs):\n",
    "        import re\n",
    "        return [ re.sub(r'Ref1: (.*)\\nRef2: (.*)\\nExtra:',\n",
    "                        r'Ref1: \\2\\nRef2: \\1\\nExtra:',\n",
    "                        z)\n",
    "                 for z in inputs ]\n",
    "\n",
    "    with ProgressPrinter(f'{rank} acc (dev)') as printer:\n",
    "        devgolds, testgolds = [], []\n",
    "        \n",
    "        for isdev, (examples, labels) in interleave(dev, test):\n",
    "            greedyrewards = []\n",
    "            for ex, label in zip(examples, labels):\n",
    "                with torch.no_grad():\n",
    "                    embeddings = dev.embed( [ ex['ref1'], ex['ref2'] ] + \n",
    "                                              [ v['title'] \n",
    "                                               for v in ex['profile']\n",
    "                                               if v['title'] != ex['title'] \n",
    "                                             ])\n",
    "                    scores = torch.max(embeddings[[0,1],:] @ embeddings[2:,:].T, dim=0).values\n",
    "                    index = torch.topk(scores, dim=0, k=rank).indices.to('cpu')\n",
    "                    prompts = []\n",
    "                    rhatprompts = []\n",
    "                    for n, oneind in enumerate(index.tolist()):\n",
    "                        titles = [ f'{ex[\"profile\"][ind][\"title\"]}' for ind in (oneind,) ]\n",
    "                        concat_titles = ' and '.join([f'\"{v}\"' for v in titles])\n",
    "                        input = dev.append_to_title(ex, concat_titles)\n",
    "                        prompts.append(input)\n",
    "                        rhatprompt = f\"Title: {ex['title']}\\nRef1: {ex['ref1']}\\nRef2: {ex['ref2']}\\nExtra: {titles[0]}\"\n",
    "                        rhatprompts.append(rhatprompt)\n",
    "\n",
    "                    rhats = rewardpredictor.predict(rhatprompts, augment=reward_augment)\n",
    "                    greedy = torch.argmax(rhats, dim=0).item()\n",
    "                    guess = taskllm.predict([ prompts[greedy] ], augment=dev.swap_refs).argmax(dim=1)\n",
    "                    if isdev:\n",
    "                        target = int(label == dev.choices[1])\n",
    "                        reward = int(guess.item() == target)\n",
    "                        greedyrewards.append(reward)\n",
    "\n",
    "                    (devgolds if isdev else testgolds).append({ 'id': ex['id'], 'output': \"[2]\" if guess else \"[1]\" })\n",
    "\n",
    "            greedyacc = torch.Tensor(greedyrewards).float().mean().item() if isdev else None\n",
    "\n",
    "            printer.addobs(greedyacc)\n",
    "\n",
    "        printer.print()\n",
    "        printer.autoprint = False\n",
    "\n",
    "        for wut, golds in ( ('dev', devgolds), ('test', testgolds) ):\n",
    "            with open(f'lamp1u_{wut}golds_rankeq{rank}.json', 'w') as jsonfile:\n",
    "                json.dump({ 'task': 'LaMP_1', 'golds': golds }, jsonfile)\n",
    "            \n",
    "from Util import Filter\n",
    "import sys\n",
    "sys.stderr = sys.stderr if type(sys.stderr) == Filter else Filter(sys.stderr, r'Bad pipe message') \n",
    "from Fork import SubProcess\n",
    "with SubProcess() as process: process.parent or prepare_submission(rank=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58810c2b-35e5-4efe-bd43-1b5f25c16357",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
