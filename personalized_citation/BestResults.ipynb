{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "823936ad-6dcd-485e-aac0-cb8ada9f09a8",
   "metadata": {},
   "source": [
    "# flan-t5-base \n",
    "\n",
    "peft ia3, top-k titles based upon `'\\n\\n'.join([ ex['ref1'], ex['ref2'] ])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fc0d49-3d39-4c40-b82a-3dbc31d9ccf4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n                  iter       since      0 loss       since       0 acc       since 0 acc (dev)       since      dt (s)\n",
      "1                     0           0       0.697       0.697         0.5         0.5           0           0       0.865\n",
      "2                     0           0       0.775       0.853       0.375        0.25           0           0        1.15\n",
      "4                     0           0       0.866        1.05       0.417         0.5        0.25        0.25        1.56\n",
      "8                     0           0       0.829       0.801         0.5       0.562        0.25           0        2.61\n",
      "16                    0           0       0.784       0.732       0.481       0.458        0.25        0.25        4.64\n",
      "32                    0           0       0.741       0.697       0.519       0.558         0.5        0.75        8.67\n",
      "64                    0           0       0.737       0.732        0.51         0.5       0.462       0.429        16.7\n",
      "128                   0           0       0.728        0.72       0.502       0.495       0.519       0.577        32.6\n",
      "256                   0           0       0.717       0.705       0.496        0.49       0.466       0.413        64.6\n",
      "512                   0           0       0.708       0.699       0.512       0.527       0.483         0.5         127\n",
      "1024                  0           0       0.703       0.698       0.512       0.512       0.487        0.49         252\n",
      "2048                  0           0       0.699       0.695       0.505       0.498       0.499       0.511         504\n",
      "3045                  0           0       0.698       0.694       0.498       0.485       0.508       0.526         747\n",
      "6090                0.5           1       0.695       0.693       0.504       0.509       0.509        0.51    1.48e+03\n",
      "n                  iter       since      1 loss       since       1 acc       since 1 acc (dev)       since      dt (s)\n",
      "1                     0           0       0.707       0.707        0.25        0.25           0           0        0.91\n",
      "2                     0           0       0.751       0.795        0.25        0.25           0           0        1.19\n",
      "4                     0           0       0.847        1.04       0.333         0.5        0.25        0.25        1.59\n",
      "8                     0           0       0.826       0.811       0.357       0.375        0.25           0        2.63\n",
      "16                    0           0        0.78       0.726       0.404       0.458        0.25        0.25        4.65\n",
      "32                    0           0       0.742       0.703       0.481       0.558         0.5        0.75        8.58\n",
      "64                    0           0       0.737       0.733       0.485        0.49       0.462       0.429        16.5\n",
      "128                   0           0       0.728       0.719       0.488        0.49         0.5       0.538        32.2\n",
      "256                   0           0       0.716       0.703        0.49       0.493       0.457       0.413        63.8\n",
      "512                   0           0       0.704       0.692       0.526       0.562        0.49       0.524         127\n",
      "1024                  0           0       0.686       0.668       0.563         0.6       0.542       0.593         254\n",
      "2048                  0           0       0.661       0.637       0.599       0.635        0.57       0.598         512\n",
      "3045                  0           0       0.648       0.622       0.611       0.635       0.594       0.643         757\n",
      "6090                0.5           1       0.627       0.605       0.636       0.661       0.618       0.643    1.51e+03\n",
      "n                  iter       since      2 loss       since       2 acc       since 2 acc (dev)       since      dt (s)\n",
      "1                     0           0       0.712       0.712        0.25        0.25           0           0       0.857\n",
      "2                     0           0       0.817       0.923        0.25        0.25           0           0        1.14\n",
      "4                     0           0        0.88           1       0.333         0.5        0.25        0.25        1.55\n",
      "8                     0           0       0.834       0.799         0.5       0.625        0.25           0        2.62\n",
      "16                    0           0       0.786       0.729       0.462       0.417        0.25        0.25        4.71\n",
      "32                    0           0       0.745       0.704        0.51       0.558         0.5        0.75         8.8\n",
      "64                    0           0       0.737       0.728        0.52        0.53       0.442       0.393          17\n",
      "128                   0           0       0.727       0.717        0.51         0.5         0.5       0.558        33.4\n",
      "256                   0           0       0.709       0.692       0.505         0.5       0.462       0.423        66.5\n",
      "512                   0           0       0.687       0.665       0.555       0.605         0.5       0.538         133\n",
      "1024                  0           0        0.66       0.633       0.602       0.649        0.56       0.619         266\n",
      "2048                  0           0       0.632       0.603       0.637       0.673       0.607       0.654         532\n",
      "3045                  0           0       0.621         0.6       0.647       0.666       0.624        0.66         792\n",
      "6090                0.5           1       0.597       0.573       0.669       0.692       0.646       0.669     1.6e+03\n",
      "n                  iter       since      3 loss       since       3 acc       since 3 acc (dev)       since      dt (s)\n",
      "1                     0           0       0.715       0.715        0.25        0.25           0           0       0.882\n",
      "2                     0           0        0.84       0.965        0.25        0.25           0           0        1.19\n",
      "4                     0           0       0.882       0.968       0.333         0.5        0.25        0.25        1.61\n",
      "8                     0           0       0.843       0.813       0.464       0.562        0.25           0        2.76\n",
      "16                    0           0       0.791       0.731       0.462       0.458        0.25        0.25        4.95\n",
      "32                    0           0       0.746         0.7        0.51       0.558         0.5        0.75        9.28\n",
      "64                    0           0       0.737       0.728       0.515        0.52       0.442       0.393          18\n",
      "128                   0           0       0.726       0.714       0.502        0.49        0.49       0.538        35.2\n",
      "256                   0           0       0.705       0.685       0.516       0.529       0.486       0.481        70.4\n",
      "512                   0           0       0.682       0.658       0.563        0.61       0.519       0.552         141\n",
      "1024                  0           0       0.653       0.625       0.605       0.647       0.594       0.669         285\n",
      "2048                  0           0       0.621       0.588       0.644       0.682        0.62       0.645         570\n",
      "3045                  0           0        0.61       0.589       0.656       0.683       0.642       0.689         849\n",
      "6090                0.5           1       0.585       0.561       0.681       0.707       0.667       0.692     1.7e+03\n",
      "n                  iter       since      4 loss       since       4 acc       since 4 acc (dev)       since      dt (s)\n",
      "1                     0           0       0.726       0.726        0.25        0.25           0           0        0.92\n",
      "2                     0           0       0.859       0.991        0.25        0.25           0           0        1.33\n",
      "4                     0           0       0.882        0.93       0.333         0.5        0.25        0.25        1.79\n",
      "8                     0           0       0.845       0.816         0.5       0.625        0.25           0           3\n",
      "16                    0           0        0.79       0.726       0.481       0.458        0.25        0.25        5.38\n",
      "32                    0           0       0.742       0.694       0.519       0.558         0.5        0.75        10.1\n",
      "64                    0           0       0.736       0.729       0.515        0.51       0.423       0.357        19.4\n",
      "128                   0           0       0.724       0.713       0.495       0.475        0.49       0.558        38.3\n",
      "256                   0           0       0.697        0.67       0.533       0.571       0.514       0.538        76.3\n",
      "512                   0           0        0.67       0.642       0.581       0.629       0.543       0.571         153\n",
      "1024                  0           0       0.642       0.615       0.626        0.67       0.598       0.652         308\n",
      "2048                  0           0        0.61       0.579       0.662       0.698       0.629       0.661         616\n",
      "3045                  0           0       0.601       0.582       0.672       0.693       0.646       0.682         916\n",
      "6090                0.5           1       0.576       0.551       0.693       0.713       0.671       0.695    1.83e+03\n"
     ]
    }
   ],
   "source": [
    "def peft_t5_baselines(k):\n",
    "    from MegaT5 import PeftT5Classifier\n",
    "    from PersonalizedCitation import train_loader, dev_loader\n",
    "    from ProgressPrinter import ProgressPrinter\n",
    "    from peft import IA3Config, TaskType, prepare_model_for_kbit_training\n",
    "    from transformers import T5ForConditionalGeneration\n",
    "    import torch\n",
    "\n",
    "    device = 'cuda'\n",
    "    torch.set_default_device(device)\n",
    "    torch.manual_seed(2112)\n",
    "\n",
    "    train = train_loader(batch_size=4)\n",
    "    dev = dev_loader(batch_size=4)\n",
    "\n",
    "    def interleave(a, b):\n",
    "        from math import inf\n",
    "        \n",
    "        atot, btot = a.num_examples, b.num_examples\n",
    "        aiter, biter = a.__iter__(), b.__iter__()\n",
    "        aelem, belem = next(aiter), next(biter)\n",
    "        anum, bnum = 1, 1\n",
    "\n",
    "        while anum != inf and bnum != inf:\n",
    "            if anum * btot <= bnum * atot:\n",
    "                yield (True, aelem)\n",
    "                try:\n",
    "                    aelem = next(aiter)\n",
    "                    anum += 1\n",
    "                except StopIteration:\n",
    "                    anum = inf\n",
    "            else:\n",
    "                yield (False, belem)\n",
    "                try:\n",
    "                    belem = next(biter)\n",
    "                    bnum += 1\n",
    "                except StopIteration:\n",
    "                    bnum = inf\n",
    "\n",
    "    peft_config = IA3Config(task_type=TaskType.SEQ_2_SEQ_LM)\n",
    "    t5 = T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\n",
    "    fewshot = PeftT5Classifier(train.num_labels, peft_config, t5=t5)\n",
    "\n",
    "    with ProgressPrinter('iter', f'{k} loss', f'{k} acc', f'{k} acc (dev)') as printer:\n",
    "        for iteration in range(2):\n",
    "            for istrain, (examples, labels) in interleave(train, dev):\n",
    "                with torch.no_grad():\n",
    "                    inputs = []\n",
    "                    target = torch.Tensor([ int(label == train.choices[1]) for label in labels ]).long().to(device)\n",
    "    \n",
    "                    for ex in examples:\n",
    "                        embeddings = train.embed([ '\\n\\n'.join([ ex['ref1'], ex['ref2'] ]) ] + \n",
    "                                                 [ v['title'] \n",
    "                                                   for v in ex['profile']\n",
    "                                                   if v['title'] != ex['title'] \n",
    "                                                 ])\n",
    "                        index = torch.topk(embeddings[0,:] @ embeddings[1:,:].T, dim=0, k=k).indices.to('cpu')\n",
    "                        titles = [ f'\"{ex[\"profile\"][ind][\"title\"]}\"' for ind in index.tolist() ]\n",
    "                        concat_titles = ' and '.join(titles)\n",
    "                        input = train.append_to_title(ex, concat_titles)\n",
    "                        inputs.append(input)\n",
    "    \n",
    "                    fewshotacc = (fewshot.predict(inputs).argmax(dim=1) == target).float().mean().item()\n",
    "\n",
    "                fewloss = fewshot.learn(inputs, target) if istrain else None\n",
    "                printer.addobs(iteration, fewloss, fewshotacc if istrain else None, fewshotacc if not istrain else None)\n",
    "\n",
    "            printer.print()\n",
    "            printer.autoprint = False\n",
    "\n",
    " \n",
    "from Fork import SubProcess\n",
    "for k in range(0, 5):\n",
    "    with SubProcess() as process: process.parent or peft_t5_baselines(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dbad86-686e-4fb2-bd75-54fb6492fe87",
   "metadata": {},
   "source": [
    "peft ia3, top-k titles based upon max similarity with ref1 and ref2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e757cf4b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n                  iter       since      0 loss       since       0 acc       since 0 acc (dev)       since      dt (s)\n",
      "1                     0           0       0.674       0.674           1           1           0           0       0.797\n",
      "2                     0           0        1.44        2.21         0.5           0           0           0        1.02\n",
      "4                     0           0        1.23       0.799       0.333           0           0           0        1.34\n",
      "8                     0           0        0.92        0.69         0.5       0.625           0           0        2.19\n",
      "16                    0           0       0.881       0.835       0.423       0.333         0.5        0.75        3.68\n",
      "32                    0           0         0.8       0.718         0.5       0.577       0.583       0.667        6.93\n",
      "64                    0           0       0.747       0.691       0.539        0.58       0.731       0.857        13.1\n",
      "128                   0           0       0.734       0.722        0.51        0.48       0.577       0.423        25.8\n",
      "256                   0           0       0.725       0.715       0.502       0.495       0.567       0.558        50.9\n",
      "512                   0           0       0.716       0.707       0.498       0.493         0.5       0.434         101\n"
     ]
    }
   ],
   "source": [
    "def peft_t5_baselines(k):\n",
    "    from MegaT5 import PeftT5Classifier\n",
    "    from PersonalizedCitation import train_loader, dev_loader\n",
    "    from ProgressPrinter import ProgressPrinter\n",
    "    from peft import IA3Config, TaskType, prepare_model_for_kbit_training\n",
    "    from transformers import T5ForConditionalGeneration\n",
    "    import torch\n",
    "\n",
    "    device = 'cuda'\n",
    "    torch.set_default_device(device)\n",
    "    torch.manual_seed(2112)\n",
    "\n",
    "    train = train_loader(batch_size=2)\n",
    "    dev = dev_loader(batch_size=2)\n",
    "\n",
    "    def interleave(a, b):\n",
    "        from math import inf\n",
    "        \n",
    "        atot, btot = a.num_examples, b.num_examples\n",
    "        aiter, biter = a.__iter__(), b.__iter__()\n",
    "        aelem, belem = next(aiter), next(biter)\n",
    "        anum, bnum = 1, 1\n",
    "\n",
    "        while anum != inf and bnum != inf:\n",
    "            if anum * btot <= bnum * atot:\n",
    "                yield (True, aelem)\n",
    "                try:\n",
    "                    aelem = next(aiter)\n",
    "                    anum += 1\n",
    "                except StopIteration:\n",
    "                    anum = inf\n",
    "            else:\n",
    "                yield (False, belem)\n",
    "                try:\n",
    "                    belem = next(biter)\n",
    "                    bnum += 1\n",
    "                except StopIteration:\n",
    "                    bnum = inf\n",
    "\n",
    "    peft_config = IA3Config(task_type=TaskType.SEQ_2_SEQ_LM)\n",
    "    t5 = T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\n",
    "    fewshot = PeftT5Classifier(train.num_labels, peft_config, t5=t5)\n",
    "\n",
    "    with ProgressPrinter('iter', f'{k} loss', f'{k} acc', f'{k} acc (dev)') as printer:\n",
    "        for iteration in range(2):\n",
    "            for istrain, (examples, labels) in interleave(train, dev):\n",
    "                with torch.no_grad():\n",
    "                    inputs = []\n",
    "                    target = torch.Tensor([ int(label == train.choices[1]) for label in labels ]).long().to(device)\n",
    "    \n",
    "                    for ex in examples:\n",
    "                        embeddings = train.embed( [ ex['ref1'], ex['ref2'] ] + \n",
    "                                                  [ v['title'] \n",
    "                                                   for v in ex['profile']\n",
    "                                                   if v['title'] != ex['title'] \n",
    "                                                 ])\n",
    "                        scores = torch.max(embeddings[[0,1],:] @ embeddings[2:,:].T, dim=0).values\n",
    "                        index = torch.topk(scores, dim=0, k=k).indices.to('cpu')\n",
    "                        titles = [ f'\"{ex[\"profile\"][ind][\"title\"]}\"' for ind in index.tolist() ]\n",
    "                        concat_titles = ' and '.join(titles)\n",
    "                        input = train.append_to_title(ex, concat_titles)\n",
    "                        inputs.append(input)\n",
    "    \n",
    "                    fewshotacc = (fewshot.predict(inputs).argmax(dim=1) == target).float().mean().item()\n",
    "\n",
    "                fewloss = fewshot.learn(inputs, target) if istrain else None\n",
    "                printer.addobs(iteration, fewloss, fewshotacc if istrain else None, fewshotacc if not istrain else None)\n",
    "\n",
    "            printer.print()\n",
    "            printer.autoprint = False\n",
    "\n",
    " \n",
    "from Fork import SubProcess\n",
    "for k in range(0, 5):\n",
    "    with SubProcess() as process: process.parent or peft_t5_baselines(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2896b6e4-ce54-4abd-a4a2-fa5ebce81956",
   "metadata": {},
   "source": [
    "# flan-t5-xl (8bit)\n",
    "\n",
    "peft ia3, top-k titles based upon `'\\n\\n'.join([ ex['ref1'], ex['ref2'] ])`\n",
    "\n",
    "better than flan-t5-base.  unfortunately flan-t5-xxl doesn't fit on a T4 in 8bit, and 4bit doesn't seem to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "041707bc-a504-4383-ae7c-c5fac9bf5db0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n                  iter       since      0 loss       since       0 acc       since 0 acc (dev)       since      dt (s)\n",
      "1                     0           0       0.671       0.671         0.5         0.5           0           0        2.58\n",
      "2                     0           0       0.642       0.613       0.625        0.75           0           0         4.4\n",
      "4                     0           0        1.02        1.79       0.583         0.5        0.75        0.75        6.62\n",
      "8                     0           0        1.01           1       0.429       0.312        0.75           0        13.3\n",
      "16                    0           0       0.903       0.776       0.442       0.458       0.417        0.25        24.7\n",
      "32                    0           0       0.803       0.702        0.51       0.577       0.583        0.75        48.6\n",
      "64                    0           0       0.774       0.744         0.5        0.49         0.5       0.429        95.8\n",
      "128                   0           0        0.75       0.725       0.485       0.471         0.5         0.5         191\n",
      "256                   0           0       0.729       0.708        0.48       0.475       0.452       0.404         381\n",
      "512                   0           0       0.714         0.7       0.501       0.521       0.462       0.472         761\n",
      "1024                  0           0       0.706       0.697       0.509       0.517       0.464       0.467    1.52e+03\n",
      "2048                  0           0       0.699       0.692       0.515       0.522       0.495       0.526    3.04e+03\n",
      "3045                  0           0       0.697       0.694       0.514        0.51       0.507       0.532     4.5e+03\n",
      "6090                0.5           1       0.693       0.689       0.527        0.54       0.513        0.52    8.92e+03\n",
      "n                  iter       since      4 loss       since       4 acc       since 4 acc (dev)       since      dt (s)\n",
      "1                     0           0       0.645       0.645        0.75        0.75           0           0        2.85\n",
      "2                     0           0       0.661       0.676        0.75        0.75           0           0        5.22\n",
      "4                     0           0        0.73        0.87       0.667         0.5        0.25        0.25           8\n",
      "8                     0           0       0.927        1.07       0.679       0.688        0.25           0        16.5\n",
      "16                    0           0       0.859        0.78       0.596         0.5        0.25        0.25        31.5\n",
      "17                    0           0       0.832       0.479       0.607        0.75        0.25           0        35.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<class 'torch.cuda.OutOfMemoryError'>\n",
      "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 14.76 GiB total capacity; 12.26 GiB already allocated; 8.94 MiB free; 13.98 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  File \"/tmp/ipykernel_3916943/3252240670.py\", line 77, in <module>\n",
      "    with SubProcess() as process: process.parent or peft_t5_baselines(k)\n",
      "  File \"/tmp/ipykernel_3916943/3252240670.py\", line 68, in peft_t5_baselines\n",
      "    fewloss = fewshot.learn(inputs, target) if istrain else None\n",
      "  File \"/home/pmineiro/lampstuff/personalized_citation/MegaT5.py\", line 46, in learn\n",
      "    output = self(x)\n",
      "  File \"/home/pmineiro/miniconda3/envs/lampstuff/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/pmineiro/lampstuff/personalized_citation/MegaT5.py\", line 25, in forward\n",
      "    logits = self._transformer(**enc, decoder_input_ids=decoder_input_ids).logits[:,-1,[self._one,self._two]]\n",
      "  File \"/home/pmineiro/miniconda3/envs/lampstuff/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/pmineiro/miniconda3/envs/lampstuff/lib/python3.10/site-packages/peft/peft_model.py\", line 1078, in forward\n",
      "    return self.base_model(\n",
      "  File \"/home/pmineiro/miniconda3/envs/lampstuff/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/pmineiro/miniconda3/envs/lampstuff/lib/python3.10/site-packages/peft/tuners/tuners_utils.py\", line 94, in forward\n",
      "    return self.model.forward(*args, **kwargs)\n",
      "  File \"/home/pmineiro/miniconda3/envs/lampstuff/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
      "    output = old_forward(*args, **kwargs)\n",
      "  File \"/home/pmineiro/miniconda3/envs/lampstuff/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py\", line 1746, in forward\n",
      "    decoder_outputs = self.decoder(\n",
      "  File \"/home/pmineiro/miniconda3/envs/lampstuff/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/pmineiro/miniconda3/envs/lampstuff/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
      "    output = old_forward(*args, **kwargs)\n",
      "  File \"/home/pmineiro/miniconda3/envs/lampstuff/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py\", line 1123, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/pmineiro/miniconda3/envs/lampstuff/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/pmineiro/miniconda3/envs/lampstuff/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
      "    output = old_forward(*args, **kwargs)\n",
      "  File \"/home/pmineiro/miniconda3/envs/lampstuff/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py\", line 725, in forward\n",
      "    cross_attention_outputs = self.layer[1](\n",
      "  File \"/home/pmineiro/miniconda3/envs/lampstuff/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/pmineiro/miniconda3/envs/lampstuff/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
      "    output = old_forward(*args, **kwargs)\n",
      "  File \"/home/pmineiro/miniconda3/envs/lampstuff/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py\", line 636, in forward\n",
      "    attention_output = self.EncDecAttention(\n",
      "  File \"/home/pmineiro/miniconda3/envs/lampstuff/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/pmineiro/miniconda3/envs/lampstuff/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
      "    output = old_forward(*args, **kwargs)\n",
      "  File \"/home/pmineiro/miniconda3/envs/lampstuff/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py\", line 527, in forward\n",
      "    value_states = project(\n",
      "  File \"/home/pmineiro/miniconda3/envs/lampstuff/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py\", line 502, in project\n",
      "    hidden_states = shape(proj_layer(key_value_states))\n",
      "  File \"/home/pmineiro/miniconda3/envs/lampstuff/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/pmineiro/miniconda3/envs/lampstuff/lib/python3.10/site-packages/peft/tuners/ia3.py\", line 503, in forward\n",
      "    result = super().forward(x)\n",
      "  File \"/home/pmineiro/miniconda3/envs/lampstuff/lib/python3.10/site-packages/bitsandbytes/nn/modules.py\", line 441, in forward\n",
      "    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)\n",
      "  File \"/home/pmineiro/miniconda3/envs/lampstuff/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py\", line 563, in matmul\n",
      "    return MatMul8bitLt.apply(A, B, out, bias, state)\n",
      "  File \"/home/pmineiro/miniconda3/envs/lampstuff/lib/python3.10/site-packages/torch/autograd/function.py\", line 506, in apply\n",
      "    return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
      "  File \"/home/pmineiro/miniconda3/envs/lampstuff/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py\", line 439, in forward\n",
      "    return clone_func(output.view(output_shape))\n",
      "  File \"/home/pmineiro/miniconda3/envs/lampstuff/lib/python3.10/site-packages/torch/utils/_device.py\", line 62, in __torch_function__\n",
      "    return func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "def peft_t5_baselines(k):\n",
    "    from MegaT5 import PeftT5Classifier\n",
    "    from PersonalizedCitation import train_loader, dev_loader\n",
    "    from ProgressPrinter import ProgressPrinter\n",
    "    from peft import IA3Config, TaskType, prepare_model_for_kbit_training\n",
    "    from transformers import T5ForConditionalGeneration\n",
    "    import torch\n",
    "    import warnings\n",
    "\n",
    "    device = 'cuda'\n",
    "    torch.set_default_device(device)\n",
    "    torch.manual_seed(2112)\n",
    "\n",
    "    train = train_loader(batch_size=4)\n",
    "    dev = dev_loader(batch_size=4)\n",
    "\n",
    "    def interleave(a, b):\n",
    "        from math import inf\n",
    "        \n",
    "        atot, btot = a.num_examples, b.num_examples\n",
    "        aiter, biter = a.__iter__(), b.__iter__()\n",
    "        aelem, belem = next(aiter), next(biter)\n",
    "        anum, bnum = 1, 1\n",
    "\n",
    "        while anum != inf and bnum != inf:\n",
    "            if anum * btot <= bnum * atot:\n",
    "                yield (True, aelem)\n",
    "                try:\n",
    "                    aelem = next(aiter)\n",
    "                    anum += 1\n",
    "                except StopIteration:\n",
    "                    anum = inf\n",
    "            else:\n",
    "                yield (False, belem)\n",
    "                try:\n",
    "                    belem = next(biter)\n",
    "                    bnum += 1\n",
    "                except StopIteration:\n",
    "                    bnum = inf\n",
    "\n",
    "    peft_config = IA3Config(task_type=TaskType.SEQ_2_SEQ_LM)\n",
    "    t5 = prepare_model_for_kbit_training(T5ForConditionalGeneration.from_pretrained('google/flan-t5-xl', load_in_8bit=True))\n",
    "    fewshot = PeftT5Classifier(train.num_labels, peft_config, t5=t5)\n",
    "\n",
    "    with ProgressPrinter('iter', f'{k} loss', f'{k} acc', f'{k} acc (dev)') as printer, warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", message=\".*MatMul8bitLt.*\")\n",
    "        \n",
    "        for iteration in range(2):\n",
    "            for istrain, (examples, labels) in interleave(train, dev):\n",
    "                with torch.no_grad():\n",
    "                    inputs = []\n",
    "                    target = torch.Tensor([ int(label == train.choices[1]) for label in labels ]).long().to(device)\n",
    "    \n",
    "                    for ex in examples:\n",
    "                        embeddings = train.embed([ '\\n\\n'.join([ ex['ref1'], ex['ref2'] ]) ] + \n",
    "                                                 [ v['title'] \n",
    "                                                   for v in ex['profile']\n",
    "                                                   if v['title'] != ex['title'] \n",
    "                                                 ])\n",
    "                        index = torch.topk(embeddings[0,:] @ embeddings[1:,:].T, dim=0, k=k).indices.to('cpu')\n",
    "                        titles = [ f'\"{ex[\"profile\"][ind][\"title\"]}\"' for ind in index.tolist() ]\n",
    "                        concat_titles = ' and '.join(titles)\n",
    "                        input = train.append_to_title(ex, concat_titles)\n",
    "                        inputs.append(input)\n",
    "    \n",
    "                    fewshotacc = (fewshot.predict(inputs).argmax(dim=1) == target).float().mean().item()\n",
    "\n",
    "                fewloss = fewshot.learn(inputs, target) if istrain else None\n",
    "                printer.addobs(iteration, fewloss, fewshotacc if istrain else None, fewshotacc if not istrain else None)\n",
    "\n",
    "            printer.print()\n",
    "            printer.autoprint = False\n",
    "\n",
    " \n",
    "from Fork import SubProcess\n",
    "for k in [0,4]:\n",
    "    with SubProcess() as process: process.parent or peft_t5_baselines(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2d76bd3-182c-4135-9d75-5db1660142a3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n                  iter       since      4 loss       since       4 acc       since 4 acc (dev)       since      dt (s)\n",
      "1                     0           0       0.657       0.657         0.5         0.5           0           0        2.14\n",
      "2                     0           0        2.39        4.13        0.25           0           0           0        3.68\n",
      "4                     0           0         1.9       0.906       0.167           0           0           0        5.82\n",
      "8                     0           0        1.23       0.735       0.357         0.5           0           0        11.9\n",
      "16                    0           0        1.09        0.92       0.346       0.333         0.5        0.75        22.1\n",
      "32                    0           0       0.907       0.726       0.423         0.5       0.583       0.667        44.1\n",
      "64                    0           0       0.808       0.704       0.539        0.66       0.731       0.857        86.9\n",
      "128                   0           0       0.759        0.71       0.578       0.618       0.654       0.577         174\n",
      "256                   0           0       0.701       0.643       0.615       0.652       0.663       0.673         350\n",
      "512                   0           0       0.643       0.585       0.667       0.719       0.676       0.689         700\n",
      "1024                  0           0       0.609       0.575       0.682       0.698       0.667       0.657     1.4e+03\n",
      "2048                  0           0       0.575        0.54       0.708       0.734       0.683         0.7    2.79e+03\n",
      "4096                  0           0       0.547       0.519       0.726       0.744       0.704       0.725    5.57e+03\n",
      "6090                  0           0       0.538        0.52       0.731        0.74       0.717       0.743    8.27e+03\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 77\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mFork\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SubProcess\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m4\u001b[39m]:\n\u001b[0;32m---> 77\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m SubProcess() \u001b[38;5;28;01mas\u001b[39;00m process: process\u001b[38;5;241m.\u001b[39mparent \u001b[38;5;129;01mor\u001b[39;00m peft_t5_baselines(k)\n",
      "File \u001b[0;32m~/lampstuff/personalized_citation/Fork.py:16\u001b[0m, in \u001b[0;36mSubProcess.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfork()\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 16\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitpid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10027             0.393           1       0.517       0.486       0.742        0.76       0.721       0.728    1.36e+04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<class 'KeyboardInterrupt'>\n",
      "\n",
      "  File \"/tmp/ipykernel_3916943/3896096721.py\", line 77, in <module>\n",
      "    with SubProcess() as process: process.parent or peft_t5_baselines(k)\n",
      "  File \"/tmp/ipykernel_3916943/3896096721.py\", line 68, in peft_t5_baselines\n",
      "    fewloss = fewshot.learn(inputs, target) if istrain else None\n",
      "  File \"/home/pmineiro/lampstuff/personalized_citation/MegaT5.py\", line 48, in learn\n",
      "    loss.backward()\n",
      "  File \"/home/pmineiro/miniconda3/envs/lampstuff/lib/python3.10/site-packages/torch/_tensor.py\", line 478, in backward\n",
      "    return handle_torch_function(\n",
      "  File \"/home/pmineiro/miniconda3/envs/lampstuff/lib/python3.10/site-packages/torch/overrides.py\", line 1534, in handle_torch_function\n",
      "    result = mode.__torch_function__(public_api, types, args, kwargs)\n",
      "  File \"/home/pmineiro/miniconda3/envs/lampstuff/lib/python3.10/site-packages/torch/utils/_device.py\", line 62, in __torch_function__\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/pmineiro/miniconda3/envs/lampstuff/lib/python3.10/site-packages/torch/_tensor.py\", line 487, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/home/pmineiro/miniconda3/envs/lampstuff/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 200, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    }
   ],
   "source": [
    "def peft_t5_baselines(k):\n",
    "    from MegaT5 import PeftT5Classifier\n",
    "    from PersonalizedCitation import train_loader, dev_loader\n",
    "    from ProgressPrinter import ProgressPrinter\n",
    "    from peft import IA3Config, TaskType, prepare_model_for_kbit_training\n",
    "    from transformers import T5ForConditionalGeneration\n",
    "    import torch\n",
    "    import warnings\n",
    "\n",
    "    device = 'cuda'\n",
    "    torch.set_default_device(device)\n",
    "    torch.manual_seed(2112)\n",
    "\n",
    "    train = train_loader(batch_size=2)\n",
    "    dev = dev_loader(batch_size=2)\n",
    "\n",
    "    def interleave(a, b):\n",
    "        from math import inf\n",
    "        \n",
    "        atot, btot = a.num_examples, b.num_examples\n",
    "        aiter, biter = a.__iter__(), b.__iter__()\n",
    "        aelem, belem = next(aiter), next(biter)\n",
    "        anum, bnum = 1, 1\n",
    "\n",
    "        while anum != inf and bnum != inf:\n",
    "            if anum * btot <= bnum * atot:\n",
    "                yield (True, aelem)\n",
    "                try:\n",
    "                    aelem = next(aiter)\n",
    "                    anum += 1\n",
    "                except StopIteration:\n",
    "                    anum = inf\n",
    "            else:\n",
    "                yield (False, belem)\n",
    "                try:\n",
    "                    belem = next(biter)\n",
    "                    bnum += 1\n",
    "                except StopIteration:\n",
    "                    bnum = inf\n",
    "\n",
    "    peft_config = IA3Config(task_type=TaskType.SEQ_2_SEQ_LM)\n",
    "    t5 = prepare_model_for_kbit_training(T5ForConditionalGeneration.from_pretrained('google/flan-t5-xl', load_in_8bit=True))\n",
    "    fewshot = PeftT5Classifier(train.num_labels, peft_config, t5=t5)\n",
    "\n",
    "    with ProgressPrinter('iter', f'{k} loss', f'{k} acc', f'{k} acc (dev)') as printer, warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", message=\".*MatMul8bitLt.*\")\n",
    "        \n",
    "        for iteration in range(2):\n",
    "            for istrain, (examples, labels) in interleave(train, dev):\n",
    "                with torch.no_grad():\n",
    "                    inputs = []\n",
    "                    target = torch.Tensor([ int(label == train.choices[1]) for label in labels ]).long().to(device)\n",
    "    \n",
    "                    for ex in examples:\n",
    "                        embeddings = train.embed([ '\\n\\n'.join([ ex['ref1'], ex['ref2'] ]) ] + \n",
    "                                                 [ v['title'] \n",
    "                                                   for v in ex['profile']\n",
    "                                                   if v['title'] != ex['title'] \n",
    "                                                 ])\n",
    "                        index = torch.topk(embeddings[0,:] @ embeddings[1:,:].T, dim=0, k=k).indices.to('cpu')\n",
    "                        titles = [ f'\"{ex[\"profile\"][ind][\"title\"]}\"' for ind in index.tolist() ]\n",
    "                        concat_titles = ' and '.join(titles)\n",
    "                        input = train.append_to_title(ex, concat_titles)\n",
    "                        inputs.append(input)\n",
    "    \n",
    "                    fewshotacc = (fewshot.predict(inputs).argmax(dim=1) == target).float().mean().item()\n",
    "\n",
    "                fewloss = fewshot.learn(inputs, target) if istrain else None\n",
    "                printer.addobs(iteration, fewloss, fewshotacc if istrain else None, fewshotacc if not istrain else None)\n",
    "\n",
    "            printer.print()\n",
    "            printer.autoprint = False\n",
    "\n",
    " \n",
    "from Fork import SubProcess\n",
    "for k in [4]:\n",
    "    with SubProcess() as process: process.parent or peft_t5_baselines(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78e38ce-2900-473c-a10d-909c67d89f33",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
