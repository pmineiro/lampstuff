{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a258105a-3b77-499a-bc5c-6e1ceb3e92e5",
   "metadata": {},
   "source": [
    "# Step 1: fine-tune LLM using top result from (fixed) ranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83f4483a-9d5d-4c65-923c-cc9e327c9498",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n                  iter       since      4 loss       since       4 acc       since 4 acc (dev)       since      dt (s)\n",
      "1                     0           0       0.702       0.702       0.125       0.125           0           0       0.995\n",
      "2                     0           0       0.691        0.68       0.312         0.5           0           0        1.37\n",
      "4                     0           0        0.68       0.658       0.542           1           1           1        1.91\n",
      "8                     0           0       0.692         0.7         0.5       0.469           1           0        3.35\n",
      "16                    0           0       0.692       0.693       0.548       0.604       0.542       0.312        5.87\n",
      "32                    0           0       0.689       0.686       0.543       0.538       0.604       0.667        11.3\n",
      "64                    0           0       0.687       0.684       0.561        0.58       0.615       0.625        21.6\n",
      "128                   0           0        0.69       0.693        0.57       0.578       0.543       0.471        42.4\n",
      "256                   0           0       0.685        0.68       0.576       0.582       0.546       0.548        83.4\n",
      "512                   0           0        0.68       0.676       0.589       0.603       0.577       0.608         166\n",
      "1024                  0           0       0.661       0.641       0.613       0.636       0.578       0.579         335\n",
      "2048                  0           0       0.644       0.628       0.631       0.648       0.617       0.657         669\n",
      "4096                  0           0       0.625       0.607       0.648       0.666       0.634       0.651    1.32e+03\n",
      "6090                  0           0        0.62        0.61       0.653       0.663       0.639       0.649    1.95e+03\n",
      "7339               0.17           1        0.62           0       0.653           0       0.647       0.656    2.18e+03\n"
     ]
    }
   ],
   "source": [
    "def step_one(k):\n",
    "    from TaskLLM import TaskLLM\n",
    "    from PersonalizedCitation import train_loader, dev_loader\n",
    "    from ProgressPrinter import ProgressPrinter\n",
    "    from peft import IA3Config, TaskType, prepare_model_for_kbit_training\n",
    "    from transformers import T5ForConditionalGeneration\n",
    "    import torch\n",
    "    from Util import interleave\n",
    "    \n",
    "    device = 'cuda'\n",
    "    torch.set_default_device(device)\n",
    "    torch.manual_seed(2112)\n",
    "\n",
    "    train = train_loader(batch_size=2)\n",
    "    dev = dev_loader(batch_size=2)\n",
    "\n",
    "    t5 = T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\n",
    "    taskllm_config = IA3Config(task_type=TaskType.SEQ_2_SEQ_LM)\n",
    "    t5.add_adapter(taskllm_config, \"taskllm\")\n",
    "    t5.enable_adapters()\n",
    "\n",
    "    taskllm = TaskLLM(t5=t5, adapter_name=\"taskllm\")\n",
    "\n",
    "    with ProgressPrinter('iter', f'{k} loss', f'{k} acc', f'{k} acc (dev)') as printer:\n",
    "        for iteration in range(2):\n",
    "            for istrain, (examples, labels) in interleave(train, dev):\n",
    "                if iteration == 0 or not istrain:\n",
    "                    with torch.no_grad():\n",
    "                        prompts = []\n",
    "                        target = []\n",
    "        \n",
    "                        for ex, label in zip(examples, labels):\n",
    "                            embeddings = train.embed( [ ex['ref1'], ex['ref2'] ] + \n",
    "                                                      [ v['title'] \n",
    "                                                       for v in ex['profile']\n",
    "                                                       if v['title'] != ex['title'] \n",
    "                                                     ])\n",
    "                            scores = torch.max(embeddings[[0,1],:] @ embeddings[2:,:].T, dim=0).values\n",
    "                            index = torch.topk(scores, dim=0, k=k).indices.to('cpu')\n",
    "                            for n, oneind in enumerate(index.tolist()):\n",
    "                                titles = [ f'{ex[\"profile\"][ind][\"title\"]}' for ind in (oneind,) ]\n",
    "                                concat_titles = ' and '.join([f'\"{v}\"' for v in titles])\n",
    "                                prompt = train.append_to_title(ex, concat_titles)\n",
    "                                prompts.append(prompt)\n",
    "                                target.append(int(label == train.choices[1]))\n",
    "\n",
    "                        target = torch.Tensor(target).long().to(device)\n",
    "                        acc = (taskllm.predict(prompts, augment=train.swap_refs).argmax(dim=1) == target).float().mean().item()\n",
    "    \n",
    "                    loss = taskllm.learn(prompts, target, augment=train.swap_refs) if istrain else None\n",
    "                    printer.addobs(iteration, loss, acc if istrain else None, acc if not istrain else None)\n",
    "\n",
    "            printer.print()\n",
    "            printer.autoprint = False\n",
    "            if iteration == 0:\n",
    "                taskllm.save_pretrained(f'User_keq{k}_t5base_step1')\n",
    "\n",
    "from Fork import SubProcess\n",
    "with SubProcess() as process: process.parent or step_one(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf16df8-9320-4be9-98f9-da38da180a91",
   "metadata": {},
   "source": [
    "# Step 2: learn ranker using (fixed pre-finetuned) task LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a3c4dfb-cb62-4d2b-a0e7-d1c281da1c4d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n                  iter       since      8 loss       since       8 acc       since 8 acc (dev)       since      dt (s)\n",
      "1                     0           0       0.693       0.693           1           1           0           0        1.71\n",
      "2                     0           0       0.694       0.695           1           1           0           0        2.81\n",
      "4                     0           0        0.68       0.665        0.75         0.5           0           0        5.07\n",
      "8                     0           0       0.615       0.529       0.857           1           1           1        9.01\n",
      "16                    0           0        0.67       0.719       0.667         0.5           1           0          18\n",
      "32                    0           0       0.606       0.538       0.724       0.786           1           1        34.5\n",
      "64                    0           0       0.654       0.704       0.667       0.607       0.857        0.75        68.8\n",
      "128                   0           0       0.627         0.6       0.667       0.667       0.714       0.571         135\n",
      "256                   0           0       0.613       0.598       0.692       0.717       0.759         0.8         269\n",
      "512                   0           0       0.602       0.591        0.72       0.749       0.672       0.586         535\n",
      "1024                  0           0       0.604       0.605       0.716       0.711       0.675       0.678    1.07e+03\n",
      "2048                  0           0       0.609       0.614       0.709       0.703        0.65       0.624    2.12e+03\n",
      "4096                  0           0       0.609        0.61        0.71        0.71        0.66       0.671    4.23e+03\n",
      "8192                  0           0       0.608       0.607       0.709       0.709       0.677       0.694     8.4e+03\n",
      "10931                 0           0       0.609       0.611       0.706       0.696       0.674       0.665    1.12e+04\n",
      "21862               0.5           1       0.598       0.587       0.719       0.732       0.689       0.704    2.24e+04\n",
      "23111             0.581           2       0.598           0       0.719           0       0.697       0.712    2.34e+04\n"
     ]
    }
   ],
   "source": [
    "def learn_ranker(rank, k=4):\n",
    "    from RewardPredictor import RewardPredictor\n",
    "    from TaskLLM import TaskLLM\n",
    "    from PersonalizedCitation import train_loader, dev_loader\n",
    "    from ProgressPrinter import ProgressPrinter\n",
    "    from SimpleRegret import SimpleRegretGreedyDoubleSampler\n",
    "    from peft import PeftConfig, IA3Config, TaskType\n",
    "    from transformers import T5ForConditionalGeneration\n",
    "    import torch\n",
    "    from Util import interleave\n",
    "    \n",
    "    device = 'cuda'\n",
    "    torch.set_default_device(device)\n",
    "    torch.manual_seed(2112)\n",
    "\n",
    "    train = train_loader(batch_size=2, double_data=True)\n",
    "    dev = dev_loader(batch_size=2)\n",
    "\n",
    "    t5 = T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\n",
    "    t5.load_adapter('User_keq4_t5base_step1', 'taskllm')\n",
    "\n",
    "    rhat_config = IA3Config(task_type=TaskType.SEQ_2_SEQ_LM)\n",
    "    t5.add_adapter(rhat_config, \"rhat\")\n",
    "    t5.enable_adapters()\n",
    "    \n",
    "    taskllm = TaskLLM(t5=t5, adapter_name=\"taskllm\")\n",
    "    rewardpredictor = RewardPredictor(t5=t5, adapter_name=\"rhat\")\n",
    "    \n",
    "    def reward_augment(prompts):\n",
    "        import re\n",
    "        return [ re.sub(r'Ref1: (.*)\\nRef2: (.*)\\nExtra:',\n",
    "                        r'Ref1: \\2\\nRef2: \\1\\nExtra:',\n",
    "                        z)\n",
    "                 for z in prompts ]\n",
    "\n",
    "    gumbel = torch.distributions.gumbel.Gumbel(0,1)\n",
    "    def randomized_similarity(ex, nsamples):\n",
    "        embeddings = train.embed( [ ex['ref1'], ex['ref2'] ] + \n",
    "                                  [ v['title'] \n",
    "                                   for v in ex['profile']\n",
    "                                   if v['title'] != ex['title'] \n",
    "                                 ])\n",
    "        scores = torch.max(embeddings[[0,1],:] @ embeddings[2:,:].T, dim=0).values\n",
    "        temperature = scores[0].item() - scores[4].item()\n",
    "        gumbel_shape = torch.Size([nsamples, scores.shape[0]])\n",
    "        gumbels = temperature * gumbel.sample(gumbel_shape)\n",
    "        return torch.unique(torch.topk(scores.unsqueeze(0) + gumbels, dim=1, k=k).indices, sorted=False, dim=0).to('cpu')\n",
    "\n",
    "    with ProgressPrinter('iter', f'{rank} loss', f'{rank} acc', f'{rank} acc (dev)') as printer:\n",
    "        max_iteration = 3\n",
    "        for iteration in range(max_iteration):\n",
    "            for istrain, (examples, labels) in interleave(train, dev):\n",
    "                if iteration + 1 < max_iteration or not istrain:\n",
    "                    for ex, label in zip(examples, labels):\n",
    "                        greedyrewards = []\n",
    "                        allloss = []\n",
    "                        with torch.no_grad():\n",
    "                            randos = randomized_similarity(ex, 64)\n",
    "                            \n",
    "                            rhatprompts = []\n",
    "                            prompts = []\n",
    "                            for rando in randos:\n",
    "                                titles = [ f'{ex[\"profile\"][ind][\"title\"]}' for ind in rando ]\n",
    "                                concat_titles = ' and '.join([f'\"{v}\"' for v in titles])\n",
    "                                prompt = train.append_to_title(ex, concat_titles)\n",
    "                                prompts.append(prompt)\n",
    "                                rhatprompt = f\"Title: {ex['title']}\\nRef1: {ex['ref1']}\\nRef2: {ex['ref2']}\\n\" + '\\n'.join(\n",
    "                                           [ f\"Extra: {t}\" for t in titles ])\n",
    "                                rhatprompts.append(rhatprompt)\n",
    "\n",
    "                            rhats = rewardpredictor.predict(rhatprompts, augment=reward_augment)\n",
    "                            if len(rhats) > 1:\n",
    "                                explore, exploit = SimpleRegretGreedyDoubleSampler(rhats.view(1, -1), gamma=10)\n",
    "                                actionind = [exploit.item(), explore.item()]\n",
    "                            else:\n",
    "                                actionind = [0]\n",
    "\n",
    "                            guesses = taskllm.predict([ prompts[a] for a in actionind ], augment=train.swap_refs).argmax(dim=1)\n",
    "                            target = int(label == train.choices[1])\n",
    "                            rewards = (guesses == target).float().unsqueeze(1)\n",
    "                            greedyreward = rewards[0, 0].item()\n",
    "                            greedyrewards.append(greedyreward)\n",
    "                            \n",
    "                        loss = rewardpredictor.learn([ rhatprompts[a] for a in actionind ], rewards, augment=reward_augment) if istrain else None\n",
    "                        allloss.append(loss)\n",
    "\n",
    "                    greedyacc = torch.Tensor(greedyrewards).float().mean().item()\n",
    "                    predloss = torch.Tensor(allloss).mean().item() if istrain else None\n",
    "\n",
    "                    printer.addobs(iteration, predloss, greedyacc if istrain else None, greedyacc if not istrain else None)\n",
    "\n",
    "            printer.print()\n",
    "            printer.autoprint = False\n",
    "            \n",
    "        rewardpredictor.save_pretrained(f'User_keq{k}_t5base_step2_rankeq{rank}')\n",
    "\n",
    "from Fork import SubProcess\n",
    "for rank in range(8, 9):\n",
    "    with SubProcess() as process: process.parent or learn_ranker(rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0498ff16-f362-4447-a4fe-cf1e87721616",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n                  iter       since      8 loss       since       8 acc       since 8 acc (dev)       since      dt (s)\n",
      "1                     0           0       0.693       0.693           1           1           0           0        1.67\n",
      "2                     0           0       0.694       0.695           1           1           0           0        2.76\n",
      "4                     0           0        0.68       0.665        0.75         0.5           0           0           5\n",
      "8                     0           0       0.615       0.529       0.857           1           1           1        8.86\n",
      "16                    0           0        0.67       0.719       0.667         0.5           1           0        17.8\n",
      "32                    0           0       0.606       0.538       0.724       0.786           1           1          34\n",
      "64                    0           0       0.654       0.704       0.667       0.607       0.857        0.75        67.8\n",
      "128                   0           0       0.627         0.6       0.667       0.667       0.714       0.571         134\n",
      "256                   0           0       0.613       0.598       0.692       0.717       0.759         0.8         269\n",
      "512                   0           0       0.602       0.591        0.72       0.749       0.672       0.586         537\n",
      "1024                  0           0       0.604       0.605       0.716       0.711       0.675       0.678    1.08e+03\n",
      "2048                  0           0       0.609       0.614       0.709       0.703        0.65       0.624    2.15e+03\n",
      "4096                  0           0       0.609        0.61        0.71        0.71        0.66       0.671    4.28e+03\n",
      "8192                  0           0       0.608       0.607       0.709       0.709       0.677       0.694    8.47e+03\n",
      "10931                 0           0       0.609       0.611       0.706       0.696       0.674       0.665    1.13e+04\n",
      "21862               0.5           1       0.598       0.587       0.719       0.732       0.689       0.704    2.31e+04\n",
      "32793                 1           2       0.585       0.558       0.727       0.743       0.698       0.717     3.8e+04\n",
      "34042              1.07           3       0.585           0       0.727           0       0.705       0.724    3.94e+04\n"
     ]
    }
   ],
   "source": [
    "# 3 training passes\n",
    "def learn_ranker(rank, k=4):\n",
    "    from RewardPredictor import RewardPredictor\n",
    "    from TaskLLM import TaskLLM\n",
    "    from PersonalizedCitation import train_loader, dev_loader\n",
    "    from ProgressPrinter import ProgressPrinter\n",
    "    from SimpleRegret import SimpleRegretGreedyDoubleSampler\n",
    "    from peft import PeftConfig, IA3Config, TaskType\n",
    "    from transformers import T5ForConditionalGeneration\n",
    "    import torch\n",
    "    from Util import interleave\n",
    "    \n",
    "    device = 'cuda'\n",
    "    torch.set_default_device(device)\n",
    "    torch.manual_seed(2112)\n",
    "\n",
    "    train = train_loader(batch_size=2, double_data=True)\n",
    "    dev = dev_loader(batch_size=2)\n",
    "\n",
    "    t5 = T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\n",
    "    t5.load_adapter('User_keq4_t5base_step1', 'taskllm')\n",
    "\n",
    "    rhat_config = IA3Config(task_type=TaskType.SEQ_2_SEQ_LM)\n",
    "    t5.add_adapter(rhat_config, \"rhat\")\n",
    "    t5.enable_adapters()\n",
    "    \n",
    "    taskllm = TaskLLM(t5=t5, adapter_name=\"taskllm\")\n",
    "    rewardpredictor = RewardPredictor(t5=t5, adapter_name=\"rhat\")\n",
    "    \n",
    "    def reward_augment(prompts):\n",
    "        import re\n",
    "        return [ re.sub(r'Ref1: (.*)\\nRef2: (.*)\\nExtra:',\n",
    "                        r'Ref1: \\2\\nRef2: \\1\\nExtra:',\n",
    "                        z)\n",
    "                 for z in prompts ]\n",
    "\n",
    "    gumbel = torch.distributions.gumbel.Gumbel(0,1)\n",
    "    def randomized_similarity(ex, nsamples):\n",
    "        embeddings = train.embed( [ ex['ref1'], ex['ref2'] ] + \n",
    "                                  [ v['title'] \n",
    "                                   for v in ex['profile']\n",
    "                                   if v['title'] != ex['title'] \n",
    "                                 ])\n",
    "        scores = torch.max(embeddings[[0,1],:] @ embeddings[2:,:].T, dim=0).values\n",
    "        temperature = scores[0].item() - scores[4].item()\n",
    "        gumbel_shape = torch.Size([nsamples, scores.shape[0]])\n",
    "        gumbels = temperature * gumbel.sample(gumbel_shape)\n",
    "        return torch.unique(torch.topk(scores.unsqueeze(0) + gumbels, dim=1, k=k).indices, sorted=False, dim=0).to('cpu')\n",
    "\n",
    "    with ProgressPrinter('iter', f'{rank} loss', f'{rank} acc', f'{rank} acc (dev)') as printer:\n",
    "        max_iteration = 4\n",
    "        for iteration in range(max_iteration):\n",
    "            for istrain, (examples, labels) in interleave(train, dev):\n",
    "                if iteration + 1 < max_iteration or not istrain:\n",
    "                    for ex, label in zip(examples, labels):\n",
    "                        greedyrewards = []\n",
    "                        allloss = []\n",
    "                        with torch.no_grad():\n",
    "                            randos = randomized_similarity(ex, 64)\n",
    "                            \n",
    "                            rhatprompts = []\n",
    "                            prompts = []\n",
    "                            for rando in randos:\n",
    "                                titles = [ f'{ex[\"profile\"][ind][\"title\"]}' for ind in rando ]\n",
    "                                concat_titles = ' and '.join([f'\"{v}\"' for v in titles])\n",
    "                                prompt = train.append_to_title(ex, concat_titles)\n",
    "                                prompts.append(prompt)\n",
    "                                rhatprompt = f\"Title: {ex['title']}\\nRef1: {ex['ref1']}\\nRef2: {ex['ref2']}\\n\" + '\\n'.join(\n",
    "                                           [ f\"Extra: {t}\" for t in titles ])\n",
    "                                rhatprompts.append(rhatprompt)\n",
    "\n",
    "                            rhats = rewardpredictor.predict(rhatprompts, augment=reward_augment)\n",
    "                            if len(rhats) > 1:\n",
    "                                explore, exploit = SimpleRegretGreedyDoubleSampler(rhats.view(1, -1), gamma=10)\n",
    "                                actionind = [exploit.item(), explore.item()]\n",
    "                            else:\n",
    "                                actionind = [0]\n",
    "\n",
    "                            guesses = taskllm.predict([ prompts[a] for a in actionind ], augment=train.swap_refs).argmax(dim=1)\n",
    "                            target = int(label == train.choices[1])\n",
    "                            rewards = (guesses == target).float().unsqueeze(1)\n",
    "                            greedyreward = rewards[0, 0].item()\n",
    "                            greedyrewards.append(greedyreward)\n",
    "                            \n",
    "                        loss = rewardpredictor.learn([ rhatprompts[a] for a in actionind ], rewards, augment=reward_augment) if istrain else None\n",
    "                        allloss.append(loss)\n",
    "\n",
    "                    greedyacc = torch.Tensor(greedyrewards).float().mean().item()\n",
    "                    predloss = torch.Tensor(allloss).mean().item() if istrain else None\n",
    "\n",
    "                    printer.addobs(iteration, predloss, greedyacc if istrain else None, greedyacc if not istrain else None)\n",
    "\n",
    "            printer.print()\n",
    "            printer.autoprint = False\n",
    "            \n",
    "        rewardpredictor.save_pretrained(f'User_keq{k}_t5base_step2_rankeq{rank}')\n",
    "\n",
    "from Fork import SubProcess\n",
    "for rank in range(8, 9):\n",
    "    with SubProcess() as process: process.parent or learn_ranker(rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f18337c-9b70-4ec7-bdea-ff7605cf1983",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n                  iter       since      8 loss       since       8 acc       since 8 acc (dev)       since      dt (s)\n",
      "1                     0           0       0.693       0.693           1           1           0           0        2.54\n",
      "2                     0           0       0.694       0.695           1           1           0           0        4.21\n",
      "4                     0           0        0.68       0.665        0.75         0.5           0           0        7.41\n",
      "8                     0           0       0.615       0.529       0.857           1           1           1        12.8\n",
      "16                    0           0        0.67       0.719       0.667         0.5           1           0        24.9\n",
      "32                    0           0       0.606       0.538       0.724       0.786           1           1        47.3\n",
      "64                    0           0       0.654       0.704       0.667       0.607       0.857        0.75        93.2\n",
      "128                   0           0       0.627         0.6       0.667       0.667       0.714       0.571         186\n",
      "256                   0           0       0.613       0.598       0.692       0.717       0.759         0.8         369\n",
      "512                   0           0       0.602       0.591        0.72       0.749       0.672       0.586         735\n",
      "1024                  0           0       0.604       0.605       0.716       0.711       0.675       0.678    1.47e+03\n",
      "2048                  0           0       0.609       0.614       0.709       0.703        0.65       0.624    2.94e+03\n",
      "4096                  0           0       0.609        0.61        0.71        0.71        0.66       0.671     5.9e+03\n",
      "8192                  0           0       0.608       0.607       0.709       0.709       0.677       0.694    1.17e+04\n",
      "10931                 0           0       0.609       0.611       0.706       0.696       0.674       0.665    1.55e+04\n",
      "21862               0.5           1       0.598       0.587       0.719       0.732       0.689       0.704     3.1e+04\n",
      "32793                 1           2       0.585       0.558       0.727       0.743       0.698       0.717    4.65e+04\n",
      "43724               1.5           3       0.575       0.544       0.731       0.746       0.702       0.712    6.19e+04\n",
      "44973              1.57           4       0.575           0       0.731           0        0.71       0.746    6.34e+04\n"
     ]
    }
   ],
   "source": [
    "# 4 training passes\n",
    "def learn_ranker(rank, max_iteration, k=4):\n",
    "    from RewardPredictor import RewardPredictor\n",
    "    from TaskLLM import TaskLLM\n",
    "    from PersonalizedCitation import train_loader, dev_loader\n",
    "    from ProgressPrinter import ProgressPrinter\n",
    "    from SimpleRegret import SimpleRegretGreedyDoubleSampler\n",
    "    from peft import PeftConfig, IA3Config, TaskType\n",
    "    from transformers import T5ForConditionalGeneration\n",
    "    import torch\n",
    "    from Util import interleave\n",
    "    \n",
    "    device = 'cuda'\n",
    "    torch.set_default_device(device)\n",
    "    torch.manual_seed(2112)\n",
    "\n",
    "    train = train_loader(batch_size=2, double_data=True)\n",
    "    dev = dev_loader(batch_size=2)\n",
    "\n",
    "    t5 = T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\n",
    "    t5.load_adapter('User_keq4_t5base_step1', 'taskllm')\n",
    "\n",
    "    rhat_config = IA3Config(task_type=TaskType.SEQ_2_SEQ_LM)\n",
    "    t5.add_adapter(rhat_config, \"rhat\")\n",
    "    t5.enable_adapters()\n",
    "    \n",
    "    taskllm = TaskLLM(t5=t5, adapter_name=\"taskllm\")\n",
    "    rewardpredictor = RewardPredictor(t5=t5, adapter_name=\"rhat\")\n",
    "    \n",
    "    def reward_augment(prompts):\n",
    "        import re\n",
    "        return [ re.sub(r'Ref1: (.*)\\nRef2: (.*)\\nExtra:',\n",
    "                        r'Ref1: \\2\\nRef2: \\1\\nExtra:',\n",
    "                        z)\n",
    "                 for z in prompts ]\n",
    "\n",
    "    gumbel = torch.distributions.gumbel.Gumbel(0,1)\n",
    "    def randomized_similarity(ex, nsamples):\n",
    "        embeddings = train.embed( [ ex['ref1'], ex['ref2'] ] + \n",
    "                                  [ v['title'] \n",
    "                                   for v in ex['profile']\n",
    "                                   if v['title'] != ex['title'] \n",
    "                                 ])\n",
    "        scores = torch.max(embeddings[[0,1],:] @ embeddings[2:,:].T, dim=0).values\n",
    "        temperature = scores[0].item() - scores[4].item()\n",
    "        gumbel_shape = torch.Size([nsamples, scores.shape[0]])\n",
    "        gumbels = temperature * gumbel.sample(gumbel_shape)\n",
    "        return torch.unique(torch.topk(scores.unsqueeze(0) + gumbels, dim=1, k=k).indices, sorted=False, dim=0).to('cpu')\n",
    "\n",
    "    with ProgressPrinter('iter', f'{rank} loss', f'{rank} acc', f'{rank} acc (dev)') as printer:\n",
    "        for iteration in range(max_iteration):\n",
    "            for istrain, (examples, labels) in interleave(train, dev):\n",
    "                if iteration + 1 < max_iteration or not istrain:\n",
    "                    for ex, label in zip(examples, labels):\n",
    "                        greedyrewards = []\n",
    "                        allloss = []\n",
    "                        with torch.no_grad():\n",
    "                            randos = randomized_similarity(ex, 64)\n",
    "                            \n",
    "                            rhatprompts = []\n",
    "                            prompts = []\n",
    "                            for rando in randos:\n",
    "                                titles = [ f'{ex[\"profile\"][ind][\"title\"]}' for ind in rando ]\n",
    "                                concat_titles = ' and '.join([f'\"{v}\"' for v in titles])\n",
    "                                prompt = train.append_to_title(ex, concat_titles)\n",
    "                                prompts.append(prompt)\n",
    "                                rhatprompt = f\"Title: {ex['title']}\\nRef1: {ex['ref1']}\\nRef2: {ex['ref2']}\\n\" + '\\n'.join(\n",
    "                                           [ f\"Extra: {t}\" for t in titles ])\n",
    "                                rhatprompts.append(rhatprompt)\n",
    "\n",
    "                            rhats = rewardpredictor.predict(rhatprompts, augment=reward_augment)\n",
    "                            if len(rhats) > 1:\n",
    "                                explore, exploit = SimpleRegretGreedyDoubleSampler(rhats.view(1, -1), gamma=10)\n",
    "                                actionind = [exploit.item(), explore.item()]\n",
    "                            else:\n",
    "                                actionind = [0]\n",
    "\n",
    "                            guesses = taskllm.predict([ prompts[a] for a in actionind ], augment=train.swap_refs).argmax(dim=1)\n",
    "                            target = int(label == train.choices[1])\n",
    "                            rewards = (guesses == target).float().unsqueeze(1)\n",
    "                            greedyreward = rewards[0, 0].item()\n",
    "                            greedyrewards.append(greedyreward)\n",
    "                            \n",
    "                        loss = rewardpredictor.learn([ rhatprompts[a] for a in actionind ], rewards, augment=reward_augment) if istrain else None\n",
    "                        allloss.append(loss)\n",
    "\n",
    "                    greedyacc = torch.Tensor(greedyrewards).float().mean().item()\n",
    "                    predloss = torch.Tensor(allloss).mean().item() if istrain else None\n",
    "\n",
    "                    printer.addobs(iteration, predloss, greedyacc if istrain else None, greedyacc if not istrain else None)\n",
    "\n",
    "            printer.print()\n",
    "            printer.autoprint = False\n",
    "            \n",
    "        rewardpredictor.save_pretrained(f'User_keq{k}_t5base_step2_rankeq{rank}')\n",
    "\n",
    "from Fork import SubProcess\n",
    "for rank in range(8, 9):\n",
    "    with SubProcess() as process: process.parent or learn_ranker(rank, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28408ef1-b7b6-4ccd-b137-df6a70a9fe57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
